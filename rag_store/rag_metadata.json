[
  {
    "file_name": "LICENSE.txt",
    "file_contents": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
  },
  {
    "file_name": "README.md",
    "file_contents": "<div align=\"center\" xmlns=\"http://www.w3.org/1999/html\">\n<h1 align=\"center\">\nMonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm\n</h1>\n\n[![arXiv](https://img.shields.io/badge/Arxiv-MonkeyOCR-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2506.05218)\n[![HuggingFace](https://img.shields.io/badge/HuggingFace-black.svg?logo=HuggingFace)](https://huggingface.co/echo840/MonkeyOCR-pro-3B)\n[![GitHub issues](https://img.shields.io/github/issues/Yuliang-Liu/MonkeyOCR?color=critical&label=Issues)](https://github.com/Yuliang-Liu/MonkeyOCR/issues?q=is%3Aopen+is%3Aissue)\n[![GitHub closed issues](https://img.shields.io/github/issues-closed/Yuliang-Liu/MonkeyOCR?color=success&label=Issues)](https://github.com/Yuliang-Liu/MonkeyOCR/issues?q=is%3Aissue+is%3Aclosed)\n[![License](https://img.shields.io/badge/License-Apache%202.0-yellow)](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/LICENSE.txt)\n[![GitHub views](https://komarev.com/ghpvc/?username=Yuliang-Liu&repo=MonkeyOCR&color=brightgreen&label=Views)](https://github.com/Yuliang-Liu/MonkeyOCR)\n</div>\n\n\n> **MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm**<br>\n> Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, Xiang Bai <br>\n[![arXiv](https://img.shields.io/badge/Arxiv-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2506.05218) \n[![Source_code](https://img.shields.io/badge/Code-Available-white)](README.md)\n[![Model Weight](https://img.shields.io/badge/HuggingFace-gray)](https://huggingface.co/echo840/MonkeyOCR)\n[![Model Weight](https://img.shields.io/badge/ModelScope-green)](https://modelscope.cn/models/l1731396519/MonkeyOCR)\n[![Public Courses](https://img.shields.io/badge/Openbayes-yellow)](https://openbayes.com/console/public/tutorials/91ESrGvEvBq)\n[![Demo](https://img.shields.io/badge/Demo-blue)](http://vlrlabmonkey.xyz:7685/)\n\n\n\n## Introduction\nMonkeyOCR adopts a Structure-Recognition-Relation (SRR) triplet paradigm, which simplifies the multi-tool pipeline of modular approaches while avoiding the inefficiency of using large multimodal models for full-page document processing.\n\n1. MonkeyOCR-pro-1.2B surpasses MonkeyOCR-3B by 7.4% on Chinese documents.\n2. MonkeyOCR-pro-1.2B delivers approximately a 36% speed improvement over MonkeyOCR-pro-3B, with approximately 1.6% drop in performance.\n3. On olmOCR-Bench, MonkeyOCR-pro-1.2B outperforms Nanonets-OCR-3B by 7.3%.\n4. On OmniDocBench, MonkeyOCR-pro-3B achieves the best overall performance on both English and Chinese documents, outperforming even closed-source and extra-large open-source VLMs such as Gemini 2.0-Flash, Gemini 2.5-Pro, Qwen2.5-VL-72B, GPT-4o, and InternVL3-78B.\n\nSee detailed results below.\n\n### Comparing MonkeyOCR with closed-source and extra large open-source VLMs.\n<a href=\"https://zimgs.com/i/EKhkhY\"><img src=\"https://v1.ax1x.com/2025/07/15/EKhkhY.png\" alt=\"EKhkhY.png\" border=\"0\" /></a>\n\n## Inference Speed (Pages/s) on Different GPUs and [PDF](https://drive.google.com/drive/folders/1geumlJmVY7UUKdr8324sYZ0FHSAElh7m?usp=sharing) Page Counts\n\n<table>\n    <thead>\n\t\t<tr align='center'>\n    \t\t<th>Model</th>\n        \t<th>GPU</th>\n        \t<th>50 Pages</th>\n        \t<th>100 Pages</th>\n        \t<th>300 Pages</th>\n        \t<th>500 Pages</th>\n        \t<th>1000 Pages</th>\n    \t</tr>\n    </thead>\n    <tbody>\n    \t<tr align='center'>\n    \t\t<td rowspan='4'>MonkeyOCR-pro-3B</td>\n        \t<td>3090</td>\n        \t<td>0.492</td>\n        \t<td>0.484</td>\n        \t<td>0.497</td>\n        \t<td>0.492</td>\n        \t<td>0.496</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>A6000</td>\n        \t<td>0.585</td>\n        \t<td>0.587</td>\n        \t<td>0.609</td>\n        \t<td>0.598</td>\n        \t<td>0.608</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>H800</td>\n        \t<td>0.923</td>\n        \t<td>0.768</td>\n        \t<td>0.897</td>\n        \t<td>0.930</td>\n        \t<td>0.891</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>4090</td>\n        \t<td>0.972</td>\n        \t<td>0.969</td>\n        \t<td>1.006</td>\n        \t<td>0.986</td>\n        \t<td>1.006</td>\n    \t</tr>\n    \t<tr align='center'>\n    \t\t<td rowspan='4'>MonkeyOCR-pro-1.2B</td>\n        \t<td>3090</td>\n        \t<td>0.615</td>\n        \t<td>0.660</td>\n        \t<td>0.677</td>\n        \t<td>0.687</td>\n        \t<td>0.683</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>A6000</td>\n        \t<td>0.709</td>\n        \t<td>0.786</td>\n        \t<td>0.825</td>\n        \t<td>0.829</td>\n        \t<td>0.825</td>\n   \t\t</tr>\n    \t<tr align='center'>\n        \t<td>H800</td>\n        \t<td>0.965</td>\n        \t<td>1.082</td>\n        \t<td>1.101</td>\n        \t<td>1.145</td>\n        \t<td>1.015</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>4090</td>\n        \t<td>1.194</td>\n        \t<td>1.314</td>\n        \t<td>1.436</td>\n        \t<td>1.442</td>\n        \t<td>1.434</td>\n    \t</tr>\n    </tbody>\n</table>\n\n## VLM OCR Speed (Pages/s) on Different GPUs and [PDF](https://drive.google.com/drive/folders/1geumlJmVY7UUKdr8324sYZ0FHSAElh7m?usp=sharing) Page Counts\n\n<table>\n    <thead>\n\t\t<tr align='center'>\n    \t\t<th>Model</th>\n        \t<th>GPU</th>\n        \t<th>50 Pages</th>\n        \t<th>100 Pages</th>\n        \t<th>300 Pages</th>\n        \t<th>500 Pages</th>\n        \t<th>1000 Pages</th>\n    \t</tr>\n    </thead>\n    <tbody>\n    \t<tr align='center'>\n    \t\t<td rowspan='4'>MonkeyOCR-pro-3B</td>\n        \t<td>3090</td>\n        \t<td>0.705</td>\n        \t<td>0.680</td>\n        \t<td>0.711</td>\n        \t<td>0.700</td>\n        \t<td>0.724</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>A6000</td>\n        \t<td>0.885</td>\n        \t<td>0.860</td>\n        \t<td>0.915</td>\n        \t<td>0.892</td>\n        \t<td>0.934</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>H800</td>\n        \t<td>1.371</td>\n        \t<td>1.135</td>\n        \t<td>1.339</td>\n        \t<td>1.433</td>\n        \t<td>1.509</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>4090</td>\n        \t<td>1.321</td>\n        \t<td>1.300</td>\n        \t<td>1.384</td>\n        \t<td>1.343</td>\n        \t<td>1.410</td>\n    \t</tr>\n    \t<tr align='center'>\n    \t\t<td rowspan='4'>MonkeyOCR-pro-1.2B</td>\n        \t<td>3090</td>\n        \t<td>0.919</td>\n        \t<td>1.086</td>\n        \t<td>1.166</td>\n        \t<td>1.182</td>\n        \t<td>1.199</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>A6000</td>\n        \t<td>1.177</td>\n        \t<td>1.361</td>\n        \t<td>1.506</td>\n        \t<td>1.525</td>\n        \t<td>1.569</td>\n   \t\t</tr>\n    \t<tr align='center'>\n        \t<td>H800</td>\n        \t<td>1.466</td>\n        \t<td>1.719</td>\n        \t<td>1.763</td>\n        \t<td>1.875</td>\n        \t<td>1.650</td>\n    \t</tr>\n    \t<tr align='center'>\n        \t<td>4090</td>\n        \t<td>1.759</td>\n        \t<td>1.987</td>\n        \t<td>2.260</td>\n        \t<td>2.345</td>\n        \t<td>2.415</td>\n    \t</tr>\n    </tbody>\n</table>\n\n\n## Supported Hardware\nDue to the limited types of GPUs available to us, we may not be able to provide highly accurate hardware specifications. We've tested the model on GPUs such as the 3090, 4090, A6000, H800, A100, and even the 4060 with 8GB of VRAM (suitable for deploying quantized 3B model and 1.2B model). We are very grateful for the feedback and contributions from the open-source community, who have also successfully run the model on [50-series GPUs](https://github.com/Yuliang-Liu/MonkeyOCR/issues/90), [H200](https://github.com/Yuliang-Liu/MonkeyOCR/issues/151), [L20](https://github.com/Yuliang-Liu/MonkeyOCR/issues/133), [V100](https://github.com/Yuliang-Liu/MonkeyOCR/issues/144), [2080 Ti](https://github.com/Yuliang-Liu/MonkeyOCR/pull/1) and [npu](https://github.com/Yuliang-Liu/MonkeyOCR/pull/226/files).\n\n\n## News \n* ```2025.07.10 ``` üöÄ We release [MonkeyOCR-pro-1.2B](https://huggingface.co/echo840/MonkeyOCR-pro-1.2B), ‚Äî a leaner and faster version model that outperforms our previous 3B version in accuracy, speed, and efficiency.\n* ```2025.06.12 ``` üöÄ The model‚Äôs trending on [Hugging Face](https://huggingface.co/models?sort=trending). Thanks for the love!\n* ```2025.06.05 ``` üöÄ We release [MonkeyOCR](https://huggingface.co/echo840/MonkeyOCR), an English and Chinese documents parsing model.\n\n\n# Quick Start\n## Locally Install\n### 1. Install MonkeyOCR\nSee the [installation guide](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda_pp.md#install-with-cuda-support) to set up your environment.\n### 2. Download Model Weights\nDownload our model from Huggingface.\n```python\npip install huggingface_hub\n\npython tools/download_model.py -n MonkeyOCR-pro-3B # or MonkeyOCR-pro-1.2B, MonkeyOCR\n```\nYou can also download our model from ModelScope.\n\n```python\npip install modelscope\n\npython tools/download_model.py -t modelscope -n MonkeyOCR-pro-3B  # or MonkeyOCR-pro-1.2B, MonkeyOCR\n```\n### 3. Inference\nYou can parse a file or a directory containing PDFs or images using the following commands:\n```bash\n# Replace input_path with the path to a PDF or image or directory\n\n# End-to-end parsing\npython parse.py input_path\n\n# Parse files in a dir with specific group page num\npython parse.py input_path -g 20\n\n# Single-task recognition (outputs markdown only)\npython parse.py input_path -t text/formula/table\n\n# Parse PDFs in input_path and split results by pages\npython parse.py input_path -s\n\n# Specify output directory and model config file\npython parse.py input_path -o ./output -c config.yaml\n```\n\n<details>\n<summary><b>More usage examples</b></summary>\n\n```bash\n# Single file processing\npython parse.py input.pdf                           # Parse single PDF file\npython parse.py input.pdf -o ./output               # Parse with custom output dir\npython parse.py input.pdf -s                        # Parse PDF with page splitting\npython parse.py image.jpg                           # Parse single image file\n\n# Single task recognition\npython parse.py image.jpg -t text                   # Text recognition from image\npython parse.py image.jpg -t formula                # Formula recognition from image\npython parse.py image.jpg -t table                  # Table recognition from image\npython parse.py document.pdf -t text                # Text recognition from all PDF pages\n\n# Folder processing (all files individually)\npython parse.py /path/to/folder                     # Parse all files in folder\npython parse.py /path/to/folder -s                  # Parse with page splitting\npython parse.py /path/to/folder -t text             # Single task recognition for all files\n\n# Multi-file grouping (batch processing by page count)\npython parse.py /path/to/folder -g 5                # Group files with max 5 total pages\npython parse.py /path/to/folder -g 10 -s            # Group files with page splitting\npython parse.py /path/to/folder -g 8 -t text        # Group files for single task recognition\n\n# Advanced configurations\npython parse.py input.pdf -c model_configs.yaml     # Custom model configuration\npython parse.py /path/to/folder -g 15 -s -o ./out   # Group files, split pages, custom output\npython parse.py input.pdf --pred-abandon            # Enable predicting abandon elements\n  python parse.py /path/to/folder -g 10 -m            # Group files and merge text blocks in output\n```\n\n</details>\n\n<details>\n<summary><b>Output Results</b></summary>\n\nMonkeyOCR mainly generates three types of output files:\n\n1. **Processed Markdown File** (`your.md`): The final parsed document content in markdown format, containing text, formulas, tables, and other structured elements.\n2. **Layout Results** (`your_layout.pdf`): The layout results drawed on origin PDF.\n2. **Intermediate Block Results** (`your_middle.json`): A JSON file containing detailed information about all detected blocks, including:\n   - Block coordinates and positions\n   - Block content and type information\n   - Relationship information between blocks\n\nThese files provide both the final formatted output and detailed intermediate results for further analysis or processing.\n\n</details>\n\n### 4. Gradio Demo\n```bash\npython demo/demo_gradio.py\n```\nOnce the demo is running, you can access it at http://localhost:7860.\n\n### 5. Fast API\nYou can start the MonkeyOCR FastAPI service with the following command:\n```bash\nuvicorn api.main:app --port 8000\n```\nOnce the API service is running, you can access the API documentation at http://localhost:8000/docs to explore available endpoints.\n> [!TIP]\n> To improve API concurrency performance, consider configuring the inference backend as `vllm_async`.\n\n## Docker Deployment\n\n1. Navigate to the `docker` directory:\n\n   ```bash\n   cd docker\n   ```\n\n2. **Prerequisite:** Ensure NVIDIA GPU support is available in Docker (via `nvidia-docker2`).\n   If GPU support is not enabled, run the following to set up the environment:\n\n   ```bash\n   bash env.sh\n   ```\n\n3. Build the Docker image:\n\n   ```bash\n   docker compose build monkeyocr\n   ```\n\n> [!IMPORTANT]\n>\n> If your GPU is from the 20/30/40-series, V100, L20/L40 or similar, please build the patched Docker image for LMDeploy compatibility:\n>\n> ```bash\n> docker compose build monkeyocr-fix\n> ```\n>\n> Otherwise, you may encounter the following error: `triton.runtime.errors.OutOfResources: out of resource: shared memory`\n\n4. Run the container with the Gradio demo (accessible on port 7860):\n\n   ```bash\n   docker compose up monkeyocr-demo\n   ```\n\n   Alternatively, start an interactive development environment:\n\n   ```bash\n   docker compose run --rm monkeyocr-dev\n   ```\n\n5. Run the FastAPI service (accessible on port 7861):\n   ```bash\n   docker compose up monkeyocr-api\n   ```\n   Once the API service is running, you can access the API documentation at http://localhost:7861/docs to explore available endpoints.\n\n## Windows Support \n\nSee the [windows support guide](docs/windows_support.md) for details.\n\n## Quantization\n\nThis model can be quantized using AWQ. Follow the instructions in the [quantization guide](docs/Quantization.md).\n\n## Benchmark Results\n\nHere are the evaluation results of our model on OmniDocBench. MonkeyOCR-3B uses DocLayoutYOLO as the structure detection model, while MonkeyOCR-3B* uses our trained structure detection model with improved Chinese performance.\n\n### 1. The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan=\"2\"><strong>Model<br>Type</strong></th>\n<th rowspan=\"2\"><strong>Methods</strong></th>\n<th colspan=\"2\"><strong>Overall<sup>Edit</sup>‚Üì</strong></th>\n<th colspan=\"2\"><strong>Text<sup>Edit</sup>‚Üì</strong></th>\n<th colspan=\"2\"><strong>Formula<sup>Edit</sup>‚Üì</strong></th>\n<th colspan=\"2\"><strong>Table<sup>TEDS</sup>‚Üë</strong></th>\n<th colspan=\"2\"><strong>Table<sup>Edit</sup>‚Üì</strong></th>\n<th colspan=\"2\"><strong>Read Order<sup>Edit</sup>‚Üì</strong></th>\n</tr>\n<tr>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan=\"8\"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.150</td>\n<td>0.357</td>\n<td>0.061</td>\n<td>0.215</td>\n<td>0.278</td>\n<td>0.577</td>\n<td>78.6</td>\n<td>62.1</td>\n<td>0.180</td>\n<td>0.344</td>\n<td>0.079</td>\n<td>0.292</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.336</td>\n<td>0.556</td>\n<td>0.080</td>\n<td>0.315</td>\n<td>0.530</td>\n<td>0.883</td>\n<td>67.6</td>\n<td>49.2</td>\n<td>0.619</td>\n<td>0.685</td>\n<td>0.114</td>\n<td>0.340</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.191</td>\n<td>0.365</td>\n<td>0.105</td>\n<td>0.384</td>\n<td>0.306</td>\n<td><strong>0.454</strong></td>\n<td>77.0</td>\n<td>67.1</td>\n<td>0.243</td>\n<td>0.320</td>\n<td>0.108</td>\n<td>0.304</td>\n</tr>\n<tr>\n<td>Docling</td>\n<td>0.589</td>\n<td>0.909</td>\n<td>0.416</td>\n<td>0.987</td>\n<td>0.999</td>\n<td>1</td>\n<td>61.3</td>\n<td>25.0</td>\n<td>0.627</td>\n<td>0.810</td>\n<td>0.313</td>\n<td>0.837</td>\n</tr>\n<tr>\n<td>Pix2Text</td>\n<td>0.320</td>\n<td>0.528</td>\n<td>0.138</td>\n<td>0.356</td>\n<td>0.276</td>\n<td>0.611</td>\n<td>73.6</td>\n<td>66.2</td>\n<td>0.584</td>\n<td>0.645</td>\n<td>0.281</td>\n<td>0.499</td>\n</tr>\n<tr>\n<td>Unstructured</td>\n<td>0.586</td>\n<td>0.716</td>\n<td>0.198</td>\n<td>0.481</td>\n<td>0.999</td>\n<td>1</td>\n<td>0</td>\n<td>0.06</td>\n<td>1</td>\n<td>0.998</td>\n<td>0.145</td>\n<td>0.387</td>\n</tr>\n<tr>\n<td>OpenParse</td>\n<td>0.646</td>\n<td>0.814</td>\n<td>0.681</td>\n<td>0.974</td>\n<td>0.996</td>\n<td>1</td>\n<td>64.8</td>\n<td>27.5</td>\n<td>0.284</td>\n<td>0.639</td>\n<td>0.595</td>\n<td>0.641</td>\n</tr>\n<tr>\n<td>PP-StructureV3</td>\n<td>0.145</td>\n<td><strong>0.206</strong></td>\n<td>0.058</td>\n<td><strong>0.088</strong></td>\n<td>0.295</td>\n<td>0.535</td>\n<td>-</td>\n<td>-</td>\n<td>0.159</td>\n<td><strong>0.109</strong></td>\n<td><strong>0.069</strong></td>\n<td><strong>0.091</strong></td>\n</tr>\n<tr>\n<td rowspan=\"8\"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.287</td>\n<td>0.411</td>\n<td>0.189</td>\n<td>0.315</td>\n<td>0.360</td>\n<td>0.528</td>\n<td>53.2</td>\n<td>47.2</td>\n<td>0.459</td>\n<td>0.520</td>\n<td>0.141</td>\n<td>0.280</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.452</td>\n<td>0.973</td>\n<td>0.365</td>\n<td>0.998</td>\n<td>0.488</td>\n<td>0.941</td>\n<td>39.9</td>\n<td>0</td>\n<td>0.572</td>\n<td>1.000</td>\n<td>0.382</td>\n<td>0.954</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>0.268</td>\n<td>0.439</td>\n<td>0.072</td>\n<td>0.325</td>\n<td>0.318</td>\n<td>0.495</td>\n<td>75.8</td>\n<td>63.6</td>\n<td>0.600</td>\n<td>0.650</td>\n<td>0.083</td>\n<td>0.284</td>\n</tr>\n<tr>\n<td>OLMOCR-sglang</td>\n<td>0.326</td>\n<td>0.469</td>\n<td>0.097</td>\n<td>0.293</td>\n<td>0.455</td>\n<td>0.655</td>\n<td>68.1</td>\n<td>61.3</td>\n<td>0.608<td>0.652</td>\n<td>0.145</td>\n<td>0.277</td>\n</tr>\n<tr>\n<td>SmolDocling-256M</td>\n<td>0.493</td>\n<td>0.816</td>\n<td>0.262</td>\n<td>0.838</td>\n<td>0.753</td>\n<td>0.997</td>\n<td>44.9</td>\n<td>16.5</td>\n<td>0.729</td>\n<td>0.907</td>\n<td>0.227</td>\n<td>0.522</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.206</td>\n<td>0.306</td>\n<td>0.107</td>\n<td>0.197</td>\n<td>0.447</td>\n<td>0.580</td>\n<td>77.3</td>\n<td>67.2</td>\n<td>0.180</td>\n<td>0.285</td>\n<td>0.091</td>\n<td>0.162</td>\n</tr>\n<tr>\n<td>MinerU 2</td>\n<td>0.139</td>\n<td>0.240</td>\n<td><strong>0.047</strong></td>\n<td>0.109</td>\n<td>0.297</td>\n<td>0.536</td>\n<td><strong>82.5</strong></td>\n<td>79.0</td>\n<td>0.141</td>\n<td>0.195</td>\n<td><strong>0.069</strong></td>\n<td>0.118</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n\t\n<td>0.195</td>\n<td>0.281</td>\n<td>0.064</td>\n<td>0.183</td>\n<td>0.379</td>\n<td>0.613</td>\n<td>71.6</td>\n<td>81.3</td>\n<td>0.253</td>\n<td>0.139</td>\n<td>0.086</td>\n<td>0.187</td>\n\n\n</tr>\n<tr>\n<td rowspan=\"3\"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.233</td>\n<td>0.399</td>\n<td>0.144</td>\n<td>0.409</td>\n<td>0.425</td>\n<td>0.606</td>\n<td>72.0</td>\n<td>62.9</td>\n<td>0.234</td>\n<td>0.329</td>\n<td>0.128</td>\n<td>0.251</td>\n</tr>\n<tr>\n<td>Qwen2.5-VL-7B</td>\n<td>0.312</td>\n<td>0.406</td>\n<td>0.157</td>\n<td>0.228</td>\n<td>0.351</td>\n<td>0.574</td>\n<td>76.4</td>\n<td>72.2</td>\n<td>0.588</td>\n<td>0.619</td>\n<td>0.149</td>\n<td>0.203</td>\n</tr>\n<tr>\n<td>InternVL3-8B</td>\n<td>0.314</td>\n<td>0.383</td>\n<td>0.134</td>\n<td>0.218</td>\n<td>0.417</td>\n<td>0.563</td>\n<td>66.1</td>\n<td>73.1</td>\n<td>0.586</td>\n<td>0.564</td>\n<td>0.118</td>\n<td>0.186</td>\n</tr>\n<tr>\n<td rowspan=\"4\"><strong>Mix</strong></td>\n<td><strong>MonkeyOCR-3B <a href=\"https://huggingface.co/echo840/MonkeyOCR/blob/main/Structure/doclayout_yolo_docstructbench_imgsz1280_2501.pt\">[Weight]</a></strong></td>\n<td>0.140</td>\n<td>0.297</td>\n<td>0.058</td>\n<td>0.185</td>\n<td>0.238</td>\n<td>0.506</td>\n<td>80.2</td>\n<td>77.7</td>\n<td>0.170</td>\n<td>0.253</td>\n<td>0.093</td>\n<td>0.244</td>\n</tr>\n<tr>\n<td><strong>MonkeyOCR-3B* <a href=\"https://huggingface.co/echo840/MonkeyOCR/blob/main/Structure/layout_zh.pt\">[Weight]</a></strong></td>\n<td>0.154</td>\n<td>0.277</td>\n<td>0.073</td>\n<td>0.134</td>\n<td>0.255</td>\n<td>0.529</td>\n<td>78.2</td>\n<td>76.2</td>\n<td>0.182</td>\n<td>0.262</td>\n<td>0.105</td>\n<td>0.183</td>\n</tr>\n<tr>\n<td><strong>MonkeyOCR-pro-3B <a href=\"https://huggingface.co/echo840/MonkeyOCR-pro-3B\">[Weight]</a></strong></td>\n<td><strong>0.138</strong></td>\n<td><strong>0.206</strong></td>\n<td>0.067</td>\n<td>0.107</td>\n<td><strong>0.246</strong></td>\n<td><strong>0.421</strong></td>\n<td>81.5</td>\n<td><strong>87.5</strong></td>\n<td><strong>0.139</strong></td>\n<td>0.111</td>\n<td>0.100</td>\n<td>0.185</td>\n</tr>\n<tr>\n<td><strong>MonkeyOCR-pro-1.2B <a href=\"https://huggingface.co/echo840/MonkeyOCR-pro-1.2B\">[Weight]</a></strong></td>\n<td>0.153</td>\n<td>0.223</td>\n<td>0.066</td>\n<td>0.123</td>\n<td>0.272</td>\n<td>0.449</td>\n<td>76.5</td>\n<td>83.7</td>\n<td>0.176</td>\n<td>0.131</td>\n<td>0.097</td>\n<td>0.187</td>\n</tr>\n</tbody>\n</table>\n\n\n### 2. The end-to-end text recognition performance across 9 PDF page types.\n\n<table>\n<thead>\n<tr>\n<th><strong>Model<br>Type</strong></th>\n<th><strong>Models</strong></th>\n<th><strong>Book</strong></th>\n<th><strong>Slides</strong></th>\n<th><strong>Financial<br>Report</strong></th>\n<th><strong>Textbook</strong></th>\n<th><strong>Exam<br>Paper</strong></th>\n<th><strong>Magazine</strong></th>\n<th><strong>Academic<br>Papers</strong></th>\n<th><strong>Notes</strong></th>\n<th><strong>Newspaper</strong></th>\n<th><strong>Overall</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan=\"3\"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.055</td>\n<td>0.124</td>\n<td><u>0.033</u></td>\n<td>0.102</td>\n<td>0.159</td>\n<td><strong>0.072</strong></td>\n<td><u>0.025</u></td>\n<td>0.984</td>\n<td>0.171</td>\n<td>0.206</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.074</td>\n<td>0.340</td>\n<td>0.089</td>\n<td>0.319</td>\n<td>0.452</td>\n<td>0.153</td>\n<td>0.059</td>\n<td>0.651</td>\n<td>0.192</td>\n<td>0.274</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.131</td>\n<td>0.220</td>\n<td>0.202</td>\n<td>0.216</td>\n<td>0.278</td>\n<td>0.147</td>\n<td>0.091</td>\n<td>0.634</td>\n<td>0.690</td>\n<td>0.300</td>\n</tr>\n<tr>\n<td rowspan=\"4\"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.111</td>\n<td>0.222</td>\n<td>0.067</td>\n<td>0.132</td>\n<td>0.204</td>\n<td>0.198</td>\n<td>0.179</td>\n<td>0.388</td>\n<td>0.771</td>\n<td>0.267</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.734</td>\n<td>0.958</td>\n<td>1.000</td>\n<td>0.820</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.214</td>\n<td>0.991</td>\n<td>0.871</td>\n<td>0.806</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.091</td>\n<td>0.131</td>\n<td>0.057</td>\n<td>0.146</td>\n<td>0.231</td>\n<td>0.121</td>\n<td>0.074</td>\n<td>0.363</td>\n<td>0.307</td>\n<td>0.177</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.068</td>\n<td>0.125</td>\n<td>0.092</td>\n<td>0.102</td>\n<td>0.119</td>\n<td>0.083</td>\n<td>0.047</td>\n<td>0.223</td>\n<td>0.536</td>\n<td>0.149</td>\n</tr>\n<tr>\n<td rowspan=\"3\"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.157</td>\n<td>0.163</td>\n<td>0.348</td>\n<td>0.187</td>\n<td>0.281</td>\n<td>0.173</td>\n<td>0.146</td>\n<td>0.607</td>\n<td>0.751</td>\n<td>0.316</td>\n</tr>\n<tr>\n<td>Qwen2.5-VL-7B</td>\n<td>0.148</td>\n<td><strong>0.053</strong></td>\n<td>0.111</td>\n<td>0.137</td>\n<td>0.189</td>\n<td>0.117</td>\n<td>0.134</td>\n<td>0.204</td>\n<td>0.706</td>\n<td>0.205</td>\n</tr>\n<tr>\n<td>InternVL3-8B</td>\n<td>0.163</td>\n<td><u>0.056</u></td>\n<td>0.107</td>\n<td>0.109</td>\n<td>0.129</td>\n<td>0.100</td>\n<td>0.159</td>\n<td><strong>0.150</strong></td>\n<td>0.681</td>\n<td>0.188</td>\n</tr>\n<tr>\n<td rowspan=\"4\"><strong>Mix</strong></td>\n<td><strong>MonkeyOCR-3B <a href=\"https://huggingface.co/echo840/MonkeyOCR/blob/main/Structure/doclayout_yolo_docstructbench_imgsz1280_2501.pt\">[Weight]</a></strong></td>\n<td><strong>0.046</strong></td>\n<td>0.120</td>\n<td><strong>0.024</strong></td>\n<td>0.100</td>\n<td>0.129</td>\n<td>0.086</td>\n<td><strong>0.024</strong></td>\n<td>0.643</td>\n<td><u>0.131</u></td>\n<td>0.155</td>\n</tr>\n<tr>\n<td><strong>MonkeyOCR-3B* <a href=\"https://huggingface.co/echo840/MonkeyOCR/blob/main/Structure/layout_zh.pt\">[Weight]</a></strong></td>\n<td><u>0.054</u></td>\n<td>0.203</td>\n<td>0.038</td>\n<td>0.112</td>\n<td>0.138</td>\n<td>0.111</td>\n<td>0.032</td>\n<td>0.194</td>\n<td>0.136</td>\n<td>0.120</td>\n</tr>\n<tr>\n<td><strong>MonkeyOCR-pro-3B <a href=\"https://huggingface.co/echo840/MonkeyOCR-pro-3B\">[Weight]</a></strong></td>\n<td>0.084</td>\n<td>0.129</td>\n<td>0.060</td>\n<td><strong>0.090</strong></td>\n<td><strong>0.107</strong></td>\n<td><u>0.073</u></td>\n<td>0.050</td>\n<td><u>0.171</u></td>\n<td><strong>0.107</strong></td>\n<td><strong>0.100</strong></td>\n</tr>\n<tr>\n<td><strong>MonkeyOCR-pro-1.2B <a href=\"https://huggingface.co/echo840/MonkeyOCR-pro-1.2B\">[Weight]</a></strong></td>\n<td>0.087</td>\n<td>0.142</td>\n<td>0.059</td>\n<td><u>0.093</u></td>\n<td><u>0.115</u></td>\n<td>0.085</td>\n<td>0.045</td>\n<td>0.226</td>\n<td>0.122</td>\n<td><u>0.112</u></td>\n</tr>\n</tbody>\n</table>\n\n### 3. The evaluation results of olmOCR-bench.\n\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>ArXiv</th>\n<th>Old Scans<br>Math</th>\n<th>Tables</th>\n<th>Old Scans</th>\n<th>Headers and<br>Footers</th>\n<th>Multi<br>column</th>\n<th>Long Tiny<br>Text</th>\n<th>Base</th>\n<th>Overall</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GOT OCR</td>\n<td>52.7</td>\n<td>52.0</td>\n<td>0.2</td>\n<td>22.1</td>\n<td>93.6</td>\n<td>42.0</td>\n<td>29.9</td>\n<td>94.0</td>\n<td>48.3 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>76.0</td>\n<td>57.9</td>\n<td>57.6</td>\n<td>27.8</td>\n<td>84.9</td>\n<td>72.9</td>\n<td>84.6</td>\n<td><strong>99.1</strong></td>\n<td>70.1 ¬± 1.1</td>\n</tr>\n<tr>\n<td>MinerU</td>\n<td>75.4</td>\n<td>47.4</td>\n<td>60.9</td>\n<td>17.3</td>\n<td><strong>96.6</strong></td>\n<td>59.0</td>\n<td>39.1</td>\n<td>96.6</td>\n<td>61.5 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>77.2</td>\n<td>67.5</td>\n<td>60.6</td>\n<td>29.3</td>\n<td>93.6</td>\n<td>71.3</td>\n<td>77.1</td>\n<td>99.4</td>\n<td>72.0 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Nanonets OCR</td>\n<td>67.0</td>\n<td>68.6</td>\n<td><strong>77.7</strong></td>\n<td>39.5</td>\n<td>40.7</td>\n<td>69.9</td>\n<td>53.4</td>\n<td>99.3</td>\n<td>64.5 ¬± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(No Anchor)</td>\n<td>51.5</td>\n<td><strong>75.5</strong></td>\n<td>69.1</td>\n<td>40.9</td>\n<td>94.2</td>\n<td>68.9</td>\n<td>54.1</td>\n<td>96.7</td>\n<td>68.9 ¬± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(Anchored)</td>\n<td>53.5</td>\n<td>74.5</td>\n<td>70.0</td>\n<td>40.7</td>\n<td>93.8</td>\n<td>69.3</td>\n<td>60.6</td>\n<td>96.8</td>\n<td>69.9 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(No Anchor)</td>\n<td>32.1</td>\n<td>56.3</td>\n<td>61.4</td>\n<td>27.8</td>\n<td>48.0</td>\n<td>58.7</td>\n<td><strong>84.4</strong></td>\n<td>94.0</td>\n<td>57.8 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(Anchored)</td>\n<td>54.5</td>\n<td>56.1</td>\n<td>72.1</td>\n<td>34.2</td>\n<td>64.7</td>\n<td>61.5</td>\n<td>71.5</td>\n<td>95.6</td>\n<td>63.8 ¬± 1.2</td>\n</tr>\n<tr>\n<td>Qwen 2 VL<br>(No Anchor)</td>\n<td>19.7</td>\n<td>31.7</td>\n<td>24.2</td>\n<td>17.1</td>\n<td>88.9</td>\n<td>8.3</td>\n<td>6.8</td>\n<td>55.5</td>\n<td>31.5 ¬± 0.9</td>\n</tr>\n<tr>\n<td>Qwen 2.5 VL<br>(No Anchor)</td>\n<td>63.1</td>\n<td>65.7</td>\n<td>67.3</td>\n<td>38.6</td>\n<td>73.6</td>\n<td>68.3</td>\n<td>49.1</td>\n<td>98.3</td>\n<td>65.5 ¬± 1.2</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(No Anchor)</td>\n<td>71.5</td>\n<td>71.4</td>\n<td>71.4</td>\n<td><strong>42.8</strong></td>\n<td>94.1</td>\n<td>77.7</td>\n<td>71.0</td>\n<td>97.8</td>\n<td>74.7 ¬± 1.1</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(Anchored)</td>\n<td>74.9</td>\n<td>71.2</td>\n<td>71.0</td>\n<td>42.2</td>\n<td>94.5</td>\n<td><strong>78.3</strong></td>\n<td>73.3</td>\n<td>98.3</td>\n<td>75.5 ¬± 1.0</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B <a href=\"https://huggingface.co/echo840/MonkeyOCR-pro-3B\">[Weight]</a></td>\n<td><strong>83.8</strong></td>\n<td>68.8</td>\n<td>74.6</td>\n<td>36.1</td>\n<td>91.2</td>\n<td>76.6</td>\n<td>80.1</td>\n<td>95.3</td>\n<td><strong>75.8 ¬± 1.0</strong></td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-1.2B <a href=\"https://huggingface.co/echo840/MonkeyOCR-pro-1.2B\">[Weight]</a></td>\n<td>80.5</td>\n<td>62.9</td>\n<td>71.1</td>\n<td>32.9</td>\n<td>92.2</td>\n<td>68.3</td>\n<td>74.0</td>\n<td>92.6</td>\n<td>71.8 ¬± 1.1</td>\n</tr>\n</tbody>\n</table>\n\n## Visualization Demo\n\nGet a Quick Hands-On Experience with Our Demo:  http://vlrlabmonkey.xyz:7685 (The latest model is available for selection)\n\n> Our demo is simple and easy to use:\n>\n> 1. Upload a PDF or image.\n> 2. Click ‚ÄúParse (Ëß£Êûê)‚Äù to let the model perform structure detection, content recognition, and relationship prediction on the input document. The final output will be a markdown-formatted version of the document.\n> 3. Select a prompt and click ‚ÄúTest by prompt‚Äù to let the model perform content recognition on the image based on the selected prompt.\n\n\n\n### Support diverse Chinese and English PDF types\n\n<p align=\"center\">\n  <img src=\"asserts/Visualization.GIF?raw=true\" width=\"600\"/>\n</p>\n\n### Example for formula document\n<img src=\"https://v1.ax1x.com/2025/06/10/7jVLgB.jpg\" alt=\"7jVLgB.jpg\" border=\"0\" />\n\n### Example for table document\n<img src=\"https://v1.ax1x.com/2025/06/11/7jcOaa.png\" alt=\"7jcOaa.png\" border=\"0\" />\n\n### Example for newspaper\n<img src=\"https://v1.ax1x.com/2025/06/11/7jcP5V.png\" alt=\"7jcP5V.png\" border=\"0\" />\n\n### Example for financial report\n<img src=\"https://v1.ax1x.com/2025/06/11/7jc10I.png\" alt=\"7jc10I.png\" border=\"0\" />\n<img src=\"https://v1.ax1x.com/2025/06/11/7jcRCL.png\" alt=\"7jcRCL.png\" border=\"0\" />\n\n## Citing MonkeyOCR\n\nIf you wish to refer to the baseline results published here, please use the following BibTeX entries:\n\n```BibTeX\n@misc{li2025monkeyocrdocumentparsingstructurerecognitionrelation,\n      title={MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm}, \n      author={Zhang Li and Yuliang Liu and Qiang Liu and Zhiyin Ma and Ziyang Zhang and Shuo Zhang and Zidun Guo and Jiarui Zhang and Xinyu Wang and Xiang Bai},\n      year={2025},\n      eprint={2506.05218},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2506.05218}, \n}\n```\n\n\n\n## Acknowledgments\nWe would like to thank [MinerU](https://github.com/opendatalab/MinerU), [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO), [PyMuPDF](https://github.com/pymupdf/PyMuPDF), [layoutreader](https://github.com/ppaanngggg/layoutreader), [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL), [LMDeploy](https://github.com/InternLM/lmdeploy), [PP-StructureV3](https://github.com/PaddlePaddle/PaddleOCR), [PP-DocLayout_plus-L](https://huggingface.co/PaddlePaddle/PP-DocLayout_plus-L), and [InternVL3](https://github.com/OpenGVLab/InternVL) for providing base code and models, as well as their contributions to this field. We also thank [M6Doc](https://github.com/HCIILAB/M6Doc), [DocLayNet](https://github.com/DS4SD/DocLayNet), [CDLA](https://github.com/buptlihang/CDLA), [D4LA](https://github.com/AlibabaResearch/AdvancedLiterateMachinery), [DocGenome](https://github.com/Alpha-Innovator/DocGenome), [PubTabNet](https://github.com/ibm-aur-nlp/PubTabNet), and [UniMER-1M](https://github.com/opendatalab/UniMERNet) for providing valuable datasets. We also thank everyone who contributed to this open-source effort.\n\n## Limitation\nCurrently, MonkeyOCR do not yet fully support for photographed text, handwritten content, Traditional Chinese characters, or multilingual text. We plan to consider adding support for these features in future public releases. Additionally, our model is deployed on a single GPU, so if too many users upload files at the same time, issues like ‚ÄúThis application is currently busy‚Äù may occur. The processing time shown on the demo page does not reflect computation time alone‚Äîit also includes result uploading and other overhead. During periods of high traffic, this time may be longer. The inference speeds of MonkeyOCR, MinerU, and Qwen2.5 VL-7B were measured on an H800 GPU.\n\n## Copyright\nPlease don‚Äôt hesitate to share your valuable feedback ‚Äî it‚Äôs a key motivation that drives us to continuously improve our framework. Note: Our model is intended for academic research and non-commercial use only. If you are interested in faster (smaller) or stronger one, please contact us at xbai@hust.edu.cn or ylliu@hust.edu.cn.\n"
  },
  {
    "file_name": "model_configs.yaml",
    "file_contents": "device: cuda # cuda / cpu / mps (using `transformers` as backend)\nweights:\n  doclayout_yolo: Structure/doclayout_yolo_docstructbench_imgsz1280_2501.pt # or Structure/layout_zh.pt\n  PP-DocLayout_plus-L: Structure/PP-DocLayout_plus-L\n  layoutreader: Relation\nmodels_dir: model_weight\nlayout_config: \n  model: PP-DocLayout_plus-L # PP-DocLayout_plus-L (MonkeyOCR-pro) / doclayout_yolo (MonkeyOCR)\n  reader:\n    name: layoutreader\nchat_config:\n  weight_path: model_weight/Recognition\n  backend: lmdeploy # lmdeploy / vllm / transformers / api / lmdeploy_queue / vllm_queue / vllm_async\n  data_parallelism: 1 # for lmdeploy only (test)\n  model_parallelism: 1 # for lmdeploy and vllm\n  batch_size: 10 # active when using `transformers` as backend\n  # if using xxx_queue as backend\n  queue_config:\n    max_batch_size: 256 # maximum batch size for internal processing\n    queue_timeout: 1 # seconds to wait for batching requests\n    max_queue_size: 2000 # maximum requests in queue\n\n# Uncomment the following lines if use `api` as backend \n# api_config:\n#   url: https://api.openai.com/v1\n#   model_name: gpt-4.1\n#   api_key: sk-xxx\n"
  },
  {
    "file_name": "parse.py",
    "file_contents": "import os\nimport time\nimport argparse\nimport sys\nimport traceback\nimport torch.distributed as dist\n\nfrom magic_pdf.utils.load_image import pdf_to_images\nfrom magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\nfrom magic_pdf.data.dataset import PymuDocDataset, ImageDataset, MultiFileDataset\nfrom magic_pdf.model.doc_analyze_by_custom_model_llm import doc_analyze_llm\nfrom magic_pdf.model.custom_model import MonkeyOCR\n\nTASK_INSTRUCTIONS = {\n    'text': 'Please output the text content from the image.',\n    'formula': 'Please write out the expression of the formula in the image using LaTeX format.',\n    'table': 'This is the image of a table. Please output the table in html format.'\n}\n\ndef parse_folder(folder_path, output_dir, config_path, task=None, split_pages=False, group_size=None, pred_abandon=False):\n    \"\"\"\n    Parse all PDF and image files in a folder\n    \n    Args:\n        folder_path: Input folder path\n        output_dir: Output directory\n        config_path: Configuration file path\n        task: Optional task type for single task recognition\n        group_size: Number of files to group together by total page count (None means process individually)\n    \"\"\"\n    print(f\"Starting to parse folder: {folder_path}\")\n    \n    # Record start time for total processing time\n    total_start_time = time.time()\n    \n    # Check if folder exists\n    if not os.path.exists(folder_path):\n        raise FileNotFoundError(f\"Folder does not exist: {folder_path}\")\n    \n    if not os.path.isdir(folder_path):\n        raise ValueError(f\"Path is not a directory: {folder_path}\")\n    \n    # Find all supported files\n    supported_extensions = {'.pdf', '.jpg', '.jpeg', '.png'}\n    all_files = []\n    \n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_ext = os.path.splitext(file)[1].lower()\n            if file_ext in supported_extensions:\n                all_files.append(file_path)\n    \n    all_files.sort()\n    \n    # Initialize model once for all files\n    print(\"Loading model...\")\n    MonkeyOCR_model = MonkeyOCR(config_path)\n    \n    successful_files = []\n    failed_files = []\n    \n    if group_size and group_size > 1:\n        # Group files by total page count\n        print(f\"Found {len(all_files)} files to process in groups with max {group_size} total pages\")\n        \n        file_groups = create_file_groups_by_page_count(all_files, group_size)\n        print(f\"Created {len(file_groups)} file groups\")\n        \n        for i, file_group in enumerate(file_groups, 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Processing file group {i}/{len(file_groups)} (contains {len(file_group)} files)\")\n            for file_path in file_group:\n                print(f\"  - {os.path.basename(file_path)}\")\n            print(f\"{'='*60}\")\n            \n            try:\n                if task:\n                    result_dir = single_task_recognition_multi_file_group(file_group, output_dir, MonkeyOCR_model, task, folder_path)\n                else:\n                    result_dir = parse_multi_file_group(file_group, output_dir, MonkeyOCR_model, folder_path, split_pages, pred_abandon)\n\n                successful_files.extend(file_group)\n                print(f\"‚úÖ Successfully processed file group {i}\")\n                \n            except Exception as e:\n                failed_files.extend([(path, str(e)) for path in file_group])\n                print(f\"‚ùå Failed to process file group {i}: {str(e)}\")\n    else:\n        # Process files individually\n        print(f\"Found {len(all_files)} files to process individually:\")\n        for file_path in all_files:\n            print(f\"  - {file_path}\")\n        \n        for i, file_path in enumerate(all_files, 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Processing file {i}/{len(all_files)}: {os.path.basename(file_path)}\")\n            print(f\"{'='*60}\")\n            \n            try:\n                if task:\n                    result_dir = single_task_recognition(file_path, output_dir, MonkeyOCR_model, task)\n                else:\n                    result_dir = parse_file(file_path, output_dir, MonkeyOCR_model, pred_abandon=pred_abandon)\n                \n                successful_files.append(file_path)\n                print(f\"‚úÖ Successfully processed: {os.path.basename(file_path)}\")\n                \n            except Exception as e:\n                failed_files.append((file_path, str(e)))\n                print(f\"‚ùå Failed to process {os.path.basename(file_path)}: {str(e)}\")\n    \n    if not all_files:\n        print(\"No supported files found in the folder.\")\n        return\n    \n    # Calculate total processing time\n    total_processing_time = time.time() - total_start_time\n    \n    # Summary\n    total_files = len(all_files)\n    print(f\"\\n{'='*60}\")\n    print(\"PROCESSING SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Total files: {total_files}\")\n    print(f\"Successful: {len(successful_files)}\")\n    print(f\"Failed: {len(failed_files)}\")\n    print(f\"Total processing time: {total_processing_time:.2f}s\")\n    \n    if failed_files:\n        print(\"\\nFailed files:\")\n        for file_path, error in failed_files:\n            print(f\"  - {os.path.basename(file_path)}: {error}\")\n    \n    return output_dir\n\ndef create_file_groups_by_page_count(file_paths, max_pages_per_group):\n    \"\"\"\n    Create file groups based on total page count limit\n    \n    Args:\n        file_paths: List of file paths\n        max_pages_per_group: Maximum total pages per group\n        \n    Returns:\n        List of file groups\n    \"\"\"\n    import fitz\n    \n    groups = []\n    current_group = []\n    current_page_count = 0\n    \n    for file_path in file_paths:\n        try:\n            # Get page count for this file\n            file_ext = os.path.splitext(file_path)[1].lower()\n            if file_ext == '.pdf':\n                with fitz.open(file_path) as doc:\n                    file_page_count = len(doc)\n            else:\n                # Images have 1 page\n                file_page_count = 1\n            \n            # Check if adding this file would exceed the limit\n            if current_page_count + file_page_count > max_pages_per_group and current_group:\n                # Start a new group\n                groups.append(current_group)\n                current_group = [file_path]\n                current_page_count = file_page_count\n            else:\n                # Add to current group\n                current_group.append(file_path)\n                current_page_count += file_page_count\n                \n        except Exception as e:\n            print(f\"Warning: Could not determine page count for {file_path}: {e}\")\n            # Treat as 1 page if we can't determine\n            if current_page_count + 1 > max_pages_per_group and current_group:\n                groups.append(current_group)\n                current_group = [file_path]\n                current_page_count = 1\n            else:\n                current_group.append(file_path)\n                current_page_count += 1\n    \n    # Add the last group if not empty\n    if current_group:\n        groups.append(current_group)\n    \n    return groups\n\ndef parse_multi_file_group(file_paths, output_dir, MonkeyOCR_model, base_folder_path, split_pages=False, pred_abandon=False):\n    \"\"\"\n    Parse a group of mixed PDF and image files using MultiFileDataset\n    \n    Args:\n        file_paths: List of file paths (PDF and images)\n        output_dir: Output directory\n        MonkeyOCR_model: Pre-initialized model instance\n        base_folder_path: Base folder path for maintaining relative structure\n        split_pages: Whether to further split each file's results by pages\n    \"\"\"\n    print(f\"Starting to parse multi-file group with {len(file_paths)} files\")\n    \n    # Read all files and collect extensions\n    reader = FileBasedDataReader()\n    file_bytes_list = []\n    file_extensions = []\n    \n    for file_path in file_paths:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"File does not exist: {file_path}\")\n        \n        file_bytes = reader.read(file_path)\n        file_bytes_list.append(file_bytes)\n        \n        # Extract file extension\n        file_ext = os.path.splitext(file_path)[1].lower()\n        file_extensions.append(file_ext)\n    \n    # Create MultiFileDataset with file extensions\n    ds = MultiFileDataset(file_bytes_list, file_extensions)\n    \n    # Start inference with split_files=True to get individual file results\n    print(\"Performing document parsing on multi-file group...\")\n    start_time = time.time()\n\n    infer_result = ds.apply(doc_analyze_llm, MonkeyOCR_model=MonkeyOCR_model, split_files=True, split_pages=split_pages, pred_abandon=pred_abandon)\n\n    # Process each file result separately using original file names\n    for file_idx, (file_infer_result, file_path) in enumerate(zip(infer_result, file_paths)):\n        # Get original file name without extension\n        file_name = '.'.join(os.path.basename(file_path).split(\".\")[:-1])\n        \n        # Maintain relative path structure from base folder\n        rel_path = os.path.relpath(os.path.dirname(file_path), base_folder_path)\n        \n        # Create output directory for this specific file\n        if rel_path == '.':\n            file_local_md_dir = os.path.join(output_dir, file_name)\n        else:\n            file_local_md_dir = os.path.join(output_dir, rel_path, file_name)\n        \n        file_local_image_dir = os.path.join(file_local_md_dir, \"images\")\n        image_dir = os.path.basename(file_local_image_dir)\n        \n        # Create file-specific directories\n        os.makedirs(file_local_image_dir, exist_ok=True)\n        os.makedirs(file_local_md_dir, exist_ok=True)\n        \n        print(f\"Processing file {file_idx + 1}/{len(infer_result)}: {file_name} - Output dir: {file_local_md_dir}\")\n        \n        # Handle split_pages case where file_infer_result might be a list\n        if isinstance(file_infer_result, list):\n            # Process each page result separately for this file\n            for page_idx, page_infer_result in enumerate(file_infer_result):\n                page_dir_name = f\"page_{page_idx}\"\n                page_local_image_dir = os.path.join(file_local_md_dir, page_dir_name, \"images\")\n                page_local_md_dir = os.path.join(file_local_md_dir, page_dir_name)\n                page_image_dir = os.path.basename(page_local_image_dir)\n                \n                # Create page-specific directories\n                os.makedirs(page_local_image_dir, exist_ok=True)\n                os.makedirs(page_local_md_dir, exist_ok=True)\n                \n                # Create page-specific writers\n                page_image_writer = FileBasedDataWriter(page_local_image_dir)\n                page_md_writer = FileBasedDataWriter(page_local_md_dir)\n                \n                # Pipeline processing for this page\n                page_pipe_result = page_infer_result.pipe_ocr_mode(page_image_writer, MonkeyOCR_model=MonkeyOCR_model)\n                \n                # Save page-specific results\n                page_infer_result.draw_model(os.path.join(page_local_md_dir, f\"{file_name}_page_{page_idx}_model.pdf\"))\n                page_pipe_result.draw_layout(os.path.join(page_local_md_dir, f\"{file_name}_page_{page_idx}_layout.pdf\"))\n                page_pipe_result.draw_span(os.path.join(page_local_md_dir, f\"{file_name}_page_{page_idx}_spans.pdf\"))\n                page_pipe_result.dump_md(page_md_writer, f\"{file_name}_page_{page_idx}.md\", page_image_dir)\n                page_pipe_result.dump_content_list(page_md_writer, f\"{file_name}_page_{page_idx}_content_list.json\", page_image_dir)\n                page_pipe_result.dump_middle_json(page_md_writer, f'{file_name}_page_{page_idx}_middle.json')\n        else:\n            # Create file-specific writers\n            file_image_writer = FileBasedDataWriter(file_local_image_dir)\n            file_md_writer = FileBasedDataWriter(file_local_md_dir)\n            \n            # Pipeline processing for this file\n            file_pipe_result = file_infer_result.pipe_ocr_mode(file_image_writer, MonkeyOCR_model=MonkeyOCR_model)\n            \n            # Save file-specific results using original file name\n            file_infer_result.draw_model(os.path.join(file_local_md_dir, f\"{file_name}_model.pdf\"))\n            file_pipe_result.draw_layout(os.path.join(file_local_md_dir, f\"{file_name}_layout.pdf\"))\n            file_pipe_result.draw_span(os.path.join(file_local_md_dir, f\"{file_name}_spans.pdf\"))\n            file_pipe_result.dump_md(file_md_writer, f\"{file_name}.md\", image_dir)\n            file_pipe_result.dump_content_list(file_md_writer, f\"{file_name}_content_list.json\", image_dir)\n            file_pipe_result.dump_middle_json(file_md_writer, f'{file_name}_middle.json')\n\n    parsing_time = time.time() - start_time\n    print(f\"Parsing and saving time: {parsing_time:.2f}s\")\n    \n    print(f\"All {len(infer_result)} files processed and saved in separate directories\")\n    \n    # Return the base directory containing all individual file results\n    return output_dir\n\ndef single_task_recognition_multi_file_group(file_paths, output_dir, MonkeyOCR_model, task, base_folder_path):\n    \"\"\"\n    Single task recognition for a group of mixed PDF and image files\n    \n    Args:\n        file_paths: List of file paths (PDF and images)\n        output_dir: Output directory\n        MonkeyOCR_model: Pre-initialized model instance\n        task: Task type ('text', 'formula', 'table')\n        base_folder_path: Base folder path for maintaining relative structure\n    \"\"\"\n    print(f\"Starting single task recognition: {task} for multi-file group with {len(file_paths)} files\")\n    \n    # Get task instruction\n    instruction = TASK_INSTRUCTIONS.get(task, TASK_INSTRUCTIONS['text'])\n    \n    # Process each file separately for single task recognition\n    for file_idx, file_path in enumerate(file_paths):\n        file_name = '.'.join(os.path.basename(file_path).split(\".\")[:-1])\n        \n        # Maintain relative path structure from base folder\n        rel_path = os.path.relpath(os.path.dirname(file_path), base_folder_path)\n        if rel_path == '.':\n            local_md_dir = os.path.join(output_dir, file_name)\n        else:\n            local_md_dir = os.path.join(output_dir, rel_path, file_name)\n        \n        os.makedirs(local_md_dir, exist_ok=True)\n        \n        print(f\"Processing file {file_idx + 1}/{len(file_paths)}: {file_name} - Output dir: {local_md_dir}\")\n        md_writer = FileBasedDataWriter(local_md_dir)\n        \n        # Load images for this file\n        file_extension = file_path.split(\".\")[-1].lower()\n        images = []\n        \n        if file_extension == 'pdf':\n            try:\n                # Convert PDF pages to PIL images directly\n                print(f\"Converting PDF pages to images for {file_name}...\")\n                images = pdf_to_images(file_path)\n                print(f\"Converted {len(images)} pages to images\")\n            except Exception as e:\n                raise RuntimeError(f\"Failed to convert PDF to images: {str(e)}\")\n        elif file_extension in ['jpg', 'jpeg', 'png']:\n            # Load single image\n            from PIL import Image\n            images = [Image.open(file_path)]\n        else:\n            print(f\"Skipping unsupported file: {file_path}\")\n            continue\n        \n        # Start recognition for this file\n        print(f\"Performing {task} recognition on {len(images)} image(s) from {file_name}...\")\n        start_time = time.time()\n        \n        try:\n            # Prepare instructions for all images\n            instructions = [instruction] * len(images)\n            \n            # Use chat model for single task recognition with PIL images directly\n            responses = MonkeyOCR_model.chat_model.batch_inference(images, instructions)\n            \n            recognition_time = time.time() - start_time\n            print(f\"Recognition time for {file_name}: {recognition_time:.2f}s\")\n            \n            # Combine results\n            combined_result = responses[0]\n            for i, response in enumerate(responses):\n                if i > 0:\n                    combined_result = combined_result + \"\\n\\n\" + response\n            \n            # Save result\n            result_filename = f\"{file_name}_{task}_result.md\"\n            md_writer.write(result_filename, combined_result.encode('utf-8'))\n            \n            print(f\"File {file_name} {task} recognition completed!\")\n            print(f\"Result saved to: {os.path.join(local_md_dir, result_filename)}\")\n            \n            # Clean up resources for this file\n            try:\n                for img in images:\n                    if hasattr(img, 'close'):\n                        img.close()\n            except Exception as cleanup_error:\n                print(f\"Warning: Error during cleanup for {file_name}: {cleanup_error}\")\n                \n        except Exception as e:\n            raise RuntimeError(f\"Single task recognition failed for {file_name}: {str(e)}\")\n    \n    return output_dir\n\ndef single_task_recognition(input_file, output_dir, MonkeyOCR_model, task):\n    \"\"\"\n    Single task recognition for specific content type\n    \n    Args:\n        input_file: Input file path\n        output_dir: Output directory\n        MonkeyOCR_model: Pre-initialized model instance\n        task: Task type ('text', 'formula', 'table')\n    \"\"\"\n    print(f\"Starting single task recognition: {task}\")\n    print(f\"Processing file: {input_file}\")\n    \n    # Check if input file exists\n    if not os.path.exists(input_file):\n        raise FileNotFoundError(f\"Input file does not exist: {input_file}\")\n    \n    # Get filename\n    name_without_suff = '.'.join(os.path.basename(input_file).split(\".\")[:-1])\n    \n    # Prepare output directory\n    local_md_dir = os.path.join(output_dir, name_without_suff)\n    os.makedirs(local_md_dir, exist_ok=True)\n    \n    print(f\"Output dir: {local_md_dir}\")\n    md_writer = FileBasedDataWriter(local_md_dir)\n    \n    # Get task instruction\n    instruction = TASK_INSTRUCTIONS.get(task, TASK_INSTRUCTIONS['text'])\n    \n    # Check file type and prepare images\n    file_extension = input_file.split(\".\")[-1].lower()\n    images = []\n    \n    if file_extension == 'pdf':\n        print(\"‚ö†Ô∏è  WARNING: PDF input detected for single task recognition.\")\n        print(\"‚ö†Ô∏è  WARNING: Converting all PDF pages to images for processing.\")\n        print(\"‚ö†Ô∏è  WARNING: This may take longer and use more resources than image input.\")\n        print(\"‚ö†Ô∏è  WARNING: Consider using individual images for better performance.\")\n        \n        try:\n            # Convert PDF pages to PIL images directly\n            print(\"Converting PDF pages to images...\")\n            images = pdf_to_images(input_file)\n            print(f\"Converted {len(images)} pages to images\")\n            \n        except Exception as e:\n            raise RuntimeError(f\"Failed to convert PDF to images: {str(e)}\")\n            \n    elif file_extension in ['jpg', 'jpeg', 'png']:\n        # Load single image\n        from PIL import Image\n        images = [Image.open(input_file)]\n    else:\n        raise ValueError(f\"Single task recognition supports PDF and image files, got: {file_extension}\")\n    \n    # Start recognition\n    print(f\"Performing {task} recognition on {len(images)} image(s)...\")\n    start_time = time.time()\n    \n    try:\n        # Prepare instructions for all images\n        instructions = [instruction] * len(images)\n        \n        # Use chat model for single task recognition with PIL images directly\n        responses = MonkeyOCR_model.chat_model.batch_inference(images, instructions)\n        \n        recognition_time = time.time() - start_time\n        print(f\"Recognition time: {recognition_time:.2f}s\")\n        \n        # Combine results\n        combined_result = responses[0]\n        for i, response in enumerate(responses):\n            if i > 0:\n                combined_result = combined_result + \"\\n\\n\" + response\n        \n        # Save result\n        result_filename = f\"{name_without_suff}_{task}_result.md\"\n        md_writer.write(result_filename, combined_result.encode('utf-8'))\n        \n        print(f\"Single task recognition completed!\")\n        print(f\"Task: {task}\")\n        print(f\"Processed {len(images)} image(s)\")\n        print(f\"Result saved to: {os.path.join(local_md_dir, result_filename)}\")\n        \n        # Clean up resources\n        try:\n            # Give some time for async tasks to complete\n            time.sleep(0.5)\n            \n            # Close images if they were opened\n            for img in images:\n                if hasattr(img, 'close'):\n                    img.close()\n                    \n        except Exception as cleanup_error:\n            print(f\"Warning: Error during cleanup: {cleanup_error}\")\n        \n        return local_md_dir\n        \n    except Exception as e:\n        raise RuntimeError(f\"Single task recognition failed: {str(e)}\")\n\ndef parse_file(input_file, output_dir, MonkeyOCR_model, split_pages=False, pred_abandon=False):\n    \"\"\"\n    Parse PDF or image and save results\n    \n    Args:\n        input_file: Input PDF or image file path\n        output_dir: Output directory\n        MonkeyOCR_model: Pre-initialized model instance\n        split_pages: Whether to split result by pages\n    \"\"\"\n    print(f\"Starting to parse file: {input_file}\")\n    \n    # Check if input file exists\n    if not os.path.exists(input_file):\n        raise FileNotFoundError(f\"Input file does not exist: {input_file}\")\n    \n    # Get filename\n    name_without_suff = '.'.join(os.path.basename(input_file).split(\".\")[:-1])\n    \n    # Prepare output directory\n    local_image_dir = os.path.join(output_dir, name_without_suff, \"images\")\n    local_md_dir = os.path.join(output_dir, name_without_suff)\n    image_dir = os.path.basename(local_image_dir)\n    os.makedirs(local_image_dir, exist_ok=True)\n    os.makedirs(local_md_dir, exist_ok=True)\n    \n    print(f\"Output dir: {local_md_dir}\")\n    image_writer = FileBasedDataWriter(local_image_dir)\n    md_writer = FileBasedDataWriter(local_md_dir)\n    \n    # Read file content\n    reader = FileBasedDataReader()\n    file_bytes = reader.read(input_file)\n    \n    # Create dataset instance\n    file_extension = input_file.split(\".\")[-1].lower()\n    if file_extension == \"pdf\":\n        ds = PymuDocDataset(file_bytes)\n    else:\n        ds = ImageDataset(file_bytes)\n    \n    # Start inference\n    print(\"Performing document parsing...\")\n    start_time = time.time()\n    \n    infer_result = ds.apply(doc_analyze_llm, MonkeyOCR_model=MonkeyOCR_model, split_pages=split_pages, pred_abandon=pred_abandon)\n    \n    # Check if infer_result is a list type\n    if isinstance(infer_result, list):\n        print(f\"Processing {len(infer_result)} pages separately...\")\n        \n        # Process each page result separately\n        for page_idx, page_infer_result in enumerate(infer_result):\n            page_dir_name = f\"page_{page_idx}\"\n            page_local_image_dir = os.path.join(output_dir, name_without_suff, page_dir_name, \"images\")\n            page_local_md_dir = os.path.join(output_dir, name_without_suff, page_dir_name)\n            page_image_dir = os.path.basename(page_local_image_dir)\n            \n            # Create page-specific directories\n            os.makedirs(page_local_image_dir, exist_ok=True)\n            os.makedirs(page_local_md_dir, exist_ok=True)\n            \n            # Create page-specific writers\n            page_image_writer = FileBasedDataWriter(page_local_image_dir)\n            page_md_writer = FileBasedDataWriter(page_local_md_dir)\n            \n            print(f\"Processing page {page_idx} - Output dir: {page_local_md_dir}\")\n            \n            # Pipeline processing for this page\n            page_pipe_result = page_infer_result.pipe_ocr_mode(page_image_writer, MonkeyOCR_model=MonkeyOCR_model)\n            \n            # Save page-specific results\n            page_infer_result.draw_model(os.path.join(page_local_md_dir, f\"{name_without_suff}_page_{page_idx}_model.pdf\"))\n            page_pipe_result.draw_layout(os.path.join(page_local_md_dir, f\"{name_without_suff}_page_{page_idx}_layout.pdf\"))\n            page_pipe_result.draw_span(os.path.join(page_local_md_dir, f\"{name_without_suff}_page_{page_idx}_spans.pdf\"))\n            page_pipe_result.dump_md(page_md_writer, f\"{name_without_suff}_page_{page_idx}.md\", page_image_dir)\n            page_pipe_result.dump_content_list(page_md_writer, f\"{name_without_suff}_page_{page_idx}_content_list.json\", page_image_dir)\n            page_pipe_result.dump_middle_json(page_md_writer, f'{name_without_suff}_page_{page_idx}_middle.json')\n        \n        print(f\"All {len(infer_result)} pages processed and saved in separate subdirectories\")\n    else:\n        print(\"Processing as single result...\")\n        \n        # Pipeline processing for single result\n        pipe_result = infer_result.pipe_ocr_mode(image_writer, MonkeyOCR_model=MonkeyOCR_model)\n        \n        # Save single result (original logic)\n        infer_result.draw_model(os.path.join(local_md_dir, f\"{name_without_suff}_model.pdf\"))\n        \n        pipe_result.draw_layout(os.path.join(local_md_dir, f\"{name_without_suff}_layout.pdf\"))\n\n        pipe_result.draw_span(os.path.join(local_md_dir, f\"{name_without_suff}_spans.pdf\"))\n\n        pipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", image_dir)\n        \n        pipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", image_dir)\n\n        pipe_result.dump_middle_json(md_writer, f'{name_without_suff}_middle.json')\n\n    parsing_time = time.time() - start_time\n    print(f\"Parsing and saving time: {parsing_time:.2f}s\")\n    \n    print(\"Results saved to \", local_md_dir)\n    return local_md_dir\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"PDF Document Parsing Tool\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nUsage examples:\n  # Single file processing\n  python parse.py input.pdf                           # Parse single PDF file\n  python parse.py input.pdf -o ./output               # Parse with custom output dir\n  python parse.py input.pdf -s                        # Parse PDF with page splitting\n  python parse.py image.jpg                           # Parse single image file\n  \n  # Single task recognition\n  python parse.py image.jpg -t text                   # Text recognition from image\n  python parse.py image.jpg -t formula                # Formula recognition from image\n  python parse.py image.jpg -t table                  # Table recognition from image\n  python parse.py document.pdf -t text                # Text recognition from all PDF pages\n  \n  # Folder processing (all files individually)\n  python parse.py /path/to/folder                     # Parse all files in folder\n  python parse.py /path/to/folder -s                  # Parse with page splitting\n  python parse.py /path/to/folder -t text             # Single task recognition for all files\n  \n  # Multi-file grouping (batch processing by page count)\n  python parse.py /path/to/folder -g 5                # Group files with max 5 total pages\n  python parse.py /path/to/folder -g 10 -s            # Group files with page splitting\n  python parse.py /path/to/folder -g 8 -t text        # Group files for single task recognition\n  \n  # Advanced configurations\n  python parse.py input.pdf -c model_configs.yaml     # Custom model configuration\n  python parse.py /path/to/folder -g 15 -s -o ./out   # Group files, split pages, custom output\n  python parse.py input.pdf --pred-abandon            # Enable predicting abandon elements\n  python parse.py /path/to/folder -g 10 -m            # Group files and merge text blocks in output\n        \"\"\"\n    )\n    \n    parser.add_argument(\n        \"input_path\",\n        help=\"Input PDF/image file path or folder path\"\n    )\n    \n    parser.add_argument(\n        \"-o\", \"--output\",\n        default=\"./output\",\n        help=\"Output directory (default: ./output)\"\n    )\n    \n    parser.add_argument(\n        \"-c\", \"--config\",\n        default=\"model_configs.yaml\",\n        help=\"Configuration file path (default: model_configs.yaml)\"\n    )\n    \n    parser.add_argument(\n        \"-t\", \"--task\",\n        choices=['text', 'formula', 'table'],\n        help=\"Single task recognition type (text/formula/table). Supports both image and PDF files.\"\n    )\n\n    parser.add_argument(\n        \"-s\", \"--split_pages\",\n        action='store_true',\n        help=\"Split the output of PDF pages into separate ones (default: False)\"\n    )\n    \n    parser.add_argument(\n        \"-g\", \"--group-size\",\n        type=int,\n        help=\"Maximum total page count per group when processing folders (applies to all file types)\"\n    )\n\n    parser.add_argument(\n        \"-m\", \"--merge-blocks\",\n        action='store_true',\n        help=\"Merge text blocks in the output (default: False)\"\n    )\n\n    parser.add_argument(\n        \"--pred-abandon\",\n        action='store_true',\n        help=\"Enable predicting abandon elements like footer and header (default: False)\"\n    )\n    \n    args = parser.parse_args()\n\n    if args.merge_blocks:\n        os.environ[\"MERGE_BLOCKS\"] = \"1\"\n    \n    MonkeyOCR_model = None\n    \n    try:\n        # Check if input path is a directory or file\n        if os.path.isdir(args.input_path):\n            # Process folder\n            result_dir = parse_folder(\n                folder_path = args.input_path,\n                output_dir = args.output,\n                config_path = args.config,\n                task = args.task,\n                split_pages = args.split_pages,\n                group_size = args.group_size,\n                pred_abandon = args.pred_abandon\n            )\n            \n            if args.task:\n                if args.group_size:\n                    print(f\"\\n‚úÖ Folder processing with single task ({args.task}) recognition and image grouping (size: {args.group_size}) completed! Results saved in: {result_dir}\")\n                else:\n                    print(f\"\\n‚úÖ Folder processing with single task ({args.task}) recognition completed! Results saved in: {result_dir}\")\n            else:\n                if args.group_size:\n                    print(f\"\\n‚úÖ Folder processing with image grouping (size: {args.group_size}) completed! Results saved in: {result_dir}\")\n                else:\n                    print(f\"\\n‚úÖ Folder processing completed! Results saved in: {result_dir}\")\n        elif os.path.isfile(args.input_path):\n            # Process single file - initialize model for single file processing\n            print(\"Loading model...\")\n            MonkeyOCR_model = MonkeyOCR(args.config)\n            \n            if args.task:\n                result_dir = single_task_recognition(\n                    input_file = args.input_path,\n                    output_dir = args.output,\n                    MonkeyOCR_model = MonkeyOCR_model,\n                    task = args.task\n                )\n                print(f\"\\n‚úÖ Single task ({args.task}) recognition completed! Results saved in: {result_dir}\")\n            else:\n                result_dir = parse_file(\n                    input_file = args.input_path,\n                    output_dir = args.output,\n                    MonkeyOCR_model = MonkeyOCR_model,\n                    split_pages = args.split_pages,\n                    pred_abandon = args.pred_abandon\n                )\n                print(f\"\\n‚úÖ Parsing completed! Results saved in: {result_dir}\")\n        else:\n            raise FileNotFoundError(f\"Input path does not exist: {args.input_path}\")\n            \n    except Exception as e:\n        print(\"\\n‚ùå Processing failed with traceback:\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n        sys.exit(1)\n    finally:\n        # Clean up resources\n        try:\n            if MonkeyOCR_model is not None:\n                # Clean up model resources if needed\n                if hasattr(MonkeyOCR_model, 'chat_model') and hasattr(MonkeyOCR_model.chat_model, 'close'):\n                    MonkeyOCR_model.chat_model.close()\n                    \n            # Give time for async tasks to complete before exiting\n            time.sleep(1.0)\n            \n            if dist.is_initialized():\n                dist.destroy_process_group()\n                \n        except Exception as cleanup_error:\n            print(f\"Warning: Error during final cleanup: {cleanup_error}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "file_name": "requirements.txt",
    "file_contents": "boto3>=1.28.43\nBrotli>=1.1.0\nclick>=8.1.7\nfast-langdetect>=0.2.3\nloguru>=0.6.0\nnumpy>=1.21.6,<2.0.0\npydantic>=2.7.2\nPyMuPDF>=1.24.9,<=1.24.14\npdfminer.six==20231228\npycocotools>=2.0.6\ntransformers==4.51.0\nqwen_vl_utils==0.0.10\nmatplotlib\ndoclayout_yolo==0.0.2b1\nPyYAML\ndill>=0.3.8,<1\ngradio==5.23.3\nopenai==2.6.1\nfastapi>=0.104.1\nuvicorn[standard]>=0.24.0\n"
  },
  {
    "file_name": "setup.py",
    "file_contents": "from pathlib import Path\nfrom setuptools import setup, find_packages\nfrom magic_pdf.libs.version import __version__\n\n\ndef parse_requirements(filename):\n    with open(filename) as f:\n        lines = f.read().splitlines()\n\n    requires = []\n\n    for line in lines:\n        if \"http\" in line:\n            pkg_name_without_url = line.split('@')[0].strip()\n            requires.append(pkg_name_without_url)\n        else:\n            requires.append(line)\n\n    return requires\n\n\nif __name__ == '__main__':\n    with Path(Path(__file__).parent,\n              'README.md').open(encoding='utf-8') as file:\n        long_description = file.read()\n    setup(\n        name=\"magic_pdf\",\n        version=__version__,\n        packages=find_packages() + [\"magic_pdf.resources\"],\n        package_data={\n            \"magic_pdf.resources\": [\"**\"],\n        },\n        install_requires=parse_requirements('requirements.txt'),\n        description=\"MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm\",\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        url=\"https://github.com/Yuliang-Liu/MonkeyOCR\",\n        python_requires=\">=3.9\",\n        include_package_data=True,\n        zip_safe=False,\n    )\n"
  },
  {
    "file_name": "api/__init__.py",
    "file_contents": "\"\"\"\nMonkeyOCR FastAPI Package\n\"\"\"\n"
  },
  {
    "file_name": "api/main.py",
    "file_contents": "#!/usr/bin/env python3\n\"\"\"\nMonkeyOCR FastAPI Application\n\"\"\"\n\nimport os\nimport io\nimport tempfile\nfrom typing import Optional, List\nfrom pathlib import Path\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import FastAPI, UploadFile, File, HTTPException\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nfrom tempfile import gettempdir\nimport zipfile\nfrom loguru import logger\nimport time\n\nfrom magic_pdf.model.custom_model import MonkeyOCR\nimport uvicorn\n\n# Response models\nclass TaskResponse(BaseModel):\n    success: bool\n    task_type: str\n    content: str\n    message: Optional[str] = None\n\nclass ParseResponse(BaseModel):\n    success: bool\n    message: str\n    output_dir: Optional[str] = None\n    files: Optional[List[str]] = None\n    download_url: Optional[str] = None\n\n# Global model instance and lock\nmonkey_ocr_model = None\nsupports_async = False\nmodel_lock = asyncio.Lock()\nmax_workers = int(os.getenv(\"MAX_WORKERS\", 4))\nexecutor = ThreadPoolExecutor(max_workers=max_workers)\n\ndef initialize_model():\n    \"\"\"Initialize MonkeyOCR model\"\"\"\n    global monkey_ocr_model\n    global supports_async\n    if monkey_ocr_model is None:\n        config_path = os.getenv(\"MONKEYOCR_CONFIG\", \"model_configs.yaml\")\n        monkey_ocr_model = MonkeyOCR(config_path)\n        supports_async = is_async_model(monkey_ocr_model)\n    return monkey_ocr_model\n\ndef is_async_model(model: MonkeyOCR) -> bool:\n    \"\"\"Check if the model supports async concurrent calls\"\"\"\n    if hasattr(model, 'chat_model'):\n        chat_model = model.chat_model\n        # More specific check for async models\n        is_async = hasattr(chat_model, 'async_batch_inference')\n        logger.info(f\"Model {chat_model.__class__.__name__} supports async: {is_async}\")\n        return is_async\n    return False\n\nasync def smart_model_call(func, *args, **kwargs):\n    \"\"\"\n    Smart wrapper that automatically chooses between concurrent and blocking calls\n    based on the model's capabilities\n    \"\"\"\n    global monkey_ocr_model, model_lock\n    \n    if not monkey_ocr_model:\n        raise HTTPException(status_code=500, detail=\"Model not initialized\")\n    \n    if supports_async:\n        # For async models, no need for model_lock, can run concurrently\n        logger.info(\"Using concurrent execution (async model detected)\")\n        # Use asyncio's thread pool to avoid blocking the event loop\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, func, *args, **kwargs)\n    else:\n        # For sync models, use model_lock to prevent conflicts\n        logger.info(\"Using blocking execution with lock (sync model detected)\")\n        async with model_lock:\n            loop = asyncio.get_event_loop()\n            return await loop.run_in_executor(None, func, *args, **kwargs)\n\nasync def smart_batch_model_call(images_and_questions_list, batch_func):\n    \"\"\"\n    Smart batch processing that can handle multiple requests efficiently\n    \"\"\"\n    global monkey_ocr_model\n    \n    if not monkey_ocr_model:\n        raise HTTPException(status_code=500, detail=\"Model not initialized\")\n    \n    if supports_async and hasattr(monkey_ocr_model.chat_model, 'async_batch_inference'):\n        # Use native async batch processing for maximum efficiency\n        logger.info(f\"Using native async batch processing for {len(images_and_questions_list)} requests\")\n        \n        # Flatten all images and questions\n        all_images = []\n        all_questions = []\n        request_indices = []\n        \n        for i, (images, questions) in enumerate(images_and_questions_list):\n            for img, q in zip(images, questions):\n                all_images.append(img)\n                all_questions.append(q)\n                request_indices.append(i)\n        \n        # Single batch call for all requests\n        try:\n            # Use the chat model's async batch inference method properly\n            all_results = await monkey_ocr_model.chat_model.async_batch_inference(all_images, all_questions)\n        except Exception as e:\n            logger.error(f\"Async batch inference failed: {e}, falling back to individual processing\")\n            # Fallback to individual processing using the corrected method\n            results = []\n            for images, questions in images_and_questions_list:\n                try:\n                    # Use the thread-safe smart_model_call wrapper\n                    result = await smart_model_call(batch_func, images, questions)\n                    results.append(result)\n                except Exception as inner_e:\n                    logger.error(f\"Individual processing also failed: {inner_e}\")\n                    results.append([f\"Error: {str(inner_e)}\"] * len(images))\n            return results\n        \n        # Reconstruct results for each original request\n        results = []\n        result_idx = 0\n        for images, questions in images_and_questions_list:\n            request_results = []\n            for _ in range(len(images)):\n                request_results.append(all_results[result_idx])\n                result_idx += 1\n            results.append(request_results)\n        \n        return results\n    \n    elif supports_async:\n        # Concurrent processing for async models\n        logger.info(f\"Using concurrent batch processing for {len(images_and_questions_list)} requests\")\n        tasks = []\n        for images, questions in images_and_questions_list:\n            task = smart_model_call(batch_func, images, questions)\n            tasks.append(task)\n        \n        return await asyncio.gather(*tasks, return_exceptions=True)\n    else:\n        # Sequential processing for sync models\n        logger.info(f\"Using sequential batch processing for {len(images_and_questions_list)} requests\")\n        results = []\n        for images, questions in images_and_questions_list:\n            result = await smart_model_call(batch_func, images, questions)\n            results.append(result)\n        return results\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Lifespan event handler\"\"\"\n    # Startup\n    try:\n        initialize_model()\n        model_type = \"async-capable\" if supports_async else \"sync-only\"\n        logger.info(f\"‚úÖ MonkeyOCR model initialized successfully ({model_type})\")\n    except Exception as e:\n        logger.info(f\"‚ùå Failed to initialize MonkeyOCR model: {e}\")\n        raise\n    \n    yield\n    \n    # Shutdown\n    global executor\n    executor.shutdown(wait=True)\n    logger.info(\"üîÑ Application shutdown complete\")\n\napp = FastAPI(\n    title=\"MonkeyOCR API\",\n    description=\"OCR and Document Parsing API using MonkeyOCR\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\ntemp_dir = os.getenv(\"TMPDIR\", gettempdir())\nlogger.info(f\"Using temporary directory: {temp_dir}\")\nos.makedirs(temp_dir, exist_ok=True)\napp.mount(\"/static\", StaticFiles(directory=temp_dir), name=\"static\")\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint\"\"\"\n    return {\"message\": \"MonkeyOCR API is running\", \"version\": \"1.0.0\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\", \"model_loaded\": monkey_ocr_model is not None}\n\n@app.post(\"/ocr/text\", response_model=TaskResponse)\nasync def extract_text(file: UploadFile = File(...)):\n    \"\"\"Extract text from image or PDF\"\"\"\n    return await perform_ocr_task(file, \"text\")\n\n@app.post(\"/ocr/formula\", response_model=TaskResponse)\nasync def extract_formula(file: UploadFile = File(...)):\n    \"\"\"Extract formulas from image or PDF\"\"\"\n    return await perform_ocr_task(file, \"formula\")\n\n@app.post(\"/ocr/table\", response_model=TaskResponse)\nasync def extract_table(file: UploadFile = File(...)):\n    \"\"\"Extract tables from image or PDF\"\"\"\n    return await perform_ocr_task(file, \"table\")\n\n@app.post(\"/parse\", response_model=ParseResponse)\nasync def parse_document(file: UploadFile = File(...)):\n    \"\"\"Parse complete document (PDF or image)\"\"\"\n    return await parse_document_internal(file, split_pages=False)\n\n@app.post(\"/parse/split\", response_model=ParseResponse)\nasync def parse_document_split(file: UploadFile = File(...)):\n    \"\"\"Parse complete document and split result by pages (PDF or image)\"\"\"\n    return await parse_document_internal(file, split_pages=True)\n\nasync def async_parse_file(input_file_path: str, output_dir: str, split_pages: bool = False):\n    \"\"\"\n    Optimized async version of parse_file that breaks down processing into async chunks\n    \"\"\"\n    import asyncio\n    from concurrent.futures import ThreadPoolExecutor\n    import uuid\n    \n    if not monkey_ocr_model:\n        raise HTTPException(status_code=500, detail=\"Model not initialized\")\n    \n    # Get filename with unique identifier to avoid conflicts\n    name_without_suff = '.'.join(os.path.basename(input_file_path).split(\".\")[:-1])\n    unique_id = str(uuid.uuid4())[:8]  # Short unique identifier\n    safe_name = f\"{name_without_suff}_{unique_id}\"\n    \n    # Prepare output directory with unique name\n    local_image_dir = os.path.join(output_dir, safe_name, \"images\")\n    local_md_dir = os.path.join(output_dir, safe_name)\n    image_dir = os.path.basename(local_image_dir)\n    \n    # Create directories asynchronously with better error handling\n    def create_dir_safe(path):\n        try:\n            os.makedirs(path, exist_ok=True)\n        except FileExistsError:\n            # Directory already exists, that's fine\n            pass\n        except Exception as e:\n            logger.error(f\"Failed to create directory {path}: {e}\")\n            raise\n    \n    await asyncio.get_event_loop().run_in_executor(None, create_dir_safe, local_image_dir)\n    await asyncio.get_event_loop().run_in_executor(None, create_dir_safe, local_md_dir)\n    \n    logger.info(f\"Output dir: {local_md_dir}\")\n    \n    # Read file content in thread pool\n    def read_file_sync():\n        from magic_pdf.data.data_reader_writer import FileBasedDataReader\n        reader = FileBasedDataReader()\n        return reader.read(input_file_path)\n    \n    file_bytes = await asyncio.get_event_loop().run_in_executor(None, read_file_sync)\n    \n    # Create dataset instance in thread pool\n    def create_dataset_sync():\n        from magic_pdf.data.dataset import PymuDocDataset, ImageDataset\n        file_extension = input_file_path.split(\".\")[-1].lower()\n        if file_extension == \"pdf\":\n            return PymuDocDataset(file_bytes)\n        else:\n            return ImageDataset(file_bytes)\n    \n    ds = await asyncio.get_event_loop().run_in_executor(None, create_dataset_sync)\n    \n    # Run inference in thread pool\n    def run_inference_sync():\n        from magic_pdf.model.doc_analyze_by_custom_model_llm import doc_analyze_llm\n        return ds.apply(doc_analyze_llm, MonkeyOCR_model=monkey_ocr_model, split_pages=split_pages)\n    \n    logger.info(\"Starting document parsing...\")\n    start_time = time.time()\n    \n    # Use smart model call for inference\n    if supports_async:\n        # For async models, run without lock\n        infer_result = await asyncio.get_event_loop().run_in_executor(None, run_inference_sync)\n    else:\n        # For sync models, use lock\n        async with model_lock:\n            infer_result = await asyncio.get_event_loop().run_in_executor(None, run_inference_sync)\n    \n    parsing_time = time.time() - start_time\n    logger.info(f\"Parsing time: {parsing_time:.2f}s\")\n    \n    # Process results asynchronously\n    await process_inference_results_async(\n        infer_result, output_dir, safe_name, \n        local_image_dir, local_md_dir, image_dir, split_pages\n    )\n    \n    return local_md_dir\n\nasync def process_inference_results_async(infer_result, output_dir, name_without_suff, \n                                        local_image_dir, local_md_dir, image_dir, split_pages):\n    \"\"\"\n    Process inference results asynchronously\n    \"\"\"\n    from magic_pdf.data.data_reader_writer import FileBasedDataWriter\n    \n    def create_writers():\n        image_writer = FileBasedDataWriter(local_image_dir)\n        md_writer = FileBasedDataWriter(local_md_dir)\n        return image_writer, md_writer\n    \n    # Check if infer_result is a list (split pages)\n    if isinstance(infer_result, list):\n        logger.info(f\"Processing {len(infer_result)} pages separately...\")\n        \n        # Process pages concurrently\n        tasks = []\n        for page_idx, page_infer_result in enumerate(infer_result):\n            task = process_single_page_async(\n                page_infer_result, page_idx, output_dir, name_without_suff\n            )\n            tasks.append(task)\n        \n        # Wait for all page processing to complete\n        await asyncio.gather(*tasks)\n        \n        logger.info(f\"All {len(infer_result)} pages processed and saved in separate subdirectories\")\n    else:\n        # Process single result\n        logger.info(\"Processing as single result...\")\n        await process_single_result_async(\n            infer_result, name_without_suff, local_image_dir, local_md_dir, image_dir\n        )\n\nasync def process_single_page_async(page_infer_result, page_idx, output_dir, name_without_suff):\n    \"\"\"\n    Process a single page result asynchronously\n    \"\"\"\n    import uuid\n    \n    page_dir_name = f\"page_{page_idx}\"\n    page_local_image_dir = os.path.join(output_dir, name_without_suff, page_dir_name, \"images\")\n    page_local_md_dir = os.path.join(output_dir, name_without_suff, page_dir_name)\n    page_image_dir = os.path.basename(page_local_image_dir)\n    \n    # Create page-specific directories with better error handling\n    def create_dir_safe(path):\n        try:\n            os.makedirs(path, exist_ok=True)\n        except FileExistsError:\n            # Directory already exists, that's fine\n            pass\n        except Exception as e:\n            logger.error(f\"Failed to create page directory {path}: {e}\")\n            raise\n    \n    await asyncio.get_event_loop().run_in_executor(None, create_dir_safe, page_local_image_dir)\n    await asyncio.get_event_loop().run_in_executor(None, create_dir_safe, page_local_md_dir)\n    \n    def process_page_sync():\n        from magic_pdf.data.data_reader_writer import FileBasedDataWriter\n        \n        # Create page-specific writers\n        page_image_writer = FileBasedDataWriter(page_local_image_dir)\n        page_md_writer = FileBasedDataWriter(page_local_md_dir)\n        \n        logger.info(f\"Processing page {page_idx} - Output dir: {page_local_md_dir}\")\n        \n        # Pipeline processing for this page\n        page_pipe_result = page_infer_result.pipe_ocr_mode(page_image_writer, MonkeyOCR_model=monkey_ocr_model)\n        \n        # Save page-specific results\n        page_infer_result.draw_model(os.path.join(page_local_md_dir, f\"{name_without_suff}_page_{page_idx}_model.pdf\"))\n        page_pipe_result.draw_layout(os.path.join(page_local_md_dir, f\"{name_without_suff}_page_{page_idx}_layout.pdf\"))\n        page_pipe_result.draw_span(os.path.join(page_local_md_dir, f\"{name_without_suff}_page_{page_idx}_spans.pdf\"))\n        page_pipe_result.dump_md(page_md_writer, f\"{name_without_suff}_page_{page_idx}.md\", page_image_dir)\n        page_pipe_result.dump_content_list(page_md_writer, f\"{name_without_suff}_page_{page_idx}_content_list.json\", page_image_dir)\n        page_pipe_result.dump_middle_json(page_md_writer, f'{name_without_suff}_page_{page_idx}_middle.json')\n    \n    # Run page processing in thread pool\n    await asyncio.get_event_loop().run_in_executor(None, process_page_sync)\n\nasync def process_single_result_async(infer_result, name_without_suff, local_image_dir, local_md_dir, image_dir):\n    \"\"\"\n    Process single result asynchronously\n    \"\"\"\n    def process_single_sync():\n        from magic_pdf.data.data_reader_writer import FileBasedDataWriter\n        \n        image_writer = FileBasedDataWriter(local_image_dir)\n        md_writer = FileBasedDataWriter(local_md_dir)\n        \n        # Pipeline processing for single result\n        pipe_result = infer_result.pipe_ocr_mode(image_writer, MonkeyOCR_model=monkey_ocr_model)\n        \n        # Save single result\n        infer_result.draw_model(os.path.join(local_md_dir, f\"{name_without_suff}_model.pdf\"))\n        pipe_result.draw_layout(os.path.join(local_md_dir, f\"{name_without_suff}_layout.pdf\"))\n        pipe_result.draw_span(os.path.join(local_md_dir, f\"{name_without_suff}_spans.pdf\"))\n        pipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", image_dir)\n        pipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", image_dir)\n        pipe_result.dump_middle_json(md_writer, f'{name_without_suff}_middle.json')\n    \n    # Run processing in thread pool\n    await asyncio.get_event_loop().run_in_executor(None, process_single_sync)\n\nasync def async_single_task_recognition(input_file_path: str, output_dir: str, task: str):\n    \"\"\"\n    Optimized async version of single_task_recognition\n    \"\"\"\n    import uuid\n    \n    logger.info(f\"Starting async single task recognition: {task}\")\n    \n    # Get filename with unique identifier to avoid conflicts\n    name_without_suff = '.'.join(os.path.basename(input_file_path).split(\".\")[:-1])\n    unique_id = str(uuid.uuid4())[:8]  # Short unique identifier\n    safe_name = f\"{name_without_suff}_{unique_id}\"\n    \n    # Prepare output directory with unique name\n    local_md_dir = os.path.join(output_dir, safe_name)\n    \n    def create_dir_safe(path):\n        try:\n            os.makedirs(path, exist_ok=True)\n        except FileExistsError:\n            # Directory already exists, that's fine\n            pass\n        except Exception as e:\n            logger.error(f\"Failed to create directory {path}: {e}\")\n            raise\n    \n    await asyncio.get_event_loop().run_in_executor(None, create_dir_safe, local_md_dir)\n    \n    # Get task instruction\n    from parse import TASK_INSTRUCTIONS\n    instruction = TASK_INSTRUCTIONS.get(task, TASK_INSTRUCTIONS['text'])\n    \n    # Load images asynchronously\n    def load_images_sync():\n        file_extension = input_file_path.split(\".\")[-1].lower()\n        images = []\n        \n        if file_extension == 'pdf':\n            from magic_pdf.utils.load_image import pdf_to_images\n            images = pdf_to_images(input_file_path)\n        elif file_extension in ['jpg', 'jpeg', 'png']:\n            from PIL import Image\n            images = [input_file_path]\n        else:\n            raise ValueError(f\"Unsupported file extension: {file_extension}\")\n        \n        return images, file_extension\n    \n    images, file_extension = await asyncio.get_event_loop().run_in_executor(None, load_images_sync)\n    \n    # Perform recognition\n    logger.info(f\"Performing {task} recognition on {len(images)} image(s)...\")\n    start_time = time.time()\n    \n    # Prepare instructions for all images\n    instructions = [instruction] * len(images)\n    \n    # Use chat model for recognition\n    if supports_async and hasattr(monkey_ocr_model.chat_model, 'async_batch_inference'):\n        # Use async batch inference if available\n        try:\n            responses = await monkey_ocr_model.chat_model.async_batch_inference(images, instructions)\n        except Exception as e:\n            logger.warning(f\"Async batch inference failed: {e}, falling back to sync\")\n            responses = await asyncio.get_event_loop().run_in_executor(\n                None, monkey_ocr_model.chat_model.batch_inference, images, instructions\n            )\n    else:\n        # Use sync batch inference in thread pool\n        responses = await asyncio.get_event_loop().run_in_executor(\n            None, monkey_ocr_model.chat_model.batch_inference, images, instructions\n        )\n    \n    recognition_time = time.time() - start_time\n    logger.info(f\"Recognition time: {recognition_time:.2f}s\")\n    \n    # Combine and save results\n    def save_results_sync():\n        from magic_pdf.data.data_reader_writer import FileBasedDataWriter\n        \n        md_writer = FileBasedDataWriter(local_md_dir)\n        \n        # Combine results\n        combined_result = responses[0]\n        for i, response in enumerate(responses):\n            if i > 0:\n                combined_result = combined_result + \"\\n\\n\" + response\n        \n        # Save result with original name (without unique suffix)\n        result_filename = f\"{name_without_suff}_{task}_result.md\"\n        md_writer.write(result_filename, combined_result.encode('utf-8'))\n        \n        return result_filename\n    \n    result_filename = await asyncio.get_event_loop().run_in_executor(None, save_results_sync)\n    \n    logger.info(f\"Single task recognition completed!\")\n    logger.info(f\"Result saved to: {os.path.join(local_md_dir, result_filename)}\")\n    \n    # Clean up images\n    def cleanup_images():\n        try:\n            for img in images:\n                if hasattr(img, 'close'):\n                    img.close()\n        except Exception as cleanup_error:\n            logger.warning(f\"Warning: Error during cleanup: {cleanup_error}\")\n    \n    await asyncio.get_event_loop().run_in_executor(None, cleanup_images)\n    \n    return local_md_dir\n\nasync def parse_document_internal(file: UploadFile, split_pages: bool = False):\n    \"\"\"Internal function to parse document with optional page splitting\"\"\"\n    try:\n        if not monkey_ocr_model:\n            raise HTTPException(status_code=500, detail=\"Model not initialized\")\n        \n        # Validate file type - support both PDF and image files\n        allowed_extensions = {'.pdf', '.jpg', '.jpeg', '.png'}\n        file_ext_with_dot = os.path.splitext(file.filename)[1].lower() if file.filename else ''\n        \n        if file_ext_with_dot not in allowed_extensions:\n            raise HTTPException(\n                status_code=400, \n                detail=f\"Unsupported file type: {file_ext_with_dot}. Allowed: {', '.join(allowed_extensions)}\"\n            )\n        \n        # Get original filename without extension\n        original_name = '.'.join(file.filename.split('.')[:-1])\n        \n        # Save uploaded file temporarily with unique name to avoid conflicts\n        import uuid\n        unique_suffix = str(uuid.uuid4())[:8]\n        \n        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext_with_dot, prefix=f\"upload_{unique_suffix}_\") as temp_file:\n            content = await file.read()\n            temp_file.write(content)\n            temp_file_path = temp_file.name\n        \n        try:\n            # Create output directory with unique name\n            output_dir = tempfile.mkdtemp(prefix=f\"monkeyocr_parse_{unique_suffix}_\")\n            \n            # Use optimized async parse function\n            result_dir = await async_parse_file(temp_file_path, output_dir, split_pages)\n            \n            # List generated files\n            files = []\n            if os.path.exists(result_dir):\n                for root, dirs, filenames in os.walk(result_dir):\n                    for filename in filenames:\n                        rel_path = os.path.relpath(os.path.join(root, filename), result_dir)\n                        files.append(rel_path)\n            \n            # Create download URL with original filename and timestamp\n            suffix = \"_split\" if split_pages else \"_parsed\"\n            timestamp = int(time.time() * 1000)  # Use milliseconds for better uniqueness\n            zip_filename = f\"{original_name}{suffix}_{timestamp}_{unique_suffix}.zip\"\n            zip_path = os.path.join(temp_dir, zip_filename)\n            \n            # Create ZIP file asynchronously\n            await create_zip_file_async(result_dir, zip_path, original_name, split_pages)\n            \n            download_url = f\"/static/{zip_filename}\"\n            \n            # Determine file type for response message\n            file_type = \"PDF\" if file_ext_with_dot == '.pdf' else \"image\"\n            parse_type = \"with page splitting\" if split_pages else \"standard\"\n            \n            return ParseResponse(\n                success=True,\n                message=f\"{file_type} parsing ({parse_type}) completed successfully\",\n                output_dir=result_dir,\n                files=files,\n                download_url=download_url\n            )\n            \n        finally:\n            # Clean up temporary file\n            try:\n                os.unlink(temp_file_path)\n            except Exception as cleanup_error:\n                logger.warning(f\"Failed to cleanup temp file {temp_file_path}: {cleanup_error}\")\n            \n    except Exception as e:\n        logger.error(f\"Parsing failed: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Parsing failed: {str(e)}\")\n\nasync def create_zip_file_async(result_dir, zip_path, original_name, split_pages):\n    \"\"\"Create ZIP file asynchronously\"\"\"\n    def create_zip_sync():\n        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, filenames in os.walk(result_dir):\n                for filename in filenames:\n                    file_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(file_path, result_dir)\n                    \n                    if split_pages:\n                        # For split pages, maintain the page directory structure\n                        # but add original name prefix\n                        if rel_path.startswith('page_'):\n                            # Keep the page structure: page_0/filename -> page_0/original_name_filename\n                            parts = rel_path.split('/', 1)\n                            if len(parts) == 2:\n                                page_dir, filename_part = parts\n                                if filename_part.startswith('images/'):\n                                    # Handle images: page_0/images/img.jpg -> page_0/images/original_name_img.jpg\n                                    img_name = filename_part.replace('images/', '')\n                                    new_filename = f\"{page_dir}/images/{img_name}\"\n                                else:\n                                    # Handle other files in page directories\n                                    new_filename = f\"{page_dir}/{original_name}_{filename_part}\"\n                            else:\n                                new_filename = f\"{original_name}_{rel_path}\"\n                        else:\n                            new_filename = f\"{original_name}_{rel_path}\"\n                    else:\n                        # Handle different file types\n                        if filename.endswith('.md'):\n                            new_filename = f\"{original_name}.md\"\n                        elif filename.endswith('_content_list.json'):\n                            new_filename = f\"{original_name}_content_list.json\"\n                        elif filename.endswith('_middle.json'):\n                            new_filename = f\"{original_name}_middle.json\"\n                        elif filename.endswith('_model.pdf'):\n                            new_filename = f\"{original_name}_model.pdf\"\n                        elif filename.endswith('_layout.pdf'):\n                            new_filename = f\"{original_name}_layout.pdf\"\n                        elif filename.endswith('_spans.pdf'):\n                            new_filename = f\"{original_name}_spans.pdf\"\n                        else:\n                            # For images and other files, keep relative path structure but rename\n                            if 'images/' in rel_path:\n                                # Keep images in images subfolder with original name prefix\n                                image_name = os.path.basename(rel_path)\n                                new_filename = f\"images/{image_name}\"\n                            else:\n                                new_filename = f\"{original_name}_{filename}\"\n                    \n                    zipf.write(file_path, new_filename)\n    \n    # Run ZIP creation in thread pool to avoid blocking\n    await asyncio.get_event_loop().run_in_executor(None, create_zip_sync)\n\nasync def perform_ocr_task(file: UploadFile, task_type: str) -> TaskResponse:\n    \"\"\"Perform OCR task on uploaded file\"\"\"\n    try:\n        if not monkey_ocr_model:\n            raise HTTPException(status_code=500, detail=\"Model not initialized\")\n        \n        # Validate file type\n        allowed_extensions = {'.pdf', '.jpg', '.jpeg', '.png'}\n        file_ext = Path(file.filename).suffix.lower()\n        if file_ext not in allowed_extensions:\n            raise HTTPException(\n                status_code=400, \n                detail=f\"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}\"\n            )\n        \n        # Save uploaded file temporarily with unique name\n        import uuid\n        unique_suffix = str(uuid.uuid4())[:8]\n        \n        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext, prefix=f\"ocr_{unique_suffix}_\") as temp_file:\n            content = await file.read()\n            temp_file.write(content)\n            temp_file_path = temp_file.name\n        \n        try:\n            # Create output directory with unique name\n            output_dir = tempfile.mkdtemp(prefix=f\"monkeyocr_{task_type}_{unique_suffix}_\")\n            \n            # Use optimized async single task recognition\n            result_dir = await async_single_task_recognition(temp_file_path, output_dir, task_type)\n            \n            # Read result file\n            def read_result_sync():\n                result_files = [f for f in os.listdir(result_dir) if f.endswith(f'_{task_type}_result.md')]\n                if not result_files:\n                    raise Exception(\"No result file generated\")\n                \n                result_file_path = os.path.join(result_dir, result_files[0])\n                with open(result_file_path, 'r', encoding='utf-8') as f:\n                    return f.read()\n            \n            content = await asyncio.get_event_loop().run_in_executor(None, read_result_sync)\n            \n            return TaskResponse(\n                success=True,\n                task_type=task_type,\n                content=content,\n                message=f\"{task_type.capitalize()} extraction completed successfully\"\n            )\n            \n        finally:\n            # Clean up temporary file\n            try:\n                os.unlink(temp_file_path)\n            except Exception as cleanup_error:\n                logger.warning(f\"Failed to cleanup temp file {temp_file_path}: {cleanup_error}\")\n            \n    except Exception as e:\n        logger.error(f\"OCR task failed: {str(e)}\")\n        return TaskResponse(\n            success=False,\n            task_type=task_type,\n            content=\"\",\n            message=f\"OCR task failed: {str(e)}\"\n        )\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=7861)\n"
  },
  {
    "file_name": "demo/demo.py",
    "file_contents": "import os\nimport time\n\nfrom magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\nfrom magic_pdf.data.dataset import PymuDocDataset, ImageDataset\nfrom magic_pdf.model.doc_analyze_by_custom_model_llm import doc_analyze_llm\nfrom magic_pdf.model.custom_model import MonkeyOCR\nimport torch.distributed as dist\n\nif __name__ == \"__main__\":\n    MonkeyOCR_model = MonkeyOCR('model_configs.yaml')\n\n    total_time = 0\n    # for i in range(1,9):\n    pdf_file_name = f\"demo/demo1.pdf\"  # replace with the real pdf path\n    name_without_suff = '.'.join(os.path.basename(pdf_file_name).split(\".\")[:-1])\n\n    # prepare env\n    local_image_dir, local_md_dir = f\"output/{name_without_suff}/images\", f\"output/{name_without_suff}\"\n    image_dir = str(os.path.basename(local_image_dir))\n\n    os.makedirs(local_image_dir, exist_ok=True)\n\n    image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(\n        local_md_dir\n    )\n\n    # read bytes\n    reader1 = FileBasedDataReader()\n    pdf_bytes = reader1.read(pdf_file_name)  # read the pdf content\n\n    # proc\n    ## Create Dataset Instance\n    if pdf_file_name.split(\".\")[-1] == \"pdf\":\n        ds = PymuDocDataset(pdf_bytes)\n    else:\n        ds = ImageDataset(pdf_bytes)\n\n    t1 = time.time()\n    infer_result = ds.apply(doc_analyze_llm, MonkeyOCR_model=MonkeyOCR_model)\n\n    ## pipeline\n    pipe_result = infer_result.pipe_ocr_mode(image_writer, MonkeyOCR_model=MonkeyOCR_model)\n    single_time = time.time() - t1\n    total_time += single_time\n    print(f\"parsing time: {single_time:.2f}s\")\n    print(f\"total time: {total_time:.2f}s\")\n    infer_result.draw_model(os.path.join(local_md_dir, f\"{name_without_suff}_model.pdf\"))\n\n    ### get model inference result\n    model_inference_result = infer_result.get_infer_res()\n\n    ### draw layout result on each page\n    pipe_result.draw_layout(os.path.join(local_md_dir, f\"{name_without_suff}_layout.pdf\"))\n\n    ### draw spans result on each page\n    pipe_result.draw_span(os.path.join(local_md_dir, f\"{name_without_suff}_spans.pdf\"))\n\n    ### get markdown content\n    md_content = pipe_result.get_markdown(image_dir)\n\n    ### dump markdown\n    pipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", image_dir)\n\n    ### get content list content\n    content_list_content = pipe_result.get_content_list(image_dir)\n\n    ### dump content list\n    pipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", image_dir)\n\n    ### get middle json\n    middle_json_content = pipe_result.get_middle_json()\n\n    ### dump middle json\n    pipe_result.dump_middle_json(md_writer, f'{name_without_suff}_middle.json')\n\n    print(f\"Results saved to {local_md_dir}\")\n\n    if dist.is_initialized():\n        dist.destroy_process_group()"
  },
  {
    "file_name": "demo/demo_gradio.py",
    "file_contents": "import gradio as gr\nimport os\nimport base64\nfrom magic_pdf.utils.load_image import pdf_to_images\nimport re\nimport zipfile\nimport subprocess\nimport tempfile\nimport uuid\n\nfrom magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\nfrom magic_pdf.data.dataset import PymuDocDataset, ImageDataset\nfrom magic_pdf.model.doc_analyze_by_custom_model_llm import doc_analyze_llm\nfrom magic_pdf.model.custom_model import MonkeyOCR\nfrom PIL import Image\nfrom loguru import logger\n\nif __name__ == '__main__':\n    if gr.NO_RELOAD:\n        MonkeyOCR_model = MonkeyOCR('model_configs.yaml')\n\n    def render_latex_table_to_image(latex_content, temp_dir):\n        \"\"\"\n        Render LaTeX table to image and return base64 encoding\n        \"\"\"\n        try:\n            # Use regex to extract tabular environment content\n            pattern = r\"(\\\\begin\\{tabular\\}.*?\\\\end\\{tabular\\})\"\n            matches = re.findall(pattern, latex_content, re.DOTALL)\n            \n            if matches:\n                # If complete tabular environment found, use the first one\n                table_content = matches[0]\n            elif '\\\\begin{tabular}' in latex_content:\n                # If only start tag without end tag, add end tag\n                if '\\\\end{tabular}' not in latex_content:\n                    table_content = latex_content + '\\n\\\\end{tabular}'\n                else:\n                    table_content = latex_content\n            else:\n                # If no tabular environment, might be table content that needs wrapping\n                return latex_content  # Return original content without rendering\n            \n            # Build complete LaTeX document, consistent with reference code format\n            full_latex = r\"\"\"\n    \\documentclass{article}\n    \\usepackage[utf8]{inputenc}\n    \\usepackage{booktabs}\n    \\usepackage{bm}\n    \\usepackage{multirow}\n    \\usepackage{array}\n    \\usepackage{colortbl}\n    \\usepackage[table]{xcolor}\n    \\usepackage{amsmath}\n    \\usepackage{amssymb}\n    \\usepackage{graphicx}\n    \\usepackage{geometry}\n    \\usepackage{makecell}\n    \\usepackage[active,tightpage]{preview}\n    \\PreviewEnvironment{tabular}\n    \\begin{document}\n    \"\"\" + table_content + r\"\"\"\n    \\end{document}\n    \"\"\"\n            \n            # Generate unique filename\n            unique_id = str(uuid.uuid4())[:8]\n            tex_path = os.path.join(temp_dir, f\"table_{unique_id}.tex\")\n            pdf_path = os.path.join(temp_dir, f\"table_{unique_id}.pdf\")\n            png_path = os.path.join(temp_dir, f\"table_{unique_id}.png\")\n            \n            # Write tex file\n            with open(tex_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(full_latex)\n            \n            # Call pdflatex to generate PDF, add more detailed error handling\n            result = subprocess.run(\n                [\"pdflatex\", \"-interaction=nonstopmode\", \"-output-directory\", temp_dir, tex_path], \n                timeout=20,\n                capture_output=True,\n                text=True\n            )\n            \n            if result.returncode != 0:\n                # If compilation fails, output error info and return original content\n                print(f\"LaTeX compilation failed:\")\n                print(f\"stdout: {result.stdout}\")\n                print(f\"stderr: {result.stderr}\")\n                print(f\"LaTeX content: {table_content}\")\n                return f\"<pre>{latex_content}</pre>\"  # Return original content as preformatted text\n            \n            # Check if PDF file is generated\n            if not os.path.exists(pdf_path):\n                print(f\"PDF file not generated: {pdf_path}\")\n                return f\"<pre>{latex_content}</pre>\"\n            \n            # Convert PDF to PNG image\n            images = pdf_to_images(pdf_path)\n            images[0].save(png_path, \"PNG\")\n            \n            # Read image and convert to base64\n            with open(png_path, \"rb\") as f:\n                img_data = f.read()\n            img_base64 = base64.b64encode(img_data).decode(\"utf-8\")\n            \n            # Clean up temporary files\n            for file_path in [tex_path, pdf_path, png_path]:\n                if os.path.exists(file_path):\n                    os.remove(file_path)\n            # Clean up possible auxiliary files\n            for ext in ['.aux', '.log', '.fls', '.fdb_latexmk']:\n                aux_file = os.path.join(temp_dir, f\"table_{unique_id}{ext}\")\n                if os.path.exists(aux_file):\n                    os.remove(aux_file)\n            \n            return f'<img src=\"data:image/png;base64,{img_base64}\" style=\"max-width:100%;height:auto;\">'\n            \n        except subprocess.TimeoutExpired:\n            print(\"LaTeX compilation timeout\")\n            return f\"<pre>{latex_content}</pre>\"\n        except Exception as e:\n            print(f\"LaTeX rendering error: {e}\")\n            return f\"<pre>{latex_content}</pre>\"  # If rendering fails, return original content as preformatted text\n\n    def parse_pdf_and_return_results(pdf_file):\n        if pdf_file is None:\n            return (\n                None,\n                None,\n                gr.update(value=None, visible=False),\n                gr.update(value=None, visible=False),\n                gr.update(value=\"\", visible=False)  # Hide parsing prompt\n            )\n        parent_path = os.path.dirname(pdf_file)\n        full_name = os.path.basename(pdf_file)\n        name = '.'.join(full_name.split(\".\")[:-1])\n        local_image_dir, local_md_dir = parent_path+\"/markdown/images\", parent_path+\"/markdown\"\n        image_dir = str(os.path.basename(local_image_dir))\n        os.makedirs(local_image_dir, exist_ok=True)\n        image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(local_md_dir)   \n        reader1 = FileBasedDataReader(parent_path)\n        data_bytes = reader1.read(full_name)\n        if full_name.split(\".\")[-1] in ['jpg', 'jpeg', 'png']:\n            ds = ImageDataset(data_bytes)\n        else:\n            ds = PymuDocDataset(data_bytes)\n        infer_result = ds.apply(doc_analyze_llm, MonkeyOCR_model=MonkeyOCR_model)\n        pipe_result = infer_result.pipe_ocr_mode(image_writer, MonkeyOCR_model=MonkeyOCR_model)\n        layout_pdf_path = os.path.join(parent_path, f\"{name}_layout.pdf\")\n        pipe_result.draw_layout(layout_pdf_path)\n        pipe_result.dump_md(md_writer, f\"{name}.md\", image_dir)\n        md_content_ori = FileBasedDataReader(local_md_dir).read(f\"{name}.md\").decode(\"utf-8\")\n        \n        # Create temporary directory for LaTeX rendering\n        temp_dir = tempfile.mkdtemp()\n        \n        try:\n            # Process HTML-wrapped LaTeX tables\n            def replace_html_latex_table(match):\n                html_content = match.group(1)\n                # Check if contains \\begin{tabular}\n                if '\\\\begin{tabular}' in html_content:\n                    return render_latex_table_to_image(html_content, temp_dir)\n                else:\n                    return match.group(0)  # Keep original\n            \n            # Use regex to replace LaTeX tables wrapped in <html>...</html>\n            md_content = re.sub(r'<html>(.*?)</html>', replace_html_latex_table, md_content_ori, flags=re.DOTALL)\n            \n            # Convert local image links in markdown to base64 encoded HTML\n            def replace_image_with_base64(match):\n                img_path = match.group(1)\n                # Handle relative paths\n                if not os.path.isabs(img_path):\n                    full_img_path = os.path.join(local_md_dir, img_path)\n                else:\n                    full_img_path = img_path\n                \n                try:\n                    if os.path.exists(full_img_path):\n                        with open(full_img_path, \"rb\") as f:\n                            img_data = f.read()\n                        img_base64 = base64.b64encode(img_data).decode(\"utf-8\")\n                        # Get file extension to determine MIME type\n                        ext = os.path.splitext(full_img_path)[1].lower()\n                        mime_type = \"image/jpeg\" if ext in ['.jpg', '.jpeg'] else f\"image/{ext[1:]}\"\n                        return f'<img src=\"data:{mime_type};base64,{img_base64}\" style=\"max-width:100%;height:auto;\">'\n                    else:\n                        return match.group(0)  # If file not found, keep original\n                except Exception:\n                    return match.group(0)  # If error, keep original\n            \n            # Use regex to replace markdown image syntax ![alt](path)\n            md_content = re.sub(r'!\\[.*?\\]\\(([^)]+)\\)', replace_image_with_base64, md_content)\n            \n        finally:\n            # Clean up temporary directory\n            import shutil\n            if os.path.exists(temp_dir):\n                shutil.rmtree(temp_dir, ignore_errors=True)\n        \n        # Create zip file\n        zip_path = os.path.join(parent_path, f\"{name}_markdown.zip\")\n        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            # Traverse local_md_dir folder, add all files to zip\n            for root, dirs, files in os.walk(local_md_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    # Calculate relative path, maintain folder structure\n                    arcname = os.path.relpath(file_path, local_md_dir)\n                    zipf.write(file_path, arcname)\n        \n        return (\n            md_content_ori,\n            md_content,\n            gr.update(value=layout_pdf_path, visible=True),\n            gr.update(value=zip_path, visible=True),\n        )\n\n    def chat_with_image(message, pdf_file):\n        \"\"\"Chat with the uploaded image\"\"\"\n        if pdf_file is None:\n            return \"Please upload an image or PDF file before chatting.\"\n        \n        base_dir = os.path.dirname(pdf_file)\n        file_ext = pdf_file.split(\".\")[-1].lower()\n        if file_ext not in ['jpg', 'jpeg', 'png', 'pdf']:\n            return \"Please upload an image or PDF file before chatting.\"\n        \n        try:\n            if file_ext in ['jpg', 'jpeg', 'png']:\n                # Chat directly using image file path\n                image_path = pdf_file\n                response = MonkeyOCR_model.chat_model.batch_inference([image_path], [message])[0]\n            else:\n                # PDF file processing\n                response = \"Only image chat is supported, PDF file chat is not supported.\"\n            file_writer = FileBasedDataWriter(base_dir)\n            md_name = f\"chat_response_{uuid.uuid4().hex}.md\"\n            file_writer.write(md_name, response.encode('utf-8'))\n            return response, response, gr.update(value=None, visible=True), gr.update(value=os.path.join(base_dir, md_name), visible=True)\n        except Exception as e:\n            response = f\"Chat processing error: {str(e)}\"\n            return response, response, gr.update(value=None, visible=True), gr.update(value=None, visible=True)\n\n    # Global cache: store images of each page\n    pdf_cache = {\n        \"images\": [],\n        \"current_page\": 0,\n        \"total_pages\": 0,\n    }\n\n    def load_file(file):\n        # Read PDF and convert to images (one page one image)\n        if file.endswith('.pdf'):\n            pages = pdf_to_images(file)\n        else:\n            # For image files, read directly as single-page image\n            image = Image.open(file)\n            pages = [image]\n        pdf_cache[\"images\"] = pages\n        pdf_cache[\"current_page\"] = 0\n        pdf_cache[\"total_pages\"] = len(pages)\n        return pages[0], f\"<div id='page_info_box'>1 / {len(pages)}</div>\"\n\n    def turn_page(direction):\n        if not pdf_cache[\"images\"]:\n            return None, \"<div id='page_info_box'>0 / 0</div>\"\n\n        if direction == \"prev\":\n            pdf_cache[\"current_page\"] = max(0, pdf_cache[\"current_page\"] - 1)\n        elif direction == \"next\":\n            pdf_cache[\"current_page\"] = min(pdf_cache[\"total_pages\"] - 1, pdf_cache[\"current_page\"] + 1)\n\n        index = pdf_cache[\"current_page\"]\n        return pdf_cache[\"images\"][index], f\"<div id='page_info_box'>{index + 1} / {pdf_cache['total_pages']}</div>\"\n\n    # Global variables to store parsed result file paths\n    layout_pdf_path = None\n    markdown_zip_path = None\n\n    def download_layout_pdf():\n        if layout_pdf_path and os.path.exists(layout_pdf_path):\n            return layout_pdf_path\n        return None\n\n    def download_markdown_zip():\n        if markdown_zip_path and os.path.exists(markdown_zip_path):\n            return markdown_zip_path\n        return None\n\n    def parse_and_update_view(pdf_file):\n        \"\"\"Parse PDF and update view\"\"\"\n        \n        if pdf_file is None:\n            return (\n                gr.update(),\n                \"Please upload a PDF file\",\n                \"Please upload a PDF file\",\n                \"<div id='page_info_box'>0 / 0</div>\",\n                gr.update(value=None, visible=True),\n                gr.update(value=None, visible=True),\n            )\n        \n        try:\n            # Call the original parsing function\n            md_content_ori, md_content, layout_pdf_update, zip_update = parse_pdf_and_return_results(pdf_file)\n            \n            # Update global variables\n            layout_pdf_path = layout_pdf_update['value']\n            markdown_zip_path = zip_update['value']\n            \n            # Load parsed layout PDF for preview\n            if layout_pdf_path and os.path.exists(layout_pdf_path):\n                pages = pdf_to_images(layout_pdf_path)\n                pdf_cache[\"images\"] = pages\n                pdf_cache[\"current_page\"] = 0\n                pdf_cache[\"total_pages\"] = len(pages)\n                preview_image = pages[0]\n                page_info = f\"<div id='page_info_box'>1 / {len(pages)}</div>\"\n            else:\n                preview_image = None\n                page_info = \"<div id='page_info_box'>0 / 0</div>\"\n            \n            return (\n                preview_image,\n                md_content,\n                md_content_ori,\n                page_info,\n                layout_pdf_update,\n                zip_update,\n            )\n        except:\n            logger.warning(\"Parsing failed, switching to chat mode for direct recognition...\")\n            # If parsing fails, directly use chat mode for recognition\n            md_content_ori, md_content, layout_pdf_update, zip_update = chat_with_image(instruction, pdf_file)\n            return (\n                gr.update(),\n                md_content,\n                md_content_ori,\n                \"<div id='page_info_box'>1 / 1</div>\",\n                layout_pdf_update,\n                zip_update,\n            )\n\n    def clear_all():\n        \"\"\"Clear all inputs and outputs\"\"\"\n        pdf_cache[\"images\"] = []\n        pdf_cache[\"current_page\"] = 0\n        pdf_cache[\"total_pages\"] = 0\n        return (\n            None,  # Clear file input\n            None,  # Clear PDF preview\n            \"## üïê Waiting for parsing result...\",  # Clear Markdown preview\n            \"üïê Waiting for parsing result...\",  # Clear Markdown raw text\n            \"<div id='page_info_box'>0 / 0</div>\",  # Clear page info\n            gr.update(value=None, visible=True),\n            gr.update(value=None, visible=True),\n        )\n\n    instruction = f'''Please output the text content from the image.'''\n    instruction_mf = f'''Please write out the expression of the formula in the image using LaTeX format.'''\n    instruction_table_html = f'''This is the image of a table. Please output the table in html format.'''\n    instruction_table_latex = f'''Please output the table in the image in LaTeX format.'''\n\n    css = \"\"\"\n    #page_info_html {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        height: 100%;  /* Ensure consistent height with button row */\n        margin: 0 12px;  /* Increase left and right margin for centering */\n    }\n\n    #page_info_box {\n        padding: 8px 20px;\n        font-size: 16px;\n        border: 1px solid #bbb;\n        border-radius: 8px;\n        background-color: #f8f8f8;\n        text-align: center;\n        min-width: 80px;\n        box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n    }\n\n    #markdown_output {\n        min-height: 800px;\n        overflow: auto;\n    }\n\n    footer {\n        visibility: hidden;\n    }\n    \"\"\"\n\n    with gr.Blocks(theme=\"ocean\", css=css, title='MonkeyOCR') as demo:\n        gr.HTML(\"\"\"\n            <div style=\"display: flex; align-items: center; justify-content: center; margin-bottom: 20px;\">\n                <h1 style=\"margin: 0; font-size: 2em;\">MonkeyOCR</h1>\n            </div>\n            <div style=\"text-align: center; margin-bottom: 10px;\">\n                <em>Supports PDF parse, image parse, and Q&A</em>\n            </div>\n        \"\"\")\n\n        with gr.Row():\n            with gr.Column(scale=1, variant=\"compact\"):\n                gr.Markdown(\"### üì• Upload PDF/Image (‰∏ä‰º†PDF/Image)\")\n                pdf_input = gr.File(label=\"Select File (ÈÄâÊã©Êñá‰ª∂)\", type=\"filepath\", file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\"], show_label=True)\n                chat_input = gr.Dropdown(label=\"Select Prompt (ÈÄâÊã©Prompt)\", choices=[instruction, instruction_mf, instruction_table_html, instruction_table_latex], value=instruction, show_label=True, multiselect=False, visible=True)\n                gr.Markdown(\"### ‚öôÔ∏è Actions (Êìç‰Ωú)\")\n                parse_button = gr.Button(\"üîç Parse (Ëß£Êûê)\", variant=\"primary\")\n                chat_button = gr.Button(\"üí¨ Chat (ÂØπËØù)\", variant=\"secondary\")\n                clear_button = gr.Button(\"üóëÔ∏è Clear (Ê∏ÖÈô§)\", variant=\"huggingface\")\n\n            with gr.Column(scale=6, variant=\"compact\"):\n                with gr.Row():\n                    with gr.Column(scale=3):\n                        gr.Markdown(\"### üëÅÔ∏è File Preview (Êñá‰ª∂È¢ÑËßà)\")\n                        pdf_view = gr.Image(label=\"PDF Preview (PDFÈ¢ÑËßà)\", visible=True, height=800, show_label=False)\n                        with gr.Row():\n                            prev_btn = gr.Button(\"‚¨Ö Prev Page (‰∏ä‰∏ÄÈ°µ)\")\n                            page_info = gr.HTML(value=\"<div id='page_info_box'>0 / 0</div>\", elem_id=\"page_info_html\")\n                            next_btn = gr.Button(\"(‰∏ã‰∏ÄÈ°µ) Next Page ‚û°\")\n                    with gr.Column(scale=3):\n                        gr.Markdown(\"### ‚úîÔ∏è Result Display (ÁªìÊûúÂ±ïÁ§∫)\")\n                        with gr.Tabs(elem_id=\"markdown_tabs\"):\n                            with gr.TabItem(\"Markdown Render Preview (MarkdownÊ∏≤ÊüìÈ¢ÑËßà)\"):\n                                md_view = gr.Markdown(value=\"## Please click the parse button to parse or click chat for single-task recognition...\", label=\"Markdown Preview (MarkdownÈ¢ÑËßà)\", max_height=600, latex_delimiters=[\n                                    {\"left\": \"$$\", \"right\": \"$$\", \"display\": True},\n                                    {\"left\": \"$\", \"right\": \"$\", \"display\": False},\n                                ], show_copy_button=False, elem_id=\"markdown_output\")\n                            with gr.TabItem(\"Markdown Raw Text (MarkdownÂéüÂßãÊñáÊú¨)\"):\n                                md_raw = gr.Textbox(value=\"üïê Waiting for parsing result...\", label=\"Markdown Raw Text (MarkdownÂéüÂßãÊñáÊú¨)\", max_lines=100, lines=38, show_copy_button=True, elem_id=\"markdown_output\", show_label=False)\n                with gr.Row():\n                    with gr.Column(scale=3):\n                        pdf_download_button = gr.DownloadButton(\"‚¨áÔ∏è Download PDF Layout (‰∏ãËΩΩPDF Layout)\", visible=True)\n                    with gr.Column(scale=3):\n                        md_download_button = gr.DownloadButton(\"‚¨áÔ∏è Download Markdown (‰∏ãËΩΩMarkdown)\", visible=True)\n\n        # Event handling\n        # Show PDF preview on file upload\n        pdf_input.upload(\n            fn=load_file,\n            inputs=pdf_input,\n            outputs=[pdf_view, page_info]\n        )\n        \n        # Page turning function\n        prev_btn.click(fn=lambda: turn_page(\"prev\"), outputs=[pdf_view, page_info], show_progress=False)\n        next_btn.click(fn=lambda: turn_page(\"next\"), outputs=[pdf_view, page_info], show_progress=False)\n\n        parse_button.click(\n            fn=parse_and_update_view,\n            inputs=pdf_input,\n            outputs=[pdf_view, md_view, md_raw, page_info, pdf_download_button, md_download_button],\n            show_progress=True,\n            show_progress_on=[md_view, md_raw]\n        )\n        \n        # Q&A button\n        chat_button.click(\n            fn=chat_with_image,\n            inputs=[chat_input, pdf_input],\n            outputs=[md_view, md_raw, pdf_download_button, md_download_button],\n            show_progress=True,\n            show_progress_on=[md_view, md_raw]\n        )\n        \n        # Clear button\n        clear_button.click(\n            fn=clear_all,\n            outputs=[pdf_input, pdf_view, md_view, md_raw, page_info, pdf_download_button, md_download_button],\n            show_progress=False\n        )\n\n    demo.queue().launch(server_name=\"0.0.0.0\", server_port=7860, debug=True)\n"
  },
  {
    "file_name": "docker/docker-compose.yml",
    "file_contents": "x-monkeyocr-base: &monkeyocr-base\n  image: monkeyocr:latest\n  volumes:\n    - model_data:/app/MonkeyOCR/model_weight\n  environment:\n    - TMPDIR=/app/tmp\n    - CUDA_VISIBLE_DEVICES=0\n    - HF_HUB_CACHE=/app/MonkeyOCR/model_weight\n    - MODELSCOPE_CACHE=/app/MonkeyOCR/model_weight\n  deploy:\n    resources:\n      reservations:\n        devices:\n          - driver: nvidia\n            count: 1\n            capabilities: [gpu]\n\nservices:\n  monkeyocr:\n    <<: *monkeyocr-base\n    build:\n      context: ..\n      dockerfile: docker/Dockerfile\n      args:\n        BUILDKIT_INLINE_CACHE: \"1\"\n        LMDEPLOY_PATCHED: \"false\"\n    ports:\n      - \"7860:7860\"\n\n  monkeyocr-fix:\n    <<: *monkeyocr-base\n    build:\n      context: ..\n      dockerfile: docker/Dockerfile\n      args:\n        BUILDKIT_INLINE_CACHE: \"1\"\n        LMDEPLOY_PATCHED: \"true\"\n    environment:\n      - TMPDIR=/app/tmp\n      - CUDA_VISIBLE_DEVICES=0\n      - HF_HUB_CACHE=/app/MonkeyOCR/model_weight\n      - MODELSCOPE_CACHE=/app/MonkeyOCR/model_weight\n    ports:\n      - \"7860:7860\"\n\n  monkeyocr-demo:\n    <<: *monkeyocr-base\n    entrypoint: [\"/app/MonkeyOCR/entrypoint.sh\"]\n    command: [\"demo\"]\n    ports:\n      - \"7860:7860\"\n\n  monkeyocr-dev:\n    <<: *monkeyocr-base\n    entrypoint: [\"/app/MonkeyOCR/entrypoint.sh\"]\n    command: [\"bash\"]\n    stdin_open: true\n    tty: true\n    ports:\n      - \"7860:7860\"\n\n  monkeyocr-api:\n    <<: *monkeyocr-base\n    entrypoint: [\"/app/MonkeyOCR/entrypoint.sh\"]\n    command: [\"fastapi\"]\n    ports:\n      - \"7861:7861\"\n    environment:\n      - TMPDIR=/app/tmp\n      - CUDA_VISIBLE_DEVICES=0\n      - HF_HUB_CACHE=/app/MonkeyOCR/model_weight\n      - MODELSCOPE_CACHE=/app/MonkeyOCR/model_weight\n      - FASTAPI_HOST=0.0.0.0\n      - FASTAPI_PORT=7861\n\nvolumes:\n  model_data:"
  },
  {
    "file_name": "docs/Quantization.md",
    "file_contents": "# Quantization with AWQ\n\n1.  Install the required packages.\n    ```bash\n    pip install datasets\n    ```\n2.  If you directly proceed to the third step, you may encounter the following problems:\n    ```bash\n    RuntimeError: Currently, quantification and calibration of Qwen2_5_VLTextModel are not supported.\n    The supported model types are InternLMForCausalLM, InternLM2ForCausalLM, InternLM3ForCausalLM, QWenLMHeadModel, Qwen2ForCausalLM, Qwen3ForCausalLM, BaiChuanForCausalLM, BaichuanForCausalLM, LlamaForCausalLM, LlavaLlamaForCausalLM,MGMLlamaForCausalLM, InternLMXComposer2ForCausalLM, Phi3ForCausalLM, ChatGLMForConditionalGeneration, MixtralForCausalLM, Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, MistralForCausalLM.\n    ```\n    This is because in the calibrte.py file of the lmdeploy library, the following code (lines 255-258) replaces `model` with `vl_model.language_model`, causing `model_type` to become `Qwen2_5_VLTextModel` instead of the supported `Qwen2_5_VLForConditionalGeneration`:\n    ```\n    if hasattr(vl_model, 'language_model'):  # deepseek-vl, ...\n        model = vl_model.language_model\n    if hasattr(vl_model, 'llm'):  # MiniCPMV, ...\n        model = vl_model.llm\n    ```\n    Find these codes and comment out these lines:\n    ```\n    # if hasattr(vl_model, 'language_model'):  # deepseek-vl, ...\n    #     model = vl_model.language_model\n    # if hasattr(vl_model, 'llm'):  # MiniCPMV, ...\n    #     model = vl_model.llm\n    ```    \n    You can use the following command to view the directory of the **lmdeploy** library:\n    ```bash    \n    python -c \"import lmdeploy; import os; print(os.path.dirname(lmdeploy.__file__))\"\n    ```    \n    The relative location of calibrte.py is in **lmdeploy/lite/apis/calibrate.py**\n\n    Or you can download [tools/fix_qwen2_5_vl_awq.py](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/tools/fix_qwen2_5_vl_awq.py)\n\n    Run in your environment:\n    ```bash\n    python tools/fix_qwen2_5_vl_awq.py patch\n    ```\n    **Note**: This command modifies LMDeploy‚Äôs source code in your environment. To undo the changes, simply run:\n    ```bash\n    python tools/fix_qwen2_5_vl_awq.py restore\n    ```\n    \n4.  Enter the following in the terminal.\n    ```bash\n    lmdeploy lite auto_awq \\\n        ./model_weight/Recognition \\\n        --calib-dataset 'ptb' \\\n        --calib-samples 64 \\\n        --calib-seqlen 1024 \\\n        --w-bits 4 \\\n        --w-group-size 128 \\\n        --batch-size 1 \\\n        --work-dir ./monkeyocr_quantization\n    ```\n    Wait for the quantization to complete.\n    * If the quantization process is killed, you need to check if you have sufficient memory.\n    * For reference, the maximum VRAM usage for quantization with these parameters is approximately 6.47GB.\n\n5.  You might encounter the following error:\n    ```\n    RuntimeError: Error(s) in loading state_dict for Linear:\n        size mismatch for bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1280]).\n    ```\n    This is because your installed version of LMDeploy is not yet compatible with Qwen2.5VL. You need to install the latest development version from the GitHub repository.\n    ```bash\n    pip install git+https://github.com/InternLM/lmdeploy.git\n    ```\n    After the installation is complete, try quantizing again.\n\n6.  After quantization is complete, replace the `Recognition` folder.\n    ```bash\n    mv model_weight/Recognition Recognition_backup\n\n    mv monkeyocr_quantization model_weight/Recognition\n    ```\n    Then, you can try running the program again.\n"
  },
  {
    "file_name": "docs/install_cuda.md",
    "file_contents": "# Install with CUDA Support\n\nThis guide walks you through setting up the environment for **MonkeyOCR** with CUDA support. You can choose **one** of the backends ‚Äî [**LMDeploy**](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#using-lmdeploy-as-the-inference-backend-optional)(recomended), [**vLLM**](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#using-vllm-as-the-inference-backend-optional), or [**transformers**](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#using-transformers-as-the-inference-backend-optional) ‚Äî to install and use. It covers installation instructions for each of them.\n\n> **Note:** Based on our internal test, inference speed ranking is: **LMDeploy ‚â• vLLM >>> transformers**\n\n## Using **LMDeploy** as the Inference Backend (Optional)\n> **Supporting CUDA 12.4/12.1/11.8**\n\nIf you're using **CUDA 12.4** or **CUDA 12.1**, follow these steps:\n\n```bash\nconda create -n MonkeyOCR python=3.10\nconda activate MonkeyOCR\n\ngit clone https://github.com/Yuliang-Liu/MonkeyOCR.git\ncd MonkeyOCR\n\nexport CUDA_VERSION=124 # for CUDA 12.4\n# export CUDA_VERSION=121 # for CUDA 12.1\n\n# Install PyTorch. Refer to https://pytorch.org/get-started/previous-versions/ for version compatibility\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu${CUDA_VERSION}\n\npip install -e .\n\npip install lmdeploy==0.8.0\n```\n\nIf you're using **CUDA 11.8**, use the following instead:\n\n```bash\nconda create -n MonkeyOCR python=3.10\nconda activate MonkeyOCR\n\ngit clone https://github.com/Yuliang-Liu/MonkeyOCR.git\ncd MonkeyOCR\n\n# Install PyTorch. Refer to https://pytorch.org/get-started/previous-versions/ for version compatibility\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n\npip install -e .\n\npip install https://github.com/InternLM/lmdeploy/releases/download/v0.8.0/lmdeploy-0.8.0+cu118-cp310-cp310-manylinux2014_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118\n```\n\n> [!IMPORTANT]\n> ### Fixing the **Shared Memory Error** on **20/30/40 series / V100 ...** GPUs (Optional)\n> \n> Our 3B model runs smoothly on the NVIDIA RTX 30/40 series. However, when using **LMDeploy** as the inference backend, you might run into compatibility issues on these GPUs ‚Äî typically this error:\n> \n> ```\n> triton.runtime.errors.OutOfResources: out of resource: shared memory\n> ```\n> \n> To resolve this issue, apply the following patch:\n> \n> ```bash\n> python tools/lmdeploy_patcher.py patch\n> ```\n> **Note:** This command modifies LMDeploy‚Äôs source code in your environment.\n> To undo the changes, simply run:\n> \n> ```bash\n> python tools/lmdeploy_patcher.py restore\n> ```\n> \n> Based on our tests on the **NVIDIA RTX 3090**, inference speed was **0.338 pages/second** using **LMDeploy** (with the patch applied), compared to only **0.015 pages/second** using **transformers**.\n> \n> **Special thanks to [@pineking](https://github.com/pineking) for the solution!**\n\n---\n\n## Using **vLLM** as the Inference Backend (Optional)\n> **Supporting CUDA 12.6/12.8/11.8**\n```bash\nconda create -n MonkeyOCR python=3.10\nconda activate MonkeyOCR\n\ngit clone https://github.com/Yuliang-Liu/MonkeyOCR.git\ncd MonkeyOCR\n\npip install uv --upgrade\nexport CUDA_VERSION=126 # for CUDA 12.6\n# export CUDA_VERSION=128 # for CUDA 12.8\n# export CUDA_VERSION=118 # for CUDA 11.8\nuv pip install vllm==0.9.1 --torch-backend=cu${CUDA_VERSION}\n\npip install -e .\n```\n\nThen, update the `chat_config.backend` field in your `model_configs.yaml` config file:\n\n```yaml\nchat_config:\n    backend: vllm\n```\n\n---\n\n## Using **transformers** as the Inference Backend (Optional)\n> **Supporting CUDA 12.4/12.1**\n```bash\nconda create -n MonkeyOCR python=3.10\nconda activate MonkeyOCR\n\ngit clone https://github.com/Yuliang-Liu/MonkeyOCR.git\ncd MonkeyOCR\n\npip install -e .\n```\n\nInstall PyTorch according to your CUDA version:\n\n```bash\nexport CUDA_VERSION=124 # for CUDA 12.4\n# export CUDA_VERSION=121 # for CUDA 12.1\n\n# Install pytorch\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu${CUDA_VERSION}\n```\n\nInstall Flash Attention 2:\n\n```bash\npip install flash-attn==2.7.4.post1 --no-build-isolation\n```\nThen, update the `chat_config` in your `model_configs.yaml` config file:\n```yaml\nchat_config:\n  backend: transformers\n  batch_size: 10  # Adjust based on your available GPU memory\n```\n"
  },
  {
    "file_name": "docs/install_cuda_pp.md",
    "file_contents": "# Install with CUDA Support\n\nThis guide walks you through setting up the environment for **MonkeyOCR** with CUDA support. You can choose **one** of the backends ‚Äî **LMDeploy** (recomend), **vLLM**, or **transformers** ‚Äî to install and use. It covers installation instructions for each of them.\n\n## Step 1. Install PaddleX\nTo use `PP-DocLayout_plus-L`, you must install two additional core libraries, **PaddlePaddle** and **PaddleX**.\n\nMake sure your pytorch version is compatible with the PaddlePaddle version you are installing, referring to the official **[PaddleX](https://github.com/PaddlePaddle/PaddleX)**\n\n```bash\nconda create -n MonkeyOCR python=3.10\nconda activate MonkeyOCR\n\ngit clone https://github.com/Yuliang-Liu/MonkeyOCR.git\ncd MonkeyOCR\n\nexport CUDA_VERSION=126 # for CUDA 12.6\n# export CUDA_VERSION=118 # for CUDA 11.8\n\npip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu${CUDA_VERSION}/\npip install langchain==0.3.26\npip install \"paddlex[base]==3.1.4\"\n```\n\n## Step 2. Install Inference Backend\n\n> **Note:** Based on our internal test, inference speed ranking is: **[LMDeploy](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#using-lmdeploy-as-the-inference-backend-optional) ‚â• [vLLM](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#using-vllm-as-the-inference-backend-optional) >>> [transformers](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#using-transformers-as-the-inference-backend-optional)**\n\n### Using **LMDeploy** as the Inference Backend (Recommend)\n> **Supporting CUDA 12.6/11.8**\n\n```bash\n# Install PyTorch. Refer to https://pytorch.org/get-started/previous-versions/ for version compatibility\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu${CUDA_VERSION}\n\npip install -e .\n\n# CUDA 12.6\npip install lmdeploy==0.9.2\n# CUDA 11.8\n# pip install https://github.com/InternLM/lmdeploy/releases/download/v0.9.2/lmdeploy-0.9.2+cu118-cp310-cp310-manylinux2014_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118\n```\n\n> [!IMPORTANT]\n> #### Fixing the **Shared Memory Error** on **20/30/40 series / V100 ...** GPUs (Optional)\n> \n> Our 3B model runs smoothly on the NVIDIA RTX 30/40 series. However, when using **LMDeploy** as the inference backend, you might run into compatibility issues on these GPUs ‚Äî typically this error:\n> \n> ```\n> triton.runtime.errors.OutOfResources: out of resource: shared memory\n> ```\n> \n> To resolve this issue, apply the following patch:\n> \n> ```bash\n> python tools/lmdeploy_patcher.py patch\n> ```\n> **Note:** This command modifies LMDeploy‚Äôs source code in your environment.\n> To undo the changes, simply run:\n> \n> ```bash\n> python tools/lmdeploy_patcher.py restore\n> ```\n> \n> Based on our tests on the **NVIDIA RTX 3090**, inference speed was **0.338 pages/second** using **LMDeploy** (with the patch applied), compared to only **0.015 pages/second** using **transformers**.\n> \n> **Special thanks to [@pineking](https://github.com/pineking) for the solution!**\n\n---\n\n### Using **vLLM** as the Inference Backend (Optional)\n> **Supporting CUDA 12.6/11.8**\n\n```bash\npip install uv --upgrade\n\nuv pip install vllm==0.9.1 --torch-backend=cu${CUDA_VERSION}\n\npip install -e .\n```\n\nThen, update the `chat_config.backend` field in your `model_configs.yaml` config file:\n\n```yaml\nchat_config:\n    backend: vllm\n```\n\n---\n\n### Using **transformers** as the Inference Backend (Optional)\n> **Supporting CUDA 12.6**\n\nInstall PyTorch and Flash Attention 2:\n```bash\n# Install PyTorch. Refer to https://pytorch.org/get-started/previous-versions/ for version compatibility\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n\npip install -e .\n\npip install flash-attn==2.7.4.post1 --no-build-isolation\n```\nThen, update the `chat_config` in your `model_configs.yaml` config file:\n```yaml\nchat_config:\n  backend: transformers\n  batch_size: 10  # Adjust based on your available GPU memory\n```\n\n\n"
  },
  {
    "file_name": "docs/install_paddlex.md",
    "file_contents": "# PP-DocLayout_plus-L Usage Guide\n\nWe have added support for the [PP-DocLayout_plus-L](https://huggingface.co/PaddlePaddle/PP-DocLayout_plus-L) model, which offers improved performance over `doclayout_yolo`.\n\nThis guide will walk you through the necessary steps to use the new model.\n\n## How to Use\n### **1.  Install Dependencies**\n\nTo use `PP-DocLayout_plus-L`, you must install two additional core libraries, **PaddlePaddle** and **PaddleX**, on top of the project's base environment (from `requirements.txt`).\n\n**Step 1: Install PaddlePaddle**\n\nPlease choose the command that corresponds to your **NVIDIA driver version** to install the GPU-accelerated version. Make sure your pytorch version is compatible with the PaddlePaddle version you are installing.\n\n```bash\n# gpuÔºårequires GPU driver version ‚â•450.80.02 (Linux) or ‚â•452.39 (Windows)\n python -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n\n# gpuÔºårequires GPU driver version ‚â•550.54.14 (Linux) or ‚â•550.54.14 (Windows)\n python -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n```\n\n**Step 2: Install PaddleX**\n\nExecute the following command to install the base version of PaddleX.\n```bash\npip install \"paddlex[base]\"\n```\n> [!NOTE]\n> \n> If the installation methods above are not suitable for your environment, or if you wish to explore more options, please refer to the official **[PaddleX](https://github.com/PaddlePaddle/PaddleX)**.\n\n### **2.  Modify the Configuration File**\n\nUpdate the `model` field in the [`model_configs.yaml`](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/model_configs.yaml#L7) file at the project root to `PP-DocLayout_plus-L`.\n\n```yaml\nlayout_config: \n  model: PP-DocLayout_plus-L # PP-DocLayout_plus-L / doclayout_yolo\n```\n> [!TIP]\n> \n> Model weights will be automatically downloaded to the default HuggingFace path the first time you run the program.\n> \n> To manually download and store PP-DocLayout_plus-L weight files in your configured models_dir directory, execute the following procedure:\n> \n> 1. Download PP-DocLayout_plus-L weights to your local `models_dir` directory (link: [ModelScope](https://modelscope.cn/models/PaddlePaddle/PP-DocLayout_plus-L),[HuggingFace](https://huggingface.co/PaddlePaddle/PP-DocLayout_plus-L))\n> 2. Add the following configuration to your [`model_configs.yaml`](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/model_configs.yaml) file:\n> ```yaml\n> weights:\n>   PP-DocLayout_plus-L: Structure/PP-DocLayout_plus-L # The relative path of models_dir\n> \n> layout_config: \n>   model: PP-DocLayout_plus-L # PP-DocLayout_plus-L / doclayout_yolo\n> ```\n"
  },
  {
    "file_name": "docs/windows_support.md",
    "file_contents": "# Windows Support\nFor Windows users, we provide three methods to run MonkeyOCR:\n1. Natively on Windows\n2. Using Windows Subsystem for Linux (WSL)\n3. Using WSL with Docker\n\n## Native Windows Support\nFollow the [installation guide](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#install-with-cuda-support) to set up your environment.\nDownload our model from Huggingface.\n```python\npip install huggingface_hub\n\npython tools/download_model.py\n```\nYou can also download our model from ModelScope.\n\n```python\npip install modelscope\n\npython tools/download_model.py -t modelscope\n```\nCopy and run the following command.\n```\npip install -U \"triton-windows<3.4\"\n```\nThen you can run MonkeyOCR normally.\n\n\n\n## Running with WSL2 Or WSL2 + Docker Desktop\n\n## Installing WSL2\n\nFirst, ensure your version of Windows supports WSL2.\n\n1.  Enable WSL.  \n    Launch PowerShell with administrator privileges.\n    * Enable the Virtual Machine Platform feature.\n    ```PowerShell\n    dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n    ```\n    * Enable the Windows Subsystem for Linux feature.\n    ```PowerShell\n    dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n    ```\n    * Restart your computer.\n\n2.  Install a Linux distribution.\n   ```PowerShell\n   wsl --install -d Ubuntu\n   ```\n\n3.  Download Docker Desktop.  \n    [Official Docker Website](https://www.docker.com/products/docker-desktop/)\n\n4.  Configure WSL config (Optional).  \n\n    If you need to quantize the model later, you might encounter issues with insufficient memory.\n\n    * Open your user profile folder.\n    * Enter `%UserProfile%` in the File Explorer address bar and press Enter.\n    * Create a `.wslconfig` file.\n    * Edit the file content with a code editor.\n    ```.wslconfig\n    [wsl2]\n    memory=24GB\n    ```\n    Other parameters can be set as needed.\n5. Enter WSL.\n    ```PowerShell\n    wsl\n    cd ~\n    ```\nIf you are only using the WSL method, after you enter the WSL terminal and have installed conda, you can then follow the [installation guide](https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md#install-with-cuda-support) to set up your environment.\n\n## Building the Container\n1. Run Docker Desktop.\n\n2. Enter WSL.\n    ```PowerShell\n    wsl\n    cd ~\n    ```\n3. Clone the repository.\n    ```PowerShell\n    git clone https://github.com/Yuliang-Liu/MonkeyOCR\n\n    cd MonkeyOCR\n    ```\n4. Follow the 'Docker Deployment' section in the [README.md](../README.md) file to create the Docker image.\n\nAfter entering the container, you can run MonkeyOCR normally.\n\nYou can use the `Dev Containers extension` in VS Code to connect to the container for convenient editing and modification.\n\nIf you encounter the error `RuntimeError: No enough gpu memory for runtime.`, it indicates insufficient VRAM. You can try quantizing the model.\nFor details, see [Quantization Method](Quantization.md).\n"
  },
  {
    "file_name": "magic_pdf/pdf_parse_union_core_v2_llm.py",
    "file_contents": "import copy\nimport math\nimport re\nimport statistics\nimport time\nfrom typing import List\n\nimport fitz\nimport torch\nfrom loguru import logger\n\nfrom magic_pdf.config.enums import SupportedPdfParseMethod\nfrom magic_pdf.config.ocr_content_type import BlockType, ContentType\nfrom magic_pdf.data.dataset import Dataset, PageableData\nfrom magic_pdf.libs.boxbase import calculate_overlap_area_in_bbox1_area_ratio, __is_overlaps_y_exceeds_threshold\nfrom magic_pdf.libs.clean_memory import clean_memory\nfrom magic_pdf.libs.convert_utils import dict_to_list\nfrom magic_pdf.libs.hash_utils import compute_md5\nfrom magic_pdf.libs.pdf_image_tools import cut_image_to_pil_image\nfrom magic_pdf.model.magic_model import MagicModel\n\n\nfrom magic_pdf.model.sub_modules.model_init import AtomModelSingleton\nfrom magic_pdf.post_proc.para_split_v3 import para_split\nfrom magic_pdf.pre_proc.construct_page_dict import ocr_construct_page_component_v2\nfrom magic_pdf.pre_proc.cut_image import ocr_cut_image_and_table\nfrom magic_pdf.pre_proc.ocr_detect_all_bboxes import ocr_prepare_bboxes_for_layout_split_v2\nfrom magic_pdf.pre_proc.ocr_dict_merge import fill_spans_in_blocks, fix_block_spans_v2, fix_discarded_block\nfrom magic_pdf.pre_proc.ocr_span_list_modify import get_qa_need_list_v2, remove_overlaps_low_confidence_spans, \\\n    remove_overlaps_min_spans, check_chars_is_overlap_in_span\n\n\ndef __replace_STX_ETX(text_str: str):\n    \"\"\"Replace \\u0002 and \\u0003, as these characters become garbled when extracted using pymupdf. In fact, they were originally quotation marks.\n    Drawback: This issue is only observed in English text; it has not been found in Chinese text so far.\n\n        Args:\n            text_str (str): raw text\n\n        Returns:\n            _type_: replaced text\n    \"\"\"  # noqa: E501\n    if text_str:\n        s = text_str.replace('\\u0002', \"'\")\n        s = s.replace('\\u0003', \"'\")\n        return s\n    return text_str\n\n\ndef __replace_0xfffd(text_str: str):\n    \"\"\"Replace \\ufffd, as these characters become garbled when extracted using pymupdf.\"\"\"\n    if text_str:\n        s = text_str.replace('\\ufffd', \" \")\n        return s\n    return text_str\n\n\n# Split ligature characters\ndef __replace_ligatures(text: str):\n    ligatures = {\n        'Ô¨Å': 'fi', 'Ô¨Ç': 'fl', 'Ô¨Ä': 'ff', 'Ô¨É': 'ffi', 'Ô¨Ñ': 'ffl', 'Ô¨Ö': 'ft', 'Ô¨Ü': 'st'\n    }\n    return re.sub('|'.join(map(re.escape, ligatures.keys())), lambda m: ligatures[m.group()], text)\n\n\ndef chars_to_content(span):\n    # Check if char in span is empty\n    if len(span['chars']) == 0:\n        pass\n        # span['content'] = ''\n    elif check_chars_is_overlap_in_span(span['chars']):\n        pass\n    else:\n        # First sort chars by x-coordinate of bbox center point\n        span['chars'] = sorted(span['chars'], key=lambda x: (x['bbox'][0] + x['bbox'][2]) / 2)\n\n        # Calculate average char width\n        char_width_sum = sum([char['bbox'][2] - char['bbox'][0] for char in span['chars']])\n        char_avg_width = char_width_sum / len(span['chars'])\n\n        content = ''\n        for char in span['chars']:\n\n            # If distance between next char's x0 and previous char's x1 exceeds 0.25 char width, insert a space\n            char1 = char\n            char2 = span['chars'][span['chars'].index(char) + 1] if span['chars'].index(char) + 1 < len(span['chars']) else None\n            if char2 and char2['bbox'][0] - char1['bbox'][2] > char_avg_width * 0.25 and char['c'] != ' ' and char2['c'] != ' ':\n                content += f\"{char['c']} \"\n            else:\n                content += char['c']\n\n        content = __replace_ligatures(content)\n        span['content'] = __replace_0xfffd(content)\n\n    del span['chars']\n\n\nLINE_STOP_FLAG = ('.', '!', '?', '„ÄÇ', 'ÔºÅ', 'Ôºü', ')', 'Ôºâ', '\"', '‚Äù', ':', 'Ôºö', ';', 'Ôºõ', ']', '„Äë', '}', '}', '>', '„Äã', '„ÄÅ', ',', 'Ôºå', '-', '‚Äî', '‚Äì',)\nLINE_START_FLAG = ('(', 'Ôºà', '\"', '‚Äú', '„Äê', '{', '„Ää', '<', '„Äå', '„Äé', '„Äê', '[',)\n\n\ndef fill_char_in_spans(spans, all_chars):\n\n    # Simple top-to-bottom sorting\n    spans = sorted(spans, key=lambda x: x['bbox'][1])\n\n    for char in all_chars:\n        # Skip chars with invalid bbox\n        # x1, y1, x2, y2 = char['bbox']\n        # if abs(x1 - x2) <= 0.01 or abs(y1 - y2) <= 0.01:\n        #     continue\n\n        for span in spans:\n            if calculate_char_in_span(char['bbox'], span['bbox'], char['c']):\n                span['chars'].append(char)\n                break\n\n    empty_spans = []\n\n    for span in spans:\n        chars_to_content(span)\n        # Some spans have no text but have one or two empty placeholders, filter by width/height and content length\n        if len(span['content']) * span['height'] < span['width'] * 0.5:\n            # logger.info(f\"maybe empty span: {len(span['content'])}, {span['height']}, {span['width']}\")\n            empty_spans.append(span)\n        del span['height'], span['width']\n    return empty_spans\n\n\n# Use more robust center point coordinate judgment\ndef calculate_char_in_span(char_bbox, span_bbox, char, span_height_radio=0.33):\n    char_center_x = (char_bbox[0] + char_bbox[2]) / 2\n    char_center_y = (char_bbox[1] + char_bbox[3]) / 2\n    span_center_y = (span_bbox[1] + span_bbox[3]) / 2\n    span_height = span_bbox[3] - span_bbox[1]\n\n    if (\n        span_bbox[0] < char_center_x < span_bbox[2]\n        and span_bbox[1] < char_center_y < span_bbox[3]\n        and abs(char_center_y - span_center_y) < span_height * span_height_radio\n    ):\n        return True\n    else:\n        if char in LINE_STOP_FLAG:\n            if (\n                (span_bbox[2] - span_height) < char_bbox[0] < span_bbox[2]\n                and char_center_x > span_bbox[0]\n                and span_bbox[1] < char_center_y < span_bbox[3]\n                and abs(char_center_y - span_center_y) < span_height * span_height_radio\n            ):\n                return True\n        elif char in LINE_START_FLAG:\n            if (\n                span_bbox[0] < char_bbox[2] < (span_bbox[0] + span_height)\n                and char_center_x < span_bbox[2]\n                and span_bbox[1] < char_center_y < span_bbox[3]\n                and abs(char_center_y - span_center_y) < span_height * span_height_radio\n            ):\n                return True\n        else:\n            return False\n\n\ndef remove_tilted_line(text_blocks):\n    for block in text_blocks:\n        remove_lines = []\n        for line in block['lines']:\n            cosine, sine = line['dir']\n            # Calculate radian value\n            angle_radians = math.atan2(sine, cosine)\n            # Convert radian value to degree value\n            angle_degrees = math.degrees(angle_radians)\n            if 2 < abs(angle_degrees) < 88:\n                remove_lines.append(line)\n        for line in remove_lines:\n            block['lines'].remove(line)\n\n\ndef txt_spans_extract_v2(pdf_page, spans, all_bboxes, all_discarded_blocks, lang):\n\n    # text_blocks_raw = pdf_page.get_text('rawdict', flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_MEDIABOX_CLIP)['blocks']\n\n\n    #text_blocks_raw = pdf_page.get_text('rawdict', flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_MEDIABOX_CLIP)['blocks']\n\n\n    text_blocks_raw = pdf_page.get_text('rawdict', flags=fitz.TEXTFLAGS_TEXT)['blocks']\n    # text_blocks = pdf_page.get_text('dict', flags=fitz.TEXTFLAGS_TEXT)['blocks']\n\n\n    remove_tilted_line(text_blocks_raw)\n\n    all_pymu_chars = []\n    for block in text_blocks_raw:\n        for line in block['lines']:\n            cosine, sine = line['dir']\n            if abs(cosine) < 0.9 or abs(sine) > 0.1:\n                continue\n            for span in line['spans']:\n                all_pymu_chars.extend(span['chars'])\n\n    # Calculate median height of all spans\n    span_height_list = []\n    for span in spans:\n        if span['type'] in [ContentType.InterlineEquation, ContentType.Image, ContentType.Table]:\n            continue\n        span_height = span['bbox'][3] - span['bbox'][1]\n        span['height'] = span_height\n        span['width'] = span['bbox'][2] - span['bbox'][0]\n        span_height_list.append(span_height)\n    if len(span_height_list) == 0:\n        return spans\n    else:\n        median_span_height = statistics.median(span_height_list)\n\n    useful_spans = []\n    unuseful_spans = []\n    # Two characteristics of vertical spans: 1. Height exceeds multiple lines 2. Aspect ratio exceeds certain value\n    vertical_spans = []\n    for span in spans:\n        if span['type'] in [ContentType.InterlineEquation, ContentType.Image, ContentType.Table]:\n            continue\n        for block in all_bboxes + all_discarded_blocks:\n            if block[7] in [BlockType.ImageBody, BlockType.TableBody, BlockType.InterlineEquation]:\n                continue\n            if calculate_overlap_area_in_bbox1_area_ratio(span['bbox'], block[0:4]) > 0.5:\n                if span['height'] > median_span_height * 3 and span['height'] > span['width'] * 3:\n                    vertical_spans.append(span)\n                elif block in all_bboxes:\n                    useful_spans.append(span)\n                else:\n                    unuseful_spans.append(span)\n\n                break\n\n    if len(vertical_spans) > 0:\n        text_blocks = pdf_page.get_text('dict', flags=fitz.TEXTFLAGS_TEXT)['blocks']\n        all_pymu_lines = []\n        for block in text_blocks:\n            for line in block['lines']:\n                all_pymu_lines.append(line)\n\n        for pymu_line in all_pymu_lines:\n            for span in vertical_spans:\n                if calculate_overlap_area_in_bbox1_area_ratio(pymu_line['bbox'], span['bbox']) > 0.5:\n                    for pymu_span in pymu_line['spans']:\n                        span['content'] += pymu_span['text']\n                    break\n\n        for span in vertical_spans:\n            if len(span['content']) == 0:\n                spans.remove(span)\n\n    new_spans = []\n\n    for span in useful_spans + unuseful_spans:\n        if span['type'] in [ContentType.Text]:\n            span['chars'] = []\n            new_spans.append(span)\n\n    empty_spans = fill_char_in_spans(new_spans, all_pymu_chars)\n\n    if len(empty_spans) > 0:\n\n\n        atom_model_manager = AtomModelSingleton()\n        ocr_model = atom_model_manager.get_atom_model(\n            atom_model_name='ocr',\n            ocr_show_log=False,\n            det_db_box_thresh=0.3,\n            lang=lang\n        )\n\n        for span in empty_spans:\n\n            span_img = cut_image_to_pil_image(span['bbox'], pdf_page, mode='cv2')\n            ocr_res = ocr_model.ocr(span_img, det=False)\n            if ocr_res and len(ocr_res) > 0:\n                if len(ocr_res[0]) > 0:\n                    ocr_text, ocr_score = ocr_res[0][0]\n                    # logger.info(f\"ocr_text: {ocr_text}, ocr_score: {ocr_score}\")\n                    if ocr_score > 0.5 and len(ocr_text) > 0:\n                        span['content'] = ocr_text\n                        span['score'] = ocr_score\n                    else:\n                        spans.remove(span)\n\n    return spans\n\n\ndef do_predict(boxes: List[List[int]], model) -> List[int]:\n    from magic_pdf.model.sub_modules.reading_oreder.layoutreader.helpers import (\n        boxes2inputs, parse_logits, prepare_inputs)\n\n    inputs = boxes2inputs(boxes)\n    inputs = prepare_inputs(inputs, model)\n    logits = model(**inputs).logits.cpu().squeeze(0)\n    return parse_logits(logits, len(boxes))\n\n\ndef cal_block_index(fix_blocks, sorted_bboxes):\n\n    if sorted_bboxes is not None:\n\n        for block in fix_blocks:\n            block['index'] = sorted_bboxes.index(block['bbox'])\n    else:\n\n        block_bboxes = []\n        for block in fix_blocks:\n\n            block['bbox'] = [max(0, x) for x in block['bbox']]\n            block_bboxes.append(block['bbox'])\n\n\n            if block['type'] in [BlockType.ImageBody, BlockType.TableBody]:\n                block['virtual_lines'] = copy.deepcopy(block['lines'])\n                block['lines'] = copy.deepcopy(block['real_lines'])\n                del block['real_lines']\n\n        import numpy as np\n\n        from magic_pdf.model.sub_modules.reading_oreder.layoutreader.xycut import \\\n            recursive_xy_cut\n\n        random_boxes = np.array(block_bboxes)\n        np.random.shuffle(random_boxes)\n        res = []\n        recursive_xy_cut(np.asarray(random_boxes).astype(int), np.arange(len(block_bboxes)), res)\n        assert len(res) == len(block_bboxes)\n        sorted_boxes = random_boxes[np.array(res)].tolist()\n\n        for i, block in enumerate(fix_blocks):\n            block['index'] = sorted_boxes.index(block['bbox'])\n\n\n        sorted_blocks = sorted(fix_blocks, key=lambda b: b['index'])\n        line_inedx = 1\n        for block in sorted_blocks:\n            for line in block['lines']:\n                line['index'] = line_inedx\n                line_inedx += 1\n\n    return fix_blocks\n\n\ndef insert_lines_into_block(block_bbox, line_height, page_w, page_h):\n\n    x0, y0, x1, y1 = block_bbox\n\n    block_height = y1 - y0\n    block_weight = x1 - x0\n\n\n    if line_height * 2 < block_height:\n        if (\n            block_height > page_h * 0.25 and page_w * 0.5 > block_weight > page_w * 0.25\n        ):\n            lines = int(block_height / line_height) + 1\n        else:\n\n            if block_weight > page_w * 0.4:\n                lines = 3\n                line_height = (y1 - y0) / lines\n            elif block_weight > page_w * 0.25:\n                lines = int(block_height / line_height) + 1\n            else:\n                if block_height / block_weight > 1.2:\n                    return [[x0, y0, x1, y1]]\n                else:\n                    lines = 2\n                    line_height = (y1 - y0) / lines\n\n\n        current_y = y0\n\n\n        lines_positions = []\n\n        for i in range(lines):\n            lines_positions.append([x0, current_y, x1, current_y + line_height])\n            current_y += line_height\n        return lines_positions\n\n    else:\n        return [[x0, y0, x1, y1]]\n\n\ndef sort_lines_by_model(fix_blocks, page_w, page_h, line_height, MonkeyOCR_model):\n    page_line_list = []\n\n    def add_lines_to_block(b):\n        line_bboxes = insert_lines_into_block(b['bbox'], line_height, page_w, page_h)\n        b['lines'] = []\n        for line_bbox in line_bboxes:\n            b['lines'].append({'bbox': line_bbox, 'spans': []})\n        page_line_list.extend(line_bboxes)\n\n    for block in fix_blocks:\n        page_line_list.append(block['bbox'])\n\n    if len(page_line_list) > 200:\n        return None\n\n\n    x_scale = 1000.0 / page_w\n    y_scale = 1000.0 / page_h\n    boxes = []\n    # logger.info(f\"Scale: {x_scale}, {y_scale}, Boxes len: {len(page_line_list)}\")\n    for left, top, right, bottom in page_line_list:\n        if left < 0:\n            logger.warning(\n                f'left < 0, left: {left}, right: {right}, top: {top}, bottom: {bottom}, page_w: {page_w}, page_h: {page_h}'\n            )  # noqa: E501\n            left = 0\n        if right > page_w:\n            logger.warning(\n                f'right > page_w, left: {left}, right: {right}, top: {top}, bottom: {bottom}, page_w: {page_w}, page_h: {page_h}'\n            )  # noqa: E501\n            right = page_w\n        if top < 0:\n            logger.warning(\n                f'top < 0, left: {left}, right: {right}, top: {top}, bottom: {bottom}, page_w: {page_w}, page_h: {page_h}'\n            )  # noqa: E501\n            top = 0\n        if bottom > page_h:\n            logger.warning(\n                f'bottom > page_h, left: {left}, right: {right}, top: {top}, bottom: {bottom}, page_w: {page_w}, page_h: {page_h}'\n            )  # noqa: E501\n            bottom = page_h\n\n        left = round(left * x_scale)\n        top = round(top * y_scale)\n        right = round(right * x_scale)\n        bottom = round(bottom * y_scale)\n        assert (\n            1000 >= right >= left >= 0 and 1000 >= bottom >= top >= 0\n        ), f'Invalid box. right: {right}, left: {left}, bottom: {bottom}, top: {top}'  # noqa: E126, E121\n        boxes.append([left, top, right, bottom])\n    model = MonkeyOCR_model.layoutreader_model\n    with torch.no_grad():\n        orders = do_predict(boxes, model)\n    sorted_bboxes = [page_line_list[i] for i in orders]\n\n    return sorted_bboxes\n\n\ndef get_line_height(blocks):\n    page_line_height_list = []\n    for block in blocks:\n        if block['type'] in [\n            BlockType.Text, BlockType.Title,\n            BlockType.ImageCaption, BlockType.ImageFootnote,\n            BlockType.TableCaption, BlockType.TableFootnote\n        ]:\n            for line in block['lines']:\n                bbox = line['bbox']\n                page_line_height_list.append(int(bbox[3] - bbox[1]))\n    if len(page_line_height_list) > 0:\n        return statistics.median(page_line_height_list)\n    else:\n        return 10\n\n\ndef process_groups(groups, body_key, caption_key, footnote_key):\n    body_blocks = []\n    caption_blocks = []\n    footnote_blocks = []\n    for i, group in enumerate(groups):\n        group[body_key]['group_id'] = i\n        body_blocks.append(group[body_key])\n        for caption_block in group[caption_key]:\n            caption_block['group_id'] = i\n            caption_blocks.append(caption_block)\n        for footnote_block in group[footnote_key]:\n            footnote_block['group_id'] = i\n            footnote_blocks.append(footnote_block)\n    return body_blocks, caption_blocks, footnote_blocks\n\n\ndef process_block_list(blocks, body_type, block_type):\n    indices = [block['index'] for block in blocks]\n    median_index = statistics.median(indices)\n\n    body_bbox = next((block['bbox'] for block in blocks if block.get('type') == body_type), [])\n\n    return {\n        'type': block_type,\n        'bbox': body_bbox,\n        'blocks': blocks,\n        'index': median_index,\n    }\n\n\ndef revert_group_blocks(blocks):\n    image_groups = {}\n    table_groups = {}\n    new_blocks = []\n    for block in blocks:\n        if block['type'] in [BlockType.ImageBody, BlockType.ImageCaption, BlockType.ImageFootnote]:\n            group_id = block['group_id']\n            if group_id not in image_groups:\n                image_groups[group_id] = []\n            image_groups[group_id].append(block)\n        elif block['type'] in [BlockType.TableBody, BlockType.TableCaption, BlockType.TableFootnote]:\n            group_id = block['group_id']\n            if group_id not in table_groups:\n                table_groups[group_id] = []\n            table_groups[group_id].append(block)\n        else:\n            new_blocks.append(block)\n\n    for group_id, blocks in image_groups.items():\n        new_blocks.append(process_block_list(blocks, BlockType.ImageBody, BlockType.Image))\n\n    for group_id, blocks in table_groups.items():\n        new_blocks.append(process_block_list(blocks, BlockType.TableBody, BlockType.Table))\n\n    return new_blocks\n\n\ndef remove_outside_spans(spans, all_bboxes, all_discarded_blocks):\n    def get_block_bboxes(blocks, block_type_list):\n        return [block[0:4] for block in blocks if block[7] in block_type_list]\n\n    image_bboxes = get_block_bboxes(all_bboxes, [BlockType.ImageBody])\n    table_bboxes = get_block_bboxes(all_bboxes, [BlockType.TableBody])\n    other_block_type = []\n    for block_type in BlockType.__dict__.values():\n        if not isinstance(block_type, str):\n            continue\n        if block_type not in [BlockType.ImageBody, BlockType.TableBody]:\n            other_block_type.append(block_type)\n    other_block_bboxes = get_block_bboxes(all_bboxes, other_block_type)\n    discarded_block_bboxes = get_block_bboxes(all_discarded_blocks, [BlockType.Discarded])\n\n    new_spans = []\n\n    for span in spans:\n        span_bbox = span['bbox']\n        span_type = span['type']\n\n        if any(calculate_overlap_area_in_bbox1_area_ratio(span_bbox, block_bbox) > 0.4 for block_bbox in\n               discarded_block_bboxes):\n            new_spans.append(span)\n            continue\n\n        if span_type == ContentType.Image:\n            if any(calculate_overlap_area_in_bbox1_area_ratio(span_bbox, block_bbox) > 0.5 for block_bbox in\n                   image_bboxes):\n                new_spans.append(span)\n        elif span_type == ContentType.Table:\n            if any(calculate_overlap_area_in_bbox1_area_ratio(span_bbox, block_bbox) > 0.5 for block_bbox in\n                   table_bboxes):\n                new_spans.append(span)\n        else:\n            if any(calculate_overlap_area_in_bbox1_area_ratio(span_bbox, block_bbox) > 0.5 for block_bbox in\n                   other_block_bboxes):\n                new_spans.append(span)\n\n    return new_spans\n\n\ndef parse_page_core(\n    page_doc: PageableData, magic_model, page_id, pdf_bytes_md5, imageWriter, parse_mode, lang, MonkeyOCR_model\n):\n    need_drop = False\n    drop_reason = []\n\n    img_groups = magic_model.get_imgs_v2(page_id)\n    table_groups = magic_model.get_tables_v2(page_id)\n\n    img_body_blocks, img_caption_blocks, img_footnote_blocks = process_groups(\n        img_groups, 'image_body', 'image_caption_list', 'image_footnote_list'\n    )\n\n    table_body_blocks, table_caption_blocks, table_footnote_blocks = process_groups(\n        table_groups, 'table_body', 'table_caption_list', 'table_footnote_list'\n    )\n\n    discarded_blocks = magic_model.get_discarded(page_id)\n    text_blocks = magic_model.get_text_blocks(page_id)\n    title_blocks = magic_model.get_title_blocks(page_id)\n    inline_equations, interline_equations, interline_equation_blocks = magic_model.get_equations(page_id)\n    page_w, page_h = magic_model.get_page_size(page_id)\n\n    def merge_title_blocks(blocks, x_distance_threshold=0.1*page_w):\n        def merge_two_bbox(b1, b2):\n            x_min = min(b1['bbox'][0], b2['bbox'][0])\n            y_min = min(b1['bbox'][1], b2['bbox'][1])\n            x_max = max(b1['bbox'][2], b2['bbox'][2])\n            y_max = max(b1['bbox'][3], b2['bbox'][3])\n            return x_min, y_min, x_max, y_max\n\n        def merge_two_blocks(b1, b2):\n\n            b1['bbox'] = merge_two_bbox(b1, b2)\n\n\n            line1 = b1['lines'][0]\n            line2 = b2['lines'][0]\n            line1['bbox'] = merge_two_bbox(line1, line2)\n            line1['spans'].extend(line2['spans'])\n\n            return b1, b2\n\n\n        y_overlapping_blocks = []\n        title_bs = [b for b in blocks if b['type'] == BlockType.Title]\n        while title_bs:\n            block1 = title_bs.pop(0)\n            current_row = [block1]\n            to_remove = []\n            for block2 in title_bs:\n                if (\n                    __is_overlaps_y_exceeds_threshold(block1['bbox'], block2['bbox'], 0.9)\n                    and len(block1['lines']) == 1\n                    and len(block2['lines']) == 1\n                ):\n                    current_row.append(block2)\n                    to_remove.append(block2)\n            for b in to_remove:\n                title_bs.remove(b)\n            y_overlapping_blocks.append(current_row)\n\n\n        to_remove_blocks = []\n        for row in y_overlapping_blocks:\n            if len(row) == 1:\n                continue\n\n\n            row.sort(key=lambda x: x['bbox'][0])\n\n            merged_block = row[0]\n            for i in range(1, len(row)):\n                left_block = merged_block\n                right_block = row[i]\n\n                left_height = left_block['bbox'][3] - left_block['bbox'][1]\n                right_height = right_block['bbox'][3] - right_block['bbox'][1]\n\n                if (\n                    right_block['bbox'][0] - left_block['bbox'][2] < x_distance_threshold\n                    and left_height * 0.95 < right_height < left_height * 1.05\n                ):\n                    merged_block, to_remove_block = merge_two_blocks(merged_block, right_block)\n                    to_remove_blocks.append(to_remove_block)\n                else:\n                    merged_block = right_block\n\n        for b in to_remove_blocks:\n            blocks.remove(b)\n\n\n    interline_equation_blocks = []\n    if len(interline_equation_blocks) > 0:\n        all_bboxes, all_discarded_blocks = ocr_prepare_bboxes_for_layout_split_v2(\n            img_body_blocks, img_caption_blocks, img_footnote_blocks,\n            table_body_blocks, table_caption_blocks, table_footnote_blocks,\n            discarded_blocks,\n            text_blocks,\n            title_blocks,\n            interline_equation_blocks,\n            page_w,\n            page_h,\n        )\n    else:\n        all_bboxes, all_discarded_blocks = ocr_prepare_bboxes_for_layout_split_v2(\n            img_body_blocks, img_caption_blocks, img_footnote_blocks,\n            table_body_blocks, table_caption_blocks, table_footnote_blocks,\n            discarded_blocks,\n            text_blocks,\n            title_blocks,\n            interline_equations,\n            page_w,\n            page_h,\n        )\n\n    spans = magic_model.get_all_spans(page_id)\n\n    spans = remove_outside_spans(spans, all_bboxes, all_discarded_blocks)\n\n    spans, dropped_spans_by_confidence = remove_overlaps_low_confidence_spans(spans)\n    spans, dropped_spans_by_span_overlap = remove_overlaps_min_spans(spans)\n\n    if parse_mode == SupportedPdfParseMethod.TXT:\n\n        spans = txt_spans_extract_v2(page_doc, spans, all_bboxes, all_discarded_blocks, lang)\n\n    elif parse_mode == SupportedPdfParseMethod.OCR:\n        pass\n    else:\n        raise Exception('parse_mode must be txt or ocr')\n\n    discarded_block_with_spans, spans = fill_spans_in_blocks(\n        all_discarded_blocks, spans, 0.4\n    )\n    fix_discarded_blocks = fix_discarded_block(discarded_block_with_spans)\n\n    if len(all_bboxes) == 0:\n        logger.warning(f'skip this page, not found useful bbox, page_id: {page_id}')\n        return ocr_construct_page_component_v2(\n            [],\n            [],\n            page_id,\n            page_w,\n            page_h,\n            [],\n            [],\n            [],\n            interline_equations,\n            fix_discarded_blocks,\n            need_drop,\n            drop_reason,\n        )\n\n    spans = ocr_cut_image_and_table(\n        spans, page_doc, page_id, pdf_bytes_md5, imageWriter\n    )\n\n    block_with_spans, spans = fill_spans_in_blocks(all_bboxes, spans, 0.5)\n\n    fix_blocks = fix_block_spans_v2(block_with_spans)\n\n    merge_title_blocks(fix_blocks)\n\n    line_height = get_line_height(fix_blocks)\n\n    sorted_bboxes = sort_lines_by_model(fix_blocks, page_w, page_h, line_height, MonkeyOCR_model)\n\n    fix_blocks = cal_block_index(fix_blocks, sorted_bboxes)\n\n    fix_blocks = revert_group_blocks(fix_blocks)\n\n    sorted_blocks = sorted(fix_blocks, key=lambda b: b['index'])\n\n    for block in sorted_blocks:\n        if block['type'] in [BlockType.Image, BlockType.Table]:\n            block['blocks'] = sorted(block['blocks'], key=lambda b: b['index'])\n\n    images, tables, interline_equations = get_qa_need_list_v2(sorted_blocks)\n\n    page_info = ocr_construct_page_component_v2(\n        sorted_blocks,\n        [],\n        page_id,\n        page_w,\n        page_h,\n        [],\n        images,\n        tables,\n        interline_equations,\n        fix_discarded_blocks,\n        need_drop,\n        drop_reason,\n    )\n    return page_info\n\n\ndef pdf_parse_union(\n    model_list,\n    dataset: Dataset,\n    imageWriter,\n    parse_mode,\n    MonkeyOCR_model,\n    start_page_id=0,\n    end_page_id=None,\n    debug_mode=False,\n    lang=None,\n):\n\n    pdf_bytes_md5 = compute_md5(dataset.data_bits())\n\n    pdf_info_dict = {}\n\n    magic_model = MagicModel(model_list, dataset)\n\n    # end_page_id = end_page_id if end_page_id else len(pdf_docs) - 1\n    end_page_id = (\n        end_page_id\n        if end_page_id is not None and end_page_id >= 0\n        else len(dataset) - 1\n    )\n\n    if end_page_id > len(dataset) - 1:\n        logger.warning('end_page_id is out of range, use pdf_docs length')\n        end_page_id = len(dataset) - 1\n\n    start_time = time.time()\n\n    for page_id, page in enumerate(dataset):\n        if debug_mode:\n            time_now = time.time()\n            logger.info(\n                f'page_id: {page_id}, last_page_cost_time: {round(time.time() - start_time, 2)}'\n            )\n            start_time = time_now\n\n        if start_page_id <= page_id <= end_page_id:\n            page_info = parse_page_core(\n                page, magic_model, page_id, pdf_bytes_md5, imageWriter, parse_mode, lang, MonkeyOCR_model\n            )\n        else:\n            page_info = page.get_page_info()\n            page_w = page_info.w\n            page_h = page_info.h\n            page_info = ocr_construct_page_component_v2(\n                [], [], page_id, page_w, page_h, [], [], [], [], [], True, 'skip page'\n            )\n        pdf_info_dict[f'page_{page_id}'] = page_info\n\n    para_split(pdf_info_dict)\n\n    pdf_info_list = dict_to_list(pdf_info_dict)\n    new_pdf_info_dict = {\n        'pdf_info': pdf_info_list,\n    }\n\n    clean_memory(MonkeyOCR_model.device)\n\n    return new_pdf_info_dict\n\n\nif __name__ == '__main__':\n    pass"
  },
  {
    "file_name": "tools/download_model.py",
    "file_contents": "from argparse import ArgumentParser\nimport os\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('--type', '-t', type=str, default=\"huggingface\") # huggingface or modelscope\n    parser.add_argument('--name', '-n', type=str, default=\"MonkeyOCR-pro-3B\") # MonkeyOCR or MonkeyOCR-pro-1.2B\n    args = parser.parse_args()\n    script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    model_dir = os.path.join(script_dir, \"model_weight\")\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    pp_dir = os.path.join(model_dir, \"Structure/PP-DocLayout_plus-L\")\n    if not os.path.exists(pp_dir):\n        os.makedirs(pp_dir)\n    if args.type == \"huggingface\":\n        from huggingface_hub import snapshot_download\n        snapshot_download(repo_id=\"echo840/\"+args.name, local_dir=model_dir, local_dir_use_symlinks=False, resume_download=True)\n        snapshot_download(repo_id=\"PaddlePaddle/PP-DocLayout_plus-L\", local_dir=pp_dir, local_dir_use_symlinks=False, resume_download=True)\n    elif args.type == \"modelscope\":\n        from modelscope import snapshot_download\n        snapshot_download(repo_id = 'l1731396519/'+args.name,local_dir=model_dir)\n        snapshot_download(repo_id=\"PaddlePaddle/PP-DocLayout_plus-L\", local_dir=pp_dir)\n"
  },
  {
    "file_name": "tools/fix_qwen2_5_vl_awq.py",
    "file_contents": "#!/usr/bin/env python3\nimport os\nimport shutil\nimport sys\n\ndef find_lmdeploy_calibrate_file():\n    \"\"\"Automatically find the lmdeploy calibrate.py file in current environment\"\"\"\n    try:\n        import lmdeploy\n        lmdeploy_path = os.path.dirname(lmdeploy.__file__)\n        calibrate_file = os.path.join(lmdeploy_path, 'lite', 'apis', 'calibrate.py')\n        \n        if os.path.exists(calibrate_file):\n            return calibrate_file\n        else:\n            print(f\"Error: calibrate.py file not found, expected path: {calibrate_file}\")\n            return None\n    except ImportError:\n        print(\"Error: lmdeploy is not installed in current environment\")\n        return None\n\ndef patch_calibrate_file(calibrate_file):\n    \"\"\"Patch the calibrate.py file by commenting out problematic code\"\"\"\n    # Read file content\n    try:\n        with open(calibrate_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n    except Exception as e:\n        print(f\"Error: Failed to read file - {e}\")\n        return False\n\n    # Check if already patched\n    if \"# if hasattr(vl_model, 'language_model')\" in content:\n        print(\"‚úì File already patched\")\n        return True\n\n    # Replace problematic code\n    old_code = \"\"\"        if hasattr(vl_model, 'language_model'):  # deepseek-vl, ...\n            model = vl_model.language_model\n        if hasattr(vl_model, 'llm'):  # MiniCPMV, ...\n            model = vl_model.llm\"\"\"\n\n    new_code = \"\"\"        # if hasattr(vl_model, 'language_model'):  # deepseek-vl, ...\n        #     model = vl_model.language_model\n        # if hasattr(vl_model, 'llm'):  # MiniCPMV, ...\n        #     model = vl_model.llm\"\"\"\n\n    if old_code in content:\n        content = content.replace(old_code, new_code)\n        \n        # Write back to file\n        try:\n            with open(calibrate_file, 'w', encoding='utf-8') as f:\n                f.write(content)\n            print(\"‚úì Patch applied successfully!\")\n            return True\n        except Exception as e:\n            print(f\"Error: Failed to write file - {e}\")\n            return False\n    else:\n        print(\"‚ö† Expected code snippet not found, lmdeploy version might be different\")\n        print(\"Please check the file content manually\")\n        return False\n\ndef restore_calibrate_file(calibrate_file):\n    \"\"\"Restore the calibrate.py file by removing comment symbols\"\"\"\n    # Check if backup exists\n    backup_file = calibrate_file + \".backup\"\n    if os.path.exists(backup_file):\n        try:\n            shutil.copy(backup_file, calibrate_file)\n            print(\"‚úì File restored from backup\")\n            return True\n        except Exception as e:\n            print(f\"Error: Failed to restore from backup - {e}\")\n            return False\n    \n    # Manual restore by uncommenting\n    try:\n        with open(calibrate_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n    except Exception as e:\n        print(f\"Error: Failed to read file - {e}\")\n        return False\n\n    # Check if file is in patched state\n    if \"# if hasattr(vl_model, 'language_model')\" not in content:\n        print(\"‚úì File is already in original state\")\n        return True\n\n    # Restore by uncommenting\n    patched_code = \"\"\"        # if hasattr(vl_model, 'language_model'):  # deepseek-vl, ...\n        #     model = vl_model.language_model\n        # if hasattr(vl_model, 'llm'):  # MiniCPMV, ...\n        #     model = vl_model.llm\"\"\"\n\n    original_code = \"\"\"        if hasattr(vl_model, 'language_model'):  # deepseek-vl, ...\n            model = vl_model.language_model\n        if hasattr(vl_model, 'llm'):  # MiniCPMV, ...\n            model = vl_model.llm\"\"\"\n\n    if patched_code in content:\n        content = content.replace(patched_code, original_code)\n        \n        try:\n            with open(calibrate_file, 'w', encoding='utf-8') as f:\n                f.write(content)\n            print(\"‚úì File restored successfully!\")\n            return True\n        except Exception as e:\n            print(f\"Error: Failed to write file - {e}\")\n            return False\n    else:\n        print(\"‚ö† Patched code not found, cannot restore\")\n        return False\n\ndef show_usage():\n    \"\"\"Show usage information\"\"\"\n    print(\"Usage:\")\n    print(\"  python fix_qwen2_5_vl_awq.py patch     # Apply patch for Qwen2.5-VL AWQ quantization\")\n    print(\"  python fix_qwen2_5_vl_awq.py restore   # Restore original file\")\n\ndef main():\n    if len(sys.argv) != 2 or sys.argv[1] not in ['patch', 'restore']:\n        show_usage()\n        sys.exit(1)\n    \n    command = sys.argv[1]\n    \n    print(\"Auto-detecting lmdeploy installation path...\")\n    \n    # Automatically find calibrate.py file\n    calibrate_file = find_lmdeploy_calibrate_file()\n    if not calibrate_file:\n        sys.exit(1)\n    \n    print(f\"Found file: {calibrate_file}\")\n    \n    if command == 'patch':\n        # Backup original file before patching\n        backup_file = calibrate_file + \".backup\"\n        if not os.path.exists(backup_file):\n            shutil.copy(calibrate_file, backup_file)\n            print(\"‚úì Original file backed up\")\n        else:\n            print(\"‚úì Backup file already exists\")\n        \n        if patch_calibrate_file(calibrate_file):\n            print(\"\\nüéâ Now you can run Qwen2.5-VL AWQ quantization!\")\n            print(\"Use command:\")\n            print(\"lmdeploy lite auto_awq \\\\\")\n            print(\"    ./model_weight/Recognition \\\\\")\n            print(\"    --calib-dataset 'ptb' \\\\\")\n            print(\"    --calib-samples 64 \\\\\")\n            print(\"    --calib-seqlen 1024 \\\\\")\n            print(\"    --w-bits 4 \\\\\")\n            print(\"    --w-group-size 128 \\\\\")\n            print(\"    --batch-size 1 \\\\\")\n            print(\"    --work-dir ./monkeyocr_quantization\")\n    \n    elif command == 'restore':\n        if restore_calibrate_file(calibrate_file):\n            print(\"\\n‚úì File has been restored to original state\")\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "file_name": "tools/lmdeploy_patcher.py",
    "file_contents": "import os\nimport shutil\nfrom loguru import logger\n\nclass LMDeployPatcher:\n    def __init__(self):\n        self.lmdeploy_path = self._find_lmdeploy_path()\n        self.flashattention_file = None\n        self.backup_file = None\n        self.target_line = \"BLOCK_M = min(128, BLOCK_M)\"\n        self.new_line = \"BLOCK_N = min(64, BLOCK_N)\"\n        \n        if self.lmdeploy_path:\n            self.flashattention_file = os.path.join(\n                self.lmdeploy_path, \n                \"pytorch\", \"kernels\", \"cuda\", \"flashattention.py\"\n            )\n            self.backup_file = self.flashattention_file + \".backup\"\n            logger.info(f\"Found LMDeploy path: {self.lmdeploy_path}\")\n            logger.info(f\"Target file: {self.flashattention_file}\")\n        else:\n            logger.error(\"LMDeploy installation path not found\")\n    \n    def _find_lmdeploy_path(self):\n        \"\"\"Find the installation path of LMDeploy library\"\"\"\n        try:\n            import lmdeploy\n            lmdeploy_path = os.path.dirname(lmdeploy.__file__)\n            logger.info(f\"Found LMDeploy via import: {lmdeploy_path}\")\n            return lmdeploy_path\n        except ImportError:\n            logger.warning(\"Cannot import lmdeploy directly\")\n        \n        # Try to find from common installation paths\n        possible_paths = [\n            # Conda environments\n            os.path.expanduser(\"~/anaconda3/envs/*/lib/python*/site-packages/lmdeploy\"),\n            os.path.expanduser(\"~/miniconda3/envs/*/lib/python*/site-packages/lmdeploy\"),\n            # System Python\n            \"/usr/local/lib/python*/site-packages/lmdeploy\",\n            \"/usr/lib/python*/site-packages/lmdeploy\",\n            # User local installation\n            os.path.expanduser(\"~/.local/lib/python*/site-packages/lmdeploy\"),\n        ]\n        \n        import glob\n        for pattern in possible_paths:\n            matches = glob.glob(pattern)\n            for match in matches:\n                if os.path.isdir(match) and os.path.exists(os.path.join(match, \"pytorch\", \"kernels\", \"cuda\", \"flashattention.py\")):\n                    logger.info(f\"Found LMDeploy path: {match}\")\n                    return match\n        \n        # Finally try using pip show\n        try:\n            import subprocess\n            result = subprocess.run(['pip', 'show', 'lmdeploy'], capture_output=True, text=True)\n            if result.returncode == 0:\n                for line in result.stdout.split('\\n'):\n                    if line.startswith('Location:'):\n                        location = line.split(':', 1)[1].strip()\n                        lmdeploy_path = os.path.join(location, 'lmdeploy')\n                        if os.path.exists(lmdeploy_path):\n                            logger.info(f\"Found LMDeploy via pip show: {lmdeploy_path}\")\n                            return lmdeploy_path\n        except Exception as e:\n            logger.warning(f\"pip show failed: {e}\")\n        \n        return None\n    \n    def _check_file_exists(self):\n        \"\"\"Check if the target file exists\"\"\"\n        if not self.flashattention_file or not os.path.exists(self.flashattention_file):\n            logger.error(f\"File does not exist: {self.flashattention_file}\")\n            return False\n        return True\n    \n    def _create_backup(self):\n        \"\"\"Create backup file\"\"\"\n        if not self._check_file_exists():\n            return False\n        \n        try:\n            shutil.copy2(self.flashattention_file, self.backup_file)\n            logger.info(f\"Backup created: {self.backup_file}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to create backup: {e}\")\n            return False\n    \n    def _find_target_line(self, content):\n        \"\"\"Find the position of target line (second occurrence)\"\"\"\n        lines = content.split('\\n')\n        occurrence_count = 0\n        for i, line in enumerate(lines):\n            if self.target_line in line:\n                occurrence_count += 1\n                if occurrence_count == 2:  # Return only the second occurrence\n                    return i, lines\n        return -1, lines\n    \n    def _get_line_indentation(self, line):\n        \"\"\"Get the indentation of a line\"\"\"\n        return len(line) - len(line.lstrip())\n    \n    def patch(self):\n        \"\"\"Apply patch\"\"\"\n        if not self._check_file_exists():\n            return False\n        \n        # Create backup\n        if not self._create_backup():\n            return False\n        \n        try:\n            # Read file content\n            with open(self.flashattention_file, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            # Find target line\n            target_line_idx, lines = self._find_target_line(content)\n            if target_line_idx == -1:\n                logger.error(f\"Target line not found: {self.target_line}\")\n                return False\n            \n            # Check if already modified\n            if target_line_idx + 1 < len(lines) and self.new_line in lines[target_line_idx + 1]:\n                logger.warning(\"File seems to be already modified, skipping\")\n                return True\n            \n            # Get indentation of target line\n            target_line = lines[target_line_idx]\n            indentation = ' ' * self._get_line_indentation(target_line)\n            new_line_with_indent = indentation + self.new_line\n            \n            # Insert new line after target line with proper indentation\n            lines.insert(target_line_idx + 1, new_line_with_indent)\n            \n            # Write back to file\n            with open(self.flashattention_file, 'w', encoding='utf-8') as f:\n                f.write('\\n'.join(lines))\n            \n            logger.info(f\"Successfully modified file: {self.flashattention_file}\")\n            logger.info(f\"Added after line {target_line_idx + 1}: {new_line_with_indent}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to modify file: {e}\")\n            # Try to restore backup\n            self.restore()\n            return False\n    \n    def restore(self):\n        \"\"\"Restore original file\"\"\"\n        if not os.path.exists(self.backup_file):\n            logger.error(f\"Backup file does not exist: {self.backup_file}\")\n            return False\n        \n        try:\n            shutil.copy2(self.backup_file, self.flashattention_file)\n            logger.info(f\"Original file restored: {self.flashattention_file}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to restore file: {e}\")\n            return False\n    \n    def check_status(self):\n        \"\"\"Check current status of the file\"\"\"\n        if not self._check_file_exists():\n            return \"File does not exist\"\n        \n        try:\n            with open(self.flashattention_file, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            target_line_idx, lines = self._find_target_line(content)\n            if target_line_idx == -1:\n                return \"Target line (second occurrence) not found\"\n            \n            if target_line_idx + 1 < len(lines) and self.new_line in lines[target_line_idx + 1]:\n                return \"Modified\"\n            else:\n                return \"Not modified\"\n                \n        except Exception as e:\n            return f\"Check failed: {e}\"\n    \n    def clean_backup(self):\n        \"\"\"Clean backup file\"\"\"\n        if os.path.exists(self.backup_file):\n            try:\n                os.remove(self.backup_file)\n                logger.info(f\"Backup file deleted: {self.backup_file}\")\n                return True\n            except Exception as e:\n                logger.error(f\"Failed to delete backup file: {e}\")\n                return False\n        return True\n\n\ndef main():\n    \"\"\"Main function providing command line interface\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"LMDeploy flashattention.py modification tool\")\n    parser.add_argument('action', choices=['patch', 'restore', 'status', 'clean'], \n                       help='Action type: patch=apply patch, restore=restore original file, status=check status, clean=clean backup')\n    \n    args = parser.parse_args()\n    \n    patcher = LMDeployPatcher()\n    \n    if args.action == 'patch':\n        if patcher.patch():\n            print(\"‚úÖ Patch applied successfully\")\n        else:\n            print(\"‚ùå Failed to apply patch\")\n    \n    elif args.action == 'restore':\n        if patcher.restore():\n            print(\"‚úÖ File restored successfully\")\n        else:\n            print(\"‚ùå Failed to restore file\")\n    \n    elif args.action == 'status':\n        status = patcher.check_status()\n        print(f\"üìã File status: {status}\")\n        if os.path.exists(patcher.backup_file):\n            print(f\"üíæ Backup file exists: {patcher.backup_file}\")\n    \n    elif args.action == 'clean':\n        if patcher.clean_backup():\n            print(\"‚úÖ Backup file cleaned successfully\")\n        else:\n            print(\"‚ùå Failed to clean backup file\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "file_name": "magic_pdf/config/constants.py",
    "file_contents": "\"\"\"Custom fields for span dimension.\"\"\"\n# Whether span is merged across pages\nCROSS_PAGE = 'cross_page'\n\n\"\"\"\nCustom fields for block dimension\n\"\"\"\n# Whether lines in block are deleted\nLINES_DELETED = 'lines_deleted'\n\n# table recognition max time default value\nTABLE_MAX_TIME_VALUE = 400\n\n# pp_table_result_max_length\nTABLE_MAX_LEN = 480\n\n# table master structure dict\nTABLE_MASTER_DICT = 'table_master_structure_dict.txt'\n\n# table master dir\nTABLE_MASTER_DIR = 'table_structure_tablemaster_infer/'\n\n# pp detect model dir\nDETECT_MODEL_DIR = 'ch_PP-OCRv4_det_infer'\n\n# pp rec model dir\nREC_MODEL_DIR = 'ch_PP-OCRv4_rec_infer'\n\n# pp rec char dict path\nREC_CHAR_DICT = 'ppocr_keys_v1.txt'\n\n# pp rec copy rec directory\nPP_REC_DIRECTORY = '.paddleocr/whl/rec/ch/ch_PP-OCRv4_rec_infer'\n\n# pp rec copy det directory\nPP_DET_DIRECTORY = '.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer'\n\n\nclass MODEL_NAME:\n    DocLayout_YOLO = 'doclayout_yolo'\n    PaddleXLayoutModel = 'PP-DocLayout_plus-L'\n\nPARSE_TYPE_TXT = 'txt'\nPARSE_TYPE_OCR = 'ocr'\n\n"
  },
  {
    "file_name": "magic_pdf/config/drop_reason.py",
    "file_contents": "class DropReason:\n    TEXT_BLOCK_HOR_OVERLAP = 'text_block_horizontal_overlap'\n    USEFUL_BLOCK_HOR_OVERLAP = (\n        'useful_block_horizontal_overlap'\n    )\n    COMPLICATED_LAYOUT = 'complicated_layout'\n    TOO_MANY_LAYOUT_COLUMNS = 'too_many_layout_columns'\n    COLOR_BACKGROUND_TEXT_BOX = 'color_background_text_box'\n    HIGH_COMPUTATIONAL_lOAD_BY_IMGS = (\n        'high_computational_load_by_imgs'\n    )\n    HIGH_COMPUTATIONAL_lOAD_BY_SVGS = (\n        'high_computational_load_by_svgs'\n    )\n    HIGH_COMPUTATIONAL_lOAD_BY_TOTAL_PAGES = 'high_computational_load_by_total_pages'\n    MISS_DOC_LAYOUT_RESULT = 'missing doc_layout_result'\n    Exception = '_exception'\n    ENCRYPTED = 'encrypted'\n    EMPTY_PDF = 'total_page=0'\n    NOT_IS_TEXT_PDF = 'not_is_text_pdf'\n    DENSE_SINGLE_LINE_BLOCK = 'dense_single_line_block'\n    TITLE_DETECTION_FAILED = 'title_detection_failed'\n    TITLE_LEVEL_FAILED = (\n        'title_level_failed'\n    )\n    PARA_SPLIT_FAILED = 'para_split_failed'\n    PARA_MERGE_FAILED = 'para_merge_failed'\n    NOT_ALLOW_LANGUAGE = 'not_allow_language'\n    SPECIAL_PDF = 'special_pdf'\n    PSEUDO_SINGLE_COLUMN = 'pseudo_single_column'\n    CAN_NOT_DETECT_PAGE_LAYOUT = 'can_not_detect_page_layout'\n    NEGATIVE_BBOX_AREA = 'negative_bbox_area'\n    OVERLAP_BLOCKS_CAN_NOT_SEPARATION = (\n        'overlap_blocks_can_t_separation'\n    )\n"
  },
  {
    "file_name": "magic_pdf/config/drop_tag.py",
    "file_contents": "\nCOLOR_BG_HEADER_TXT_BLOCK = 'color_background_header_txt_block'\nPAGE_NO = 'page-no'\nCONTENT_IN_FOOT_OR_HEADER = 'in-foot-header-area'\nVERTICAL_TEXT = 'vertical-text'\nROTATE_TEXT = 'rotate-text'\nEMPTY_SIDE_BLOCK = 'empty-side-block'\nON_IMAGE_TEXT = 'on-image-text'\nON_TABLE_TEXT = 'on-table-text'\n\n\nclass DropTag:\n    PAGE_NUMBER = 'page_no'\n    HEADER = 'header'\n    FOOTER = 'footer'\n    FOOTNOTE = 'footnote'\n    NOT_IN_LAYOUT = 'not_in_layout'\n    SPAN_OVERLAP = 'span_overlap'\n    BLOCK_OVERLAP = 'block_overlap'\n"
  },
  {
    "file_name": "magic_pdf/config/enums.py",
    "file_contents": "\nimport enum\n\n\nclass SupportedPdfParseMethod(enum.Enum):\n    OCR = 'ocr'\n    TXT = 'txt'\n"
  },
  {
    "file_name": "magic_pdf/config/exceptions.py",
    "file_contents": "\nclass FileNotExisted(Exception):\n\n    def __init__(self, path):\n        self.path = path\n\n    def __str__(self):\n        return f'File {self.path} does not exist.'\n\n\nclass InvalidConfig(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return f'Invalid config: {self.msg}'\n\n\nclass InvalidParams(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return f'Invalid params: {self.msg}'\n\n\nclass EmptyData(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return f'Empty data: {self.msg}'\n\nclass CUDA_NOT_AVAILABLE(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return f'CUDA not available: {self.msg}'"
  },
  {
    "file_name": "magic_pdf/config/make_content_config.py",
    "file_contents": "class MakeMode:\n    MM_MD = 'mm_markdown'\n    NLP_MD = 'nlp_markdown'\n    STANDARD_FORMAT = 'standard_format'\n\n\nclass DropMode:\n    WHOLE_PDF = 'whole_pdf'\n    SINGLE_PAGE = 'single_page'\n    NONE = 'none'\n    NONE_WITH_REASON = 'none_with_reason'\n"
  },
  {
    "file_name": "magic_pdf/config/model_block_type.py",
    "file_contents": "from enum import Enum\n\n\nclass ModelBlockTypeEnum(Enum):\n    TITLE = 0\n    PLAIN_TEXT = 1\n    ABANDON = 2\n    ISOLATE_FORMULA = 8\n    EMBEDDING = 13\n    ISOLATED = 14\n"
  },
  {
    "file_name": "magic_pdf/config/ocr_content_type.py",
    "file_contents": "class ContentType:\n    Image = 'image'\n    Table = 'table'\n    Text = 'text'\n    InlineEquation = 'inline_equation'\n    InterlineEquation = 'interline_equation'\n\n\nclass BlockType:\n    Image = 'image'\n    ImageBody = 'image_body'\n    ImageCaption = 'image_caption'\n    ImageFootnote = 'image_footnote'\n    Table = 'table'\n    TableBody = 'table_body'\n    TableCaption = 'table_caption'\n    TableFootnote = 'table_footnote'\n    Text = 'text'\n    Title = 'title'\n    InterlineEquation = 'interline_equation'\n    Footnote = 'footnote'\n    Discarded = 'discarded'\n    List = 'list'\n    Index = 'index'\n\n\nclass CategoryId:\n    Title = 0\n    Text = 1\n    Abandon = 2\n    ImageBody = 3\n    ImageCaption = 4\n    TableBody = 5\n    TableCaption = 6\n    TableFootnote = 7\n    InterlineEquation_Layout = 8\n    InlineEquation = 13\n    InterlineEquation_YOLO = 14\n    OcrText = 15\n    ImageFootnote = 101\n"
  },
  {
    "file_name": "magic_pdf/data/dataset.py",
    "file_contents": "import os\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Iterator\n\nimport fitz\nfrom loguru import logger\n\nfrom magic_pdf.config.enums import SupportedPdfParseMethod\nfrom magic_pdf.data.schemas import PageInfo\nfrom magic_pdf.data.utils import fitz_doc_to_image\nfrom magic_pdf.filter import classify\n\n\nclass PageableData(ABC):\n    @abstractmethod\n    def get_image(self) -> dict:\n        \"\"\"Transform data to image.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_doc(self) -> fitz.Page:\n        \"\"\"Get the pymudoc page.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_page_info(self) -> PageInfo:\n        \"\"\"Get the page info of the page.\n\n        Returns:\n            PageInfo: the page info of this page\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def draw_rect(self, rect_coords, color, fill, fill_opacity, width, overlay):\n        \"\"\"draw rectangle.\n\n        Args:\n            rect_coords (list[float]): four elements array contain the top-left and bottom-right coordinates, [x0, y0, x1, y1]\n            color (list[float] | None): three element tuple which describe the RGB of the board line, None means no board line\n            fill (list[float] | None): fill the board with RGB, None means will not fill with color\n            fill_opacity (float): opacity of the fill, range from [0, 1]\n            width (float): the width of board\n            overlay (bool): fill the color in foreground or background. True means fill in background.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def insert_text(self, coord, content, fontsize, color):\n        \"\"\"insert text.\n\n        Args:\n            coord (list[float]): four elements array contain the top-left and bottom-right coordinates, [x0, y0, x1, y1]\n            content (str): the text content\n            fontsize (int): font size of the text\n            color (list[float] | None):  three element tuple which describe the RGB of the board line, None will use the default font color!\n        \"\"\"\n        pass\n\n\nclass Dataset(ABC):\n    @abstractmethod\n    def __len__(self) -> int:\n        \"\"\"The length of the dataset.\"\"\"\n        pass\n\n    @abstractmethod\n    def __iter__(self) -> Iterator[PageableData]:\n        \"\"\"Yield the page data.\"\"\"\n        pass\n\n    @abstractmethod\n    def supported_methods(self) -> list[SupportedPdfParseMethod]:\n        \"\"\"The methods that this dataset support.\n\n        Returns:\n            list[SupportedPdfParseMethod]: The supported methods, Valid methods are: OCR, TXT\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def data_bits(self) -> bytes:\n        \"\"\"The bits used to create this dataset.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_page(self, page_id: int) -> PageableData:\n        \"\"\"Get the page indexed by page_id.\n\n        Args:\n            page_id (int): the index of the page\n\n        Returns:\n            PageableData: the page doc object\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dump_to_file(self, file_path: str):\n        \"\"\"Dump the file\n\n        Args: \n            file_path (str): the file path \n        \"\"\"\n        pass\n\n    @abstractmethod\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(self, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def classify(self) -> SupportedPdfParseMethod:\n        \"\"\"classify the dataset \n\n        Returns:\n            SupportedPdfParseMethod: _description_\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def clone(self):\n        \"\"\"clone this dataset\n        \"\"\"\n        pass\n\n\nclass PymuDocDataset(Dataset):\n    def __init__(self, bits: bytes, lang=None):\n        \"\"\"Initialize the dataset, which wraps the pymudoc documents.\n\n        Args:\n            bits (bytes): the bytes of the pdf\n        \"\"\"\n        self._raw_fitz = fitz.open('pdf', bits)\n        self._records = [Doc(v) for v in self._raw_fitz]\n        self._data_bits = bits\n        self._raw_data = bits\n\n        if lang == '':\n            self._lang = None\n        else:\n            self._lang = lang\n            logger.info(f\"lang: {lang}\")\n    def __len__(self) -> int:\n        \"\"\"The page number of the pdf.\"\"\"\n        return len(self._records)\n\n    def __iter__(self) -> Iterator[PageableData]:\n        \"\"\"Yield the page doc object.\"\"\"\n        return iter(self._records)\n\n    def supported_methods(self) -> list[SupportedPdfParseMethod]:\n        \"\"\"The method supported by this dataset.\n\n        Returns:\n            list[SupportedPdfParseMethod]: the supported methods\n        \"\"\"\n        return [SupportedPdfParseMethod.OCR, SupportedPdfParseMethod.TXT]\n\n    def data_bits(self) -> bytes:\n        \"\"\"The pdf bits used to create this dataset.\"\"\"\n        return self._data_bits\n\n    def get_page(self, page_id: int) -> PageableData:\n        \"\"\"The page doc object.\n\n        Args:\n            page_id (int): the page doc index\n\n        Returns:\n            PageableData: the page doc object\n        \"\"\"\n        return self._records[page_id]\n\n    def dump_to_file(self, file_path: str):\n        \"\"\"Dump the file\n\n        Args: \n            file_path (str): the file path \n        \"\"\"\n        \n        dir_name = os.path.dirname(file_path)\n        if dir_name not in ('', '.', '..'):\n            os.makedirs(dir_name, exist_ok=True)\n        self._raw_fitz.save(file_path)\n\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(dataset, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        if 'lang' in kwargs and self._lang is not None:\n            kwargs['lang'] = self._lang\n        return proc(self, *args, **kwargs)\n\n    def classify(self) -> SupportedPdfParseMethod:\n        \"\"\"classify the dataset \n\n        Returns:\n            SupportedPdfParseMethod: _description_\n        \"\"\"\n        return classify(self._data_bits)\n\n    def clone(self):\n        \"\"\"clone this dataset\n        \"\"\"\n        return PymuDocDataset(self._raw_data)\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, bits: bytes):\n        \"\"\"Initialize the dataset, which wraps the pymudoc documents.\n\n        Args:\n            bits (bytes): the bytes of the photo which will be converted to pdf first. then converted to pymudoc.\n        \"\"\"\n        pdf_bytes = fitz.open(stream=bits).convert_to_pdf()\n        self._raw_fitz = fitz.open('pdf', pdf_bytes)\n        self._records = [Doc(v) for v in self._raw_fitz]\n        self._raw_data = bits\n        self._data_bits = pdf_bytes\n\n    def __len__(self) -> int:\n        \"\"\"The length of the dataset.\"\"\"\n        return len(self._records)\n\n    def __iter__(self) -> Iterator[PageableData]:\n        \"\"\"Yield the page object.\"\"\"\n        return iter(self._records)\n\n    def supported_methods(self):\n        \"\"\"The method supported by this dataset.\n\n        Returns:\n            list[SupportedPdfParseMethod]: the supported methods\n        \"\"\"\n        return [SupportedPdfParseMethod.OCR]\n\n    def data_bits(self) -> bytes:\n        \"\"\"The pdf bits used to create this dataset.\"\"\"\n        return self._data_bits\n\n    def get_page(self, page_id: int) -> PageableData:\n        \"\"\"The page doc object.\n\n        Args:\n            page_id (int): the page doc index\n\n        Returns:\n            PageableData: the page doc object\n        \"\"\"\n        return self._records[page_id]\n\n    def dump_to_file(self, file_path: str):\n        \"\"\"Dump the file\n\n        Args: \n            file_path (str): the file path \n        \"\"\"\n        dir_name = os.path.dirname(file_path)\n        if dir_name not in ('', '.', '..'):\n            os.makedirs(dir_name, exist_ok=True)\n        self._raw_fitz.save(file_path)\n\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(dataset, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        return proc(self, *args, **kwargs)\n\n    def classify(self) -> SupportedPdfParseMethod:\n        \"\"\"classify the dataset \n\n        Returns:\n            SupportedPdfParseMethod: _description_\n        \"\"\"\n        return SupportedPdfParseMethod.OCR\n\n    def clone(self):\n        \"\"\"clone this dataset\n        \"\"\"\n        return ImageDataset(self._raw_data)\n\n\nclass MultiFileDataset(Dataset):\n    def __init__(self, file_bytes_list: list[bytes], file_extensions: list[str] = None):\n        \"\"\"Initialize the dataset with multiple files (PDFs and images).\n\n        Args:\n            file_bytes_list (list[bytes]): list of file bytes (PDF or image) that will be converted to a multi-page pdf\n            file_extensions (list[str], optional): list of file extensions (e.g., ['.pdf', '.jpg', '.png']). \n                                                  If provided, will be used to determine file type instead of trying to open files.\n        \"\"\"\n        if not file_bytes_list:\n            raise ValueError(\"file_bytes_list cannot be empty\")\n        \n        if file_extensions and len(file_extensions) != len(file_bytes_list):\n            raise ValueError(\"file_extensions length must match file_bytes_list length\")\n\n        self._file_extensions = file_extensions\n        \n        # Track file information\n        self._file_info = []\n        self._raw_data = file_bytes_list\n        \n        # Create a new PDF document\n        pdf_doc = fitz.open()\n        \n        # Process each file and insert into the main document\n        for i, file_bytes in enumerate(file_bytes_list):\n            if not file_bytes:\n                raise ValueError(f\"File at index {i} has empty bytes\")\n            \n            # Determine file type from extension if provided\n            if file_extensions:\n                ext = file_extensions[i].lower()\n                if ext == '.pdf':\n                    file_type = 'pdf'\n                    temp_doc = fitz.open('pdf', file_bytes)\n                elif ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n                    file_type = 'image'\n                    temp_doc = fitz.open(stream=file_bytes)\n                else:\n                    raise ValueError(f\"Unsupported file extension: {ext}\")\n            else:\n                # Fallback to original detection method\n                try:\n                    temp_doc = fitz.open('pdf', file_bytes)\n                    file_type = 'pdf'\n                except Exception:\n                    temp_doc = fitz.open(stream=file_bytes)\n                    file_type = 'image'\n            \n            page_count = len(temp_doc)\n            if page_count == 0:\n                temp_doc.close()\n                continue\n            \n            start_page = len(pdf_doc)\n            \n            # Convert to PDF if needed and insert\n            if file_type == 'image':\n                pdf_bytes = temp_doc.convert_to_pdf()\n                temp_pdf = fitz.open('pdf', pdf_bytes)\n                pdf_doc.insert_pdf(temp_pdf)\n                temp_pdf.close()\n            else:\n                pdf_doc.insert_pdf(temp_doc)\n            \n            temp_doc.close()\n            \n            # Record file information\n            self._file_info.append({\n                'file_index': i,\n                'file_type': file_type,\n                'page_count': page_count,\n                'start_page': start_page,\n                'end_page': start_page + page_count - 1\n            })\n        \n        if len(self._file_info) == 0:\n            raise RuntimeError(\"No valid files were processed\")\n        \n        # Get the final PDF bytes\n        self._data_bits = pdf_doc.tobytes()\n        pdf_doc.close()\n        \n        # Reopen the PDF for processing\n        self._raw_fitz = fitz.open('pdf', self._data_bits)\n        self._records = [Doc(v) for v in self._raw_fitz]\n\n    def __len__(self) -> int:\n        \"\"\"The length of the dataset.\"\"\"\n        return len(self._records)\n\n    def __iter__(self) -> Iterator[PageableData]:\n        \"\"\"Yield the page object.\"\"\"\n        return iter(self._records)\n\n    def supported_methods(self):\n        \"\"\"The method supported by this dataset.\n\n        Returns:\n            list[SupportedPdfParseMethod]: the supported methods\n        \"\"\"\n        return [SupportedPdfParseMethod.OCR]\n\n    def data_bits(self) -> bytes:\n        \"\"\"The pdf bits used to create this dataset.\"\"\"\n        return self._data_bits\n\n    def get_page(self, page_id: int) -> PageableData:\n        \"\"\"The page doc object.\n\n        Args:\n            page_id (int): the page doc index\n\n        Returns:\n            PageableData: the page doc object\n        \"\"\"\n        return self._records[page_id]\n\n    def dump_to_file(self, file_path: str):\n        \"\"\"Dump the file\n\n        Args: \n            file_path (str): the file path \n        \"\"\"\n        dir_name = os.path.dirname(file_path)\n        if dir_name not in ('', '.', '..'):\n            os.makedirs(dir_name, exist_ok=True)\n        self._raw_fitz.save(file_path)\n\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(dataset, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        return proc(self, *args, **kwargs)\n\n    def classify(self) -> SupportedPdfParseMethod:\n        \"\"\"classify the dataset \n\n        Returns:\n            SupportedPdfParseMethod: _description_\n        \"\"\"\n        return SupportedPdfParseMethod.OCR\n\n    def clone(self):\n        \"\"\"clone this dataset\n        \"\"\"\n        return MultiFileDataset(self._raw_data, file_extensions=self._file_extensions)\n\n    @property\n    def file_info(self) -> list[dict]:\n        \"\"\"Get information about each file in the dataset.\n        \n        Returns:\n            list[dict]: List of file information dictionaries containing:\n                - file_index: Index of the file in the original input\n                - file_type: 'pdf' or 'image'\n                - page_count: Number of pages in this file\n                - start_page: Starting page index in the combined dataset\n                - end_page: Ending page index in the combined dataset\n        \"\"\"\n        return self._file_info.copy()\n\n    def get_file_page_count(self, file_index: int) -> int:\n        \"\"\"Get the page count for a specific file.\n        \n        Args:\n            file_index (int): Index of the file\n            \n        Returns:\n            int: Number of pages in the file\n        \"\"\"\n        if file_index < 0 or file_index >= len(self._file_info):\n            raise IndexError(f\"File index {file_index} out of range\")\n        return self._file_info[file_index]['page_count']\n\n    def export_file_as_dataset(self, file_index: int):\n        \"\"\"Export a specific file as an appropriate Dataset.\n        \n        Args:\n            file_index (int): Index of the file to export\n            \n        Returns:\n            Dataset: ImageDataset for image files, PymuDocDataset for PDF files\n        \"\"\"\n        if file_index < 0 or file_index >= len(self._file_info):\n            raise IndexError(f\"File index {file_index} out of range\")\n        \n        file_bytes = self._raw_data[file_index]\n        file_type = self._file_info[file_index]['file_type']\n        \n        if file_type == 'image':\n            return ImageDataset(file_bytes)\n        else:  # file_type == 'pdf'\n            return PymuDocDataset(file_bytes)\n\n\nclass Doc(PageableData):\n    \"\"\"Initialized with pymudoc object.\"\"\"\n\n    def __init__(self, doc: fitz.Page):\n        self._doc = doc\n\n    def get_image(self):\n        \"\"\"Return the image info.\n\n        Returns:\n            dict: {\n                img: np.ndarray,\n                width: int,\n                height: int\n            }\n        \"\"\"\n        return fitz_doc_to_image(self._doc)\n\n    def get_doc(self) -> fitz.Page:\n        \"\"\"Get the pymudoc object.\n\n        Returns:\n            fitz.Page: the pymudoc object\n        \"\"\"\n        return self._doc\n\n    def get_page_info(self) -> PageInfo:\n        \"\"\"Get the page info of the page.\n\n        Returns:\n            PageInfo: the page info of this page\n        \"\"\"\n        page_w = self._doc.rect.width\n        page_h = self._doc.rect.height\n        return PageInfo(w=page_w, h=page_h)\n\n    def __getattr__(self, name):\n        if hasattr(self._doc, name):\n            return getattr(self._doc, name)\n\n    def draw_rect(self, rect_coords, color, fill, fill_opacity, width, overlay):\n        \"\"\"draw rectangle.\n\n        Args:\n            rect_coords (list[float]): four elements array contain the top-left and bottom-right coordinates, [x0, y0, x1, y1]\n            color (list[float] | None): three element tuple which describe the RGB of the board line, None means no board line\n            fill (list[float] | None): fill the board with RGB, None means will not fill with color\n            fill_opacity (float): opacity of the fill, range from [0, 1]\n            width (float): the width of board\n            overlay (bool): fill the color in foreground or background. True means fill in background.\n        \"\"\"\n        self._doc.draw_rect(\n            rect_coords,\n            color=color,\n            fill=fill,\n            fill_opacity=fill_opacity,\n            width=width,\n            overlay=overlay,\n        )\n\n    def insert_text(self, coord, content, fontsize, color, rotate):\n        \"\"\"insert text.\n\n        Args:\n            coord (list[float]): four elements array contain the top-left and bottom-right coordinates, [x0, y0, x1, y1]\n            content (str): the text content\n            fontsize (int): font size of the text\n            color (list[float] | None):  three element tuple which describe the RGB of the board line, None will use the default font color!\n            rotate (int): the rotation of the text, None means no rotation\n        \"\"\"\n        self._doc.insert_text(coord, content, fontsize=fontsize, color=color, rotate=rotate)\n"
  },
  {
    "file_name": "magic_pdf/data/read_api.py",
    "file_contents": "import json\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom magic_pdf.config.exceptions import EmptyData, InvalidParams\nfrom magic_pdf.data.data_reader_writer import (FileBasedDataReader,\n                                               MultiBucketS3DataReader)\nfrom magic_pdf.data.dataset import ImageDataset, PymuDocDataset\nfrom magic_pdf.utils.office_to_pdf import convert_file_to_pdf, ConvertToPdfError\n\ndef read_jsonl(\n    s3_path_or_local: str, s3_client: MultiBucketS3DataReader | None = None\n) -> list[PymuDocDataset]:\n    \"\"\"Read the jsonl file and return the list of PymuDocDataset.\n\n    Args:\n        s3_path_or_local (str): local file or s3 path\n        s3_client (MultiBucketS3DataReader | None, optional): s3 client that support multiple bucket. Defaults to None.\n\n    Raises:\n        InvalidParams: if s3_path_or_local is s3 path but s3_client is not provided.\n        EmptyData: if no pdf file location is provided in some line of jsonl file.\n        InvalidParams: if the file location is s3 path but s3_client is not provided\n\n    Returns:\n        list[PymuDocDataset]: each line in the jsonl file will be converted to a PymuDocDataset\n    \"\"\"\n    bits_arr = []\n    if s3_path_or_local.startswith('s3://'):\n        if s3_client is None:\n            raise InvalidParams('s3_client is required when s3_path is provided')\n        jsonl_bits = s3_client.read(s3_path_or_local)\n    else:\n        jsonl_bits = FileBasedDataReader('').read(s3_path_or_local)\n    jsonl_d = [\n        json.loads(line) for line in jsonl_bits.decode().split('\\n') if line.strip()\n    ]\n    for d in jsonl_d:\n        pdf_path = d.get('file_location', '') or d.get('path', '')\n        if len(pdf_path) == 0:\n            raise EmptyData('pdf file location is empty')\n        if pdf_path.startswith('s3://'):\n            if s3_client is None:\n                raise InvalidParams('s3_client is required when s3_path is provided')\n            bits_arr.append(s3_client.read(pdf_path))\n        else:\n            bits_arr.append(FileBasedDataReader('').read(pdf_path))\n    return [PymuDocDataset(bits) for bits in bits_arr]\n\n\ndef read_local_pdfs(path: str) -> list[PymuDocDataset]:\n    \"\"\"Read pdf from path or directory.\n\n    Args:\n        path (str): pdf file path or directory that contains pdf files\n\n    Returns:\n        list[PymuDocDataset]: each pdf file will converted to a PymuDocDataset\n    \"\"\"\n    if os.path.isdir(path):\n        reader = FileBasedDataReader()\n        ret = []\n        for root, _, files in os.walk(path):\n            for file in files:\n                suffix = file.split('.')\n                if suffix[-1] == 'pdf':\n                    ret.append( PymuDocDataset(reader.read(os.path.join(root, file))))\n        return ret\n    else:\n        reader = FileBasedDataReader()\n        bits = reader.read(path)\n        return [PymuDocDataset(bits)]\n\ndef read_local_office(path: str) -> list[PymuDocDataset]:\n    \"\"\"Read ms-office file (ppt, pptx, doc, docx) from path or directory.\n\n    Args:\n        path (str): ms-office file or directory that contains ms-office files\n\n    Returns:\n        list[PymuDocDataset]: each ms-office file will converted to a PymuDocDataset\n        \n    Raises:\n        ConvertToPdfError: Failed to convert ms-office file to pdf via libreoffice\n        FileNotFoundError: File not Found\n        Exception: Unknown Exception raised\n    \"\"\"\n    suffixes = ['.ppt', '.pptx', '.doc', '.docx']\n    fns = []\n    ret = []\n    if os.path.isdir(path):\n        for root, _, files in os.walk(path):\n            for file in files:\n                suffix = Path(file).suffix\n                if suffix in suffixes:\n                    fns.append((os.path.join(root, file)))\n    else:\n        fns.append(path)\n        \n    reader = FileBasedDataReader()\n    temp_dir = tempfile.mkdtemp()\n    for fn in fns:\n        try:\n            convert_file_to_pdf(fn, temp_dir)\n        except ConvertToPdfError as e:\n            raise e\n        except FileNotFoundError as e:\n            raise e\n        except Exception as e:\n            raise e\n        fn_path = Path(fn)\n        pdf_fn = f\"{temp_dir}/{fn_path.stem}.pdf\"\n        ret.append(PymuDocDataset(reader.read(pdf_fn)))\n    shutil.rmtree(temp_dir)\n    return ret\n\ndef read_local_images(path: str, suffixes: list[str]=['.png', '.jpg']) -> list[ImageDataset]:\n    \"\"\"Read images from path or directory.\n\n    Args:\n        path (str): image file path or directory that contains image files\n        suffixes (list[str]): the suffixes of the image files used to filter the files. Example: ['.jpg', '.png']\n\n    Returns:\n        list[ImageDataset]: each image file will converted to a ImageDataset\n    \"\"\"\n    if os.path.isdir(path):\n        imgs_bits = []\n        s_suffixes = set(suffixes)\n        reader = FileBasedDataReader()\n        for root, _, files in os.walk(path):\n            for file in files:\n                suffix = Path(file).suffix\n                if suffix in s_suffixes:\n                    imgs_bits.append(reader.read(os.path.join(root, file)))\n        return [ImageDataset(bits) for bits in imgs_bits]\n    else:\n        reader = FileBasedDataReader()\n        bits = reader.read(path)\n        return [ImageDataset(bits)]\n"
  },
  {
    "file_name": "magic_pdf/data/schemas.py",
    "file_contents": "\nfrom pydantic import BaseModel, Field\n\n\nclass S3Config(BaseModel):\n    \"\"\"S3 config\n    \"\"\"\n    bucket_name: str = Field(description='s3 bucket name', min_length=1)\n    access_key: str = Field(description='s3 access key', min_length=1)\n    secret_key: str = Field(description='s3 secret key', min_length=1)\n    endpoint_url: str = Field(description='s3 endpoint url', min_length=1)\n    addressing_style: str = Field(description='s3 addressing style', default='auto', min_length=1)\n\n\nclass PageInfo(BaseModel):\n    \"\"\"The width and height of page\n    \"\"\"\n    w: float = Field(description='the width of page')\n    h: float = Field(description='the height of page')\n"
  },
  {
    "file_name": "magic_pdf/data/utils.py",
    "file_contents": "import fitz\nimport numpy as np\nfrom loguru import logger\n\nfrom magic_pdf.utils.annotations import ImportPIL\n\n\n@ImportPIL\ndef fitz_doc_to_image(doc, dpi=200) -> dict:\n    \"\"\"Convert fitz.Document to image, Then convert the image to numpy array.\n\n    Args:\n        doc (_type_): pymudoc page\n        dpi (int, optional): reset the dpi of dpi. Defaults to 200.\n\n    Returns:\n        dict:  {'img': numpy array, 'width': width, 'height': height }\n    \"\"\"\n    from PIL import Image\n    mat = fitz.Matrix(dpi / 72, dpi / 72)\n    pm = doc.get_pixmap(matrix=mat, alpha=False)\n\n    # If the width or height exceeds 4500 after scaling, do not scale further.\n    if pm.width > 4500 or pm.height > 4500:\n        pm = doc.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n    img = Image.frombytes('RGB', (pm.width, pm.height), pm.samples)\n    img = np.array(img)\n\n    img_dict = {'img': img, 'width': pm.width, 'height': pm.height}\n\n    return img_dict\n\n@ImportPIL\ndef load_images_from_pdf(pdf_bytes: bytes, dpi=200, start_page_id=0, end_page_id=None) -> list:\n    from PIL import Image\n    images = []\n    with fitz.open('pdf', pdf_bytes) as doc:\n        pdf_page_num = doc.page_count\n        end_page_id = (\n            end_page_id\n            if end_page_id is not None and end_page_id >= 0\n            else pdf_page_num - 1\n        )\n        if end_page_id > pdf_page_num - 1:\n            logger.warning('end_page_id is out of range, use images length')\n            end_page_id = pdf_page_num - 1\n\n        for index in range(0, doc.page_count):\n            if start_page_id <= index <= end_page_id:\n                page = doc[index]\n                mat = fitz.Matrix(dpi / 72, dpi / 72)\n                pm = page.get_pixmap(matrix=mat, alpha=False)\n\n                # If the width or height exceeds 4500 after scaling, do not scale further.\n                if pm.width > 4500 or pm.height > 4500:\n                    pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n                img = Image.frombytes('RGB', (pm.width, pm.height), pm.samples)\n                img = np.array(img)\n                img_dict = {'img': img, 'width': pm.width, 'height': pm.height}\n            else:\n                img_dict = {'img': [], 'width': 0, 'height': 0}\n\n            images.append(img_dict)\n    return images\n"
  },
  {
    "file_name": "magic_pdf/dict2md/ocr_mkcontent.py",
    "file_contents": "import re\n\nfrom loguru import logger\n\nfrom magic_pdf.config.make_content_config import DropMode, MakeMode\nfrom magic_pdf.config.ocr_content_type import BlockType, ContentType\nfrom magic_pdf.libs.commons import join_path\nfrom magic_pdf.libs.language import detect_lang\nfrom magic_pdf.libs.markdown_utils import ocr_escape_special_markdown_char\nfrom magic_pdf.post_proc.para_split_v3 import ListLineTag\n\n\ndef __is_hyphen_at_line_end(line):\n    \"\"\"Check if a line ends with one or more letters followed by a hyphen.\n\n    Args:\n    line (str): The line of text to check.\n\n    Returns:\n    bool: True if the line ends with one or more letters followed by a hyphen, False otherwise.\n    \"\"\"\n    # Use regex to check if the line ends with one or more letters followed by a hyphen\n    return bool(re.search(r'[A-Za-z]+-\\s*$', line))\n\n\ndef ocr_mk_mm_markdown_with_para_and_pagination(pdf_info_dict: list,\n                                                img_buket_path):\n    markdown_with_para_and_pagination = []\n    page_no = 0\n    for page_info in pdf_info_dict:\n        paras_of_layout = page_info.get('para_blocks')\n        if not paras_of_layout:\n            markdown_with_para_and_pagination.append({\n                'page_no':\n                    page_no,\n                'md_content':\n                    '',\n            })\n            page_no += 1\n            continue\n        page_markdown = ocr_mk_markdown_with_para_core_v2(\n            paras_of_layout, 'mm', img_buket_path)\n        markdown_with_para_and_pagination.append({\n            'page_no':\n                page_no,\n            'md_content':\n                '\\n\\n'.join(page_markdown)\n        })\n        page_no += 1\n    return markdown_with_para_and_pagination\n\n\ndef ocr_mk_markdown_with_para_core_v2(paras_of_layout,\n                                      mode,\n                                      img_buket_path='',\n                                      ):\n    page_markdown = []\n    for para_block in paras_of_layout:\n        para_text = ''\n        para_type = para_block['type']\n        if para_type in [BlockType.Text, BlockType.List, BlockType.Index]:\n            para_text = merge_para_with_text(para_block)\n        elif para_type == BlockType.Title:\n            title_level = get_title_level(para_block)\n            para_text = f'{\"#\" * title_level} {merge_para_with_text(para_block)}'.replace('\\n', f'\\n{\"#\" * title_level}')\n        elif para_type == BlockType.InterlineEquation:\n            para_text = merge_para_with_text(para_block)\n        elif para_type == BlockType.Image:\n            if mode == 'nlp':\n                continue\n            elif mode == 'mm':\n                for block in para_block['blocks']:\n                    if block['type'] == BlockType.ImageBody:\n                        for line in block['lines']:\n                            for span in line['spans']:\n                                if span['type'] == ContentType.Image:\n                                    if span.get('image_path', ''):\n                                        para_text += f\"\\n![]({join_path(img_buket_path, span['image_path'])})  \\n\"\n                for block in para_block['blocks']:\n                    if block['type'] == BlockType.ImageCaption:\n                        para_text += merge_para_with_text(block) + '  \\n'\n                for block in para_block['blocks']:\n                    if block['type'] == BlockType.ImageFootnote:\n                        para_text += merge_para_with_text(block) + '  \\n'\n        elif para_type == BlockType.Table:\n            if mode == 'nlp':\n                continue\n            elif mode == 'mm':\n                for block in para_block['blocks']:\n                    if block['type'] == BlockType.TableCaption:\n                        para_text += merge_para_with_text(block) + '  \\n'\n                for block in para_block['blocks']:\n                    if block['type'] == BlockType.TableBody:\n                        for line in block['lines']:\n                            for span in line['spans']:\n                                if span['type'] == ContentType.Table:\n                                    # if processed by table model\n                                    if span.get('latex', ''):\n                                        para_text += f\"\\n\\n$\\n {span['latex']}\\n$\\n\\n\"\n                                    elif span.get('html', ''):\n                                        para_text += f\"\\n\\n{span['html']}\\n\\n\"\n                                    elif span.get('image_path', ''):\n                                        para_text += f\"\\n![]({join_path(img_buket_path, span['image_path'])})  \\n\"\n                for block in para_block['blocks']:\n                    if block['type'] == BlockType.TableFootnote:\n                        para_text += merge_para_with_text(block) + '  \\n'\n\n        if para_text.strip() == '':\n            continue\n        else:\n            page_markdown.append(para_text.strip() + '  ')\n\n    return page_markdown\n\n\ndef detect_language(text):\n    en_pattern = r'[a-zA-Z]+'\n    en_matches = re.findall(en_pattern, text)\n    en_length = sum(len(match) for match in en_matches)\n    if len(text) > 0:\n        if en_length / len(text) >= 0.5:\n            return 'en'\n        else:\n            return 'unknown'\n    else:\n        return 'empty'\n\n\ndef merge_para_with_text(para_block):\n    block_text = ''\n    for line in para_block['lines']:\n        for span in line['spans']:\n            if span['type'] in [ContentType.Text]:\n                block_text += span['content']\n    block_lang = detect_lang(block_text[:100])\n\n    para_text = ''\n    for i, line in enumerate(para_block['lines']):\n\n        if i >= 1 and line.get(ListLineTag.IS_LIST_START_LINE, False):\n            para_text += '  \\n'\n\n        for j, span in enumerate(line['spans']):\n\n            span_type = span['type']\n            content = ''\n            if span_type == ContentType.Text:\n                content = ocr_escape_special_markdown_char(span['content'])\n            elif span_type == ContentType.InlineEquation:\n                content = f\"${span['content']}$\"\n            elif span_type == ContentType.InterlineEquation:\n                content = f\"\\n$$\\n{span['content']}\\n$$\\n\"\n\n            content = content.strip()\n\n            if content:\n                langs = ['zh', 'ja', 'ko']\n                # logger.info(f'block_lang: {block_lang}, content: {content}')\n                if block_lang in langs: # In Chinese/Japanese/Korean context, line breaks don't need space separation, but if it's inline equation ending, still need to add space\n                    if j == len(line['spans']) - 1 and span_type not in [ContentType.InlineEquation]:\n                        para_text += content\n                    else:\n                        para_text += f'{content} '\n                else:\n                    if span_type in [ContentType.Text, ContentType.InlineEquation]:\n                        # If span is last in line and ends with hyphen, no space should be added at end, and hyphen should be removed\n                        if j == len(line['spans'])-1 and span_type == ContentType.Text and __is_hyphen_at_line_end(content):\n                            para_text += content[:-1]\n                        else:  # In Western text context, content needs space separation\n                            para_text += f'{content} '\n                    elif span_type == ContentType.InterlineEquation:\n                        para_text += content\n            else:\n                continue\n    # Split connected characters\n    # para_text = __replace_ligatures(para_text)\n\n    return para_text\n\n\ndef para_to_standard_format_v2(para_block, img_buket_path, page_idx, drop_reason=None):\n    para_type = para_block['type']\n    para_content = {}\n    if para_type in [BlockType.Text, BlockType.List, BlockType.Index]:\n        para_content = {\n            'type': 'text',\n            'text': merge_para_with_text(para_block),\n        }\n    elif para_type == BlockType.Title:\n        title_level = get_title_level(para_block)\n        para_content = {\n            'type': 'text',\n            'text': merge_para_with_text(para_block),\n            'text_level': title_level,\n        }\n    elif para_type == BlockType.InterlineEquation:\n        para_content = {\n            'type': 'equation',\n            'text': merge_para_with_text(para_block),\n            'text_format': 'latex',\n        }\n    elif para_type == BlockType.Image:\n        para_content = {'type': 'image', 'img_path': '', 'img_caption': [], 'img_footnote': []}\n        for block in para_block['blocks']:\n            if block['type'] == BlockType.ImageBody:\n                for line in block['lines']:\n                    for span in line['spans']:\n                        if span['type'] == ContentType.Image:\n                            if span.get('image_path', ''):\n                                para_content['img_path'] = join_path(img_buket_path, span['image_path'])\n            if block['type'] == BlockType.ImageCaption:\n                para_content['img_caption'].append(merge_para_with_text(block))\n            if block['type'] == BlockType.ImageFootnote:\n                para_content['img_footnote'].append(merge_para_with_text(block))\n    elif para_type == BlockType.Table:\n        para_content = {'type': 'table', 'img_path': '', 'table_caption': [], 'table_footnote': []}\n        for block in para_block['blocks']:\n            if block['type'] == BlockType.TableBody:\n                for line in block['lines']:\n                    for span in line['spans']:\n                        if span['type'] == ContentType.Table:\n\n                            if span.get('latex', ''):\n                                para_content['table_body'] = f\"\\n\\n$\\n {span['latex']}\\n$\\n\\n\"\n                            elif span.get('html', ''):\n                                para_content['table_body'] = f\"\\n\\n{span['html']}\\n\\n\"\n\n                            if span.get('image_path', ''):\n                                para_content['img_path'] = join_path(img_buket_path, span['image_path'])\n\n            if block['type'] == BlockType.TableCaption:\n                para_content['table_caption'].append(merge_para_with_text(block))\n            if block['type'] == BlockType.TableFootnote:\n                para_content['table_footnote'].append(merge_para_with_text(block))\n\n    para_content['page_idx'] = page_idx\n\n    if drop_reason is not None:\n        para_content['drop_reason'] = drop_reason\n\n    return para_content\n\n\ndef union_make(pdf_info_dict: list,\n               make_mode: str,\n               drop_mode: str,\n               img_buket_path: str = '',\n               ):\n    output_content = []\n    for page_info in pdf_info_dict:\n        drop_reason_flag = False\n        drop_reason = None\n        if page_info.get('need_drop', False):\n            drop_reason = page_info.get('drop_reason')\n            if drop_mode == DropMode.NONE:\n                pass\n            elif drop_mode == DropMode.NONE_WITH_REASON:\n                drop_reason_flag = True\n            elif drop_mode == DropMode.WHOLE_PDF:\n                raise Exception((f'drop_mode is {DropMode.WHOLE_PDF} ,'\n                                 f'drop_reason is {drop_reason}'))\n            elif drop_mode == DropMode.SINGLE_PAGE:\n                logger.warning((f'drop_mode is {DropMode.SINGLE_PAGE} ,'\n                                f'drop_reason is {drop_reason}'))\n                continue\n            else:\n                raise Exception('drop_mode can not be null')\n\n        paras_of_layout = page_info.get('para_blocks')\n        page_idx = page_info.get('page_idx')\n        if not paras_of_layout:\n            continue\n        if make_mode == MakeMode.MM_MD:\n            page_markdown = ocr_mk_markdown_with_para_core_v2(\n                paras_of_layout, 'mm', img_buket_path)\n            output_content.extend(page_markdown)\n        elif make_mode == MakeMode.NLP_MD:\n            page_markdown = ocr_mk_markdown_with_para_core_v2(\n                paras_of_layout, 'nlp')\n            output_content.extend(page_markdown)\n        elif make_mode == MakeMode.STANDARD_FORMAT:\n            for para_block in paras_of_layout:\n                if drop_reason_flag:\n                    para_content = para_to_standard_format_v2(\n                        para_block, img_buket_path, page_idx)\n                else:\n                    para_content = para_to_standard_format_v2(\n                        para_block, img_buket_path, page_idx)\n                output_content.append(para_content)\n    if make_mode in [MakeMode.MM_MD, MakeMode.NLP_MD]:\n        return '\\n\\n'.join(output_content)\n    elif make_mode == MakeMode.STANDARD_FORMAT:\n        return output_content\n\n\ndef get_title_level(block):\n    title_level = block.get('level', 1)\n    if title_level > 4:\n        title_level = 4\n    elif title_level < 1:\n        title_level = 1\n    return title_level"
  },
  {
    "file_name": "magic_pdf/filter/__init__.py",
    "file_contents": "from magic_pdf.config.drop_reason import DropReason\nfrom magic_pdf.config.enums import SupportedPdfParseMethod\nfrom magic_pdf.filter.pdf_classify_by_type import classify as do_classify\nfrom magic_pdf.filter.pdf_meta_scan import pdf_meta_scan\n\n\ndef classify(pdf_bytes: bytes) -> SupportedPdfParseMethod:\n    \"\"\"Determine whether it's text PDF or OCR PDF based on PDF metadata.\"\"\"\n    pdf_meta = pdf_meta_scan(pdf_bytes)\n    if pdf_meta.get('_need_drop', False):  # If returned flag indicates need to drop, throw exception\n        raise Exception(f\"pdf meta_scan need_drop,reason is {pdf_meta['_drop_reason']}\")\n    else:\n        is_encrypted = pdf_meta['is_encrypted']\n        is_needs_password = pdf_meta['is_needs_password']\n        if is_encrypted or is_needs_password:  # Encrypted, password-required, no pages - don't process\n            raise Exception(f'pdf meta_scan need_drop,reason is {DropReason.ENCRYPTED}')\n        else:\n            is_text_pdf, results = do_classify(\n                pdf_meta['total_page'],\n                pdf_meta['page_width_pts'],\n                pdf_meta['page_height_pts'],\n                pdf_meta['image_info_per_page'],\n                pdf_meta['text_len_per_page'],\n                pdf_meta['imgs_per_page'],\n                pdf_meta['text_layout_per_page'],\n                pdf_meta['invalid_chars'],\n            )\n            if is_text_pdf:\n                return SupportedPdfParseMethod.TXT\n            else:\n                return SupportedPdfParseMethod.OCR\n"
  },
  {
    "file_name": "magic_pdf/filter/pdf_classify_by_type.py",
    "file_contents": "\"\"\"\nClassify PDF as text version or scanned version based on results from meta_scan.\nDefinition criteria:\n1. What PDFs are text PDFs - meeting any of the following conditions:\n  1. Randomly sample N pages, if any page has more than 100 text characters\n  2. As long as there exists a page with 0 images\n2. What are scanned PDFs - meeting any of the following conditions:\n  1. ~~80% of pages have the same maximum image size and area exceeds 0.6 of page area~~\n  2. Most pages have equal text length.\n\"\"\"\nimport json\nimport sys\nfrom collections import Counter\n\nimport click\nimport numpy as np\nfrom loguru import logger\n\nfrom magic_pdf.libs.commons import mymax, get_top_percent_list\nfrom magic_pdf.filter.pdf_meta_scan import scan_max_page, junk_limit_min\n\nTEXT_LEN_THRESHOLD = 100\nAVG_TEXT_LEN_THRESHOLD = 100\nTEXT_LEN_SAMPLE_RATIO = 0.1  # Sample 0.1 of pages for text length statistics\n\n# A solution for merging images, combining certain special scanned version split images into one complete image\ndef merge_images(image_list, page_width, page_height, max_offset=5, max_gap=2):\n    # First remove all overlapping bbox image data through set\n    image_list_result = []\n    for page_images in image_list:\n        page_result = []\n        dedup = set()\n        for img in page_images:\n            x0, y0, x1, y1, img_bojid = img\n            if (x0, y0, x1, y1) in dedup:  # Some duplicate bboxes may appear, no need to repeat, need to remove\n                continue\n            else:\n                dedup.add((x0, y0, x1, y1))\n                page_result.append([x0, y0, x1, y1, img_bojid])\n        image_list_result.append(page_result)\n\n    # Next, merge images on the same page that can be stitched together\n    merged_images = []\n    for page_images in image_list_result:\n        if not page_images:\n            continue\n\n        # First sort images on the same page from top to bottom, left to right\n        page_images.sort(key=lambda img: (img[1], img[0]))\n\n        merged = [page_images[0]]\n\n        for img in page_images[1:]:\n            x0, y0, x1, y1, imgid = img\n\n            last_img = merged[-1]\n            last_x0, last_y0, last_x1, last_y1, last_imgid = last_img\n\n            # A single image width or height covering more than 90% of page width/height is a prerequisite for stitching\n            full_width = abs(x1 - x0) >= page_width * 0.9\n            full_height = abs(y1 - y0) >= page_height * 0.9\n\n            # If width meets standard, check if can stitch vertically\n            if full_width:\n                # Vertical stitching needs two prerequisites: left and right boundaries can't offset more than max_offset, first image's bottom boundary and second image's top boundary can't offset more than max_gap\n                close1 = (last_x0 - max_offset) <= x0 <= (last_x0 + max_offset) and (last_x1 - max_offset) <= x1 <= (\n                            last_x1 + max_offset) and (last_y1 - max_gap) <= y0 <= (last_y1 + max_gap)\n\n            # If height meets standard, check if can stitch horizontally\n            if full_height:\n                # Horizontal stitching needs two prerequisites: top and bottom boundaries can't offset more than max_offset, first image's right boundary and second image's left boundary can't offset more than max_gap\n                close2 = (last_y0 - max_offset) <= y0 <= (last_y0 + max_offset) and (last_y1 - max_offset) <= y1 <= (\n                            last_y1 + max_offset) and (last_x1 - max_gap) <= x0 <= (last_x1 + max_gap)\n\n            # Check if the image can be merged with the last image\n            if (full_width and close1) or (full_height and close2):\n                # Merge the image with the last image\n                merged[-1] = [min(x0, last_x0), min(y0, last_y0),\n                              max(x1, last_x1), max(y1, last_y1), imgid]\n            else:\n                # Add the image as a new image\n                merged.append(img)\n\n        merged_images.append(merged)\n\n    return merged_images\n\n\ndef classify_by_area(total_page: int, page_width, page_height, img_sz_list, text_len_list: list):\n    \"\"\"\n    Returns False if 80% of pages have the same maximum image size and area exceeds 0.6 of page area, otherwise returns True\n    \"\"\"\n    # # Only one page without images means it's a text PDF. But also needs to meet one condition, that is, there can't be any text on the page. Some scanned PDFs have blank pages with neither images nor text.\n    # if any([len(img_sz) == 0 for img_sz in img_sz_list]):  # Contains pages without images\n    #     # Now find the index of these pages\n    #     empty_page_index = [i for i, img_sz in enumerate(img_sz_list) if len(img_sz) == 0]\n    #     # Then check if there is any text on these pages\n    #     text_len_at_page_idx = [text_len for i, text_len in enumerate(text_len_list) if i in empty_page_index and text_len > 0]\n    #     if len(text_len_at_page_idx) > TEXT_LEN_THRESHOLD:  # No images, but has text, indicating it might be a text version, if no text then can't be determined, left for next step, now requires the text on this page to exceed a certain threshold\n    #         return True\n\n    # Remove images that appear more than 10 times by objid, these are hidden transparent layers with same id\n    # First count occurrences of each id\n    objid_cnt = Counter([objid for page_img_sz in img_sz_list for _, _, _, _, objid in page_img_sz])\n    # Then remove those appearing more than 10 times\n    if total_page >= scan_max_page:  # New meta_scan only scans first scan_max_page pages, when page count > scan_max_page, treat total_page as scan_max_page\n        total_page = scan_max_page\n\n    repeat_threshold = 2  # Set bad_image threshold to 2\n    # repeat_threshold = min(2, total_page)  # When total_page is 1, repeat_threshold is 1, will cause misjudgment making all img become bad_img\n    bad_image_objid = set([objid for objid, cnt in objid_cnt.items() if cnt >= repeat_threshold])\n    # bad_image_page_idx = [i for i, page_img_sz in enumerate(img_sz_list) if any([objid in bad_image_objid for _, _, _, _, objid in page_img_sz])]\n    # text_len_at_bad_image_page_idx = [text_len for i, text_len in enumerate(text_len_list) if i in bad_image_page_idx and text_len > 0]\n\n    # Special case: a text PDF covers each page with a huge transparent image, huge means image covers more than 90% of page area\n    # fake_image_ids = [objid for objid in bad_image_objid if\n    #                   any([abs((x1 - x0) * (y1 - y0) / page_width * page_height) > 0.9 for images in img_sz_list for\n    #                        x0, y0, x1, y1, _ in images])]  # Original code, any inside always true, reasonÔºüÔºüÔºü\n    # fake_image_ids = [objid for objid in bad_image_objid for images in img_sz_list for x0, y0, x1, y1, img_id in images\n    #                   if img_id == objid and abs((x1 - x0) * (y1 - y0)) / (page_width * page_height) > 0.9]\n\n    # if len(fake_image_ids) > 0 and any([l > TEXT_LEN_THRESHOLD for l in text_len_at_bad_image_page_idx]):  # These transparent images' pages have text greater than threshold\n    #     return True\n\n    img_sz_list = [[img_sz for img_sz in page_img_sz if img_sz[-1] not in bad_image_objid] for page_img_sz in\n                   img_sz_list]  # Filter out repeatedly appearing images\n\n    # Some scanned versions split one page image into many, need to stitch images first then calculate\n    img_sz_list = merge_images(img_sz_list, page_width, page_height)\n\n    # Calculate maximum image area per page, then calculate ratio of this area to page area\n    max_image_area_per_page = [mymax([(x1 - x0) * (y1 - y0) for x0, y0, x1, y1, _ in page_img_sz]) for page_img_sz in\n                               img_sz_list]\n    page_area = page_width * page_height\n    max_image_area_per_page = [area / page_area for area in max_image_area_per_page]\n    max_image_area_per_page = [area for area in max_image_area_per_page if area > 0.5]\n\n    if len(max_image_area_per_page) >= 0.5 * total_page:  # Threshold changed from 0.8 to 0.5, adapt to cases like 2 out of 3 pages and 1 out of 2 pages\n        # Prerequisite for this condition is removing repeatedly appearing images. These are hidden transparent layers with same id\n        return False\n    else:\n        return True\n\n\ndef classify_by_text_len(text_len_list: list, total_page: int):\n    \"\"\"\n    Randomly sample 10% of pages, if less than 5 pages, take all pages.\n    Check text length on pages, if any page has text length > TEXT_LEN_THRESHOLD, then it's text PDF\n    \"\"\"\n    select_page_cnt = int(total_page * TEXT_LEN_SAMPLE_RATIO)  # Select 10% of pages\n    if select_page_cnt < 5:\n        select_page_cnt = total_page\n\n    # # Excluding first and last 10 pages\n    # if total_page > 20:  # If total pages > 20\n    #     page_range = list(range(10, total_page - 10))  # From 11th page to the last 11th page\n    # else:\n    #     page_range = list(range(total_page))  # Otherwise select all pages\n    # page_num = np.random.choice(page_range, min(select_page_cnt, len(page_range)), replace=False)\n    # Excluding first and last 10 pages is awkward for PDFs with only 21, 22 pages, if the selected middle 1-2 pages happen to have no text, easy to misjudge, with avg_words rule, this rule can be ignored\n    page_num = np.random.choice(total_page, select_page_cnt, replace=False)\n    text_len_lst = [text_len_list[i] for i in page_num]\n    is_text_pdf = any([text_len > TEXT_LEN_THRESHOLD for text_len in text_len_lst])\n    return is_text_pdf\n\n\ndef classify_by_avg_words(text_len_list: list):\n    \"\"\"\n    Supplementary rule: if average words per page < AVG_TEXT_LEN_THRESHOLD, not text PDF\n    Mainly for various image collections\n    \"\"\"\n    sum_words = sum(text_len_list)\n    count_of_numbers = len(text_len_list)\n    if count_of_numbers == 0:\n        is_text_pdf = False\n    else:\n        avg_words = round(sum_words / count_of_numbers)\n        if avg_words > AVG_TEXT_LEN_THRESHOLD:\n            is_text_pdf = True\n        else:\n            is_text_pdf = False\n\n    return is_text_pdf\n\n\ndef classify_by_img_num(img_sz_list: list, img_num_list: list):\n    \"\"\"\n    Supplementary rule: there's a type of scanned PDF that puts all scanned pages on each page, which gets deduplicated during metascan,\n    characteristic of this PDF's metascan result is all empty elements in img_sz_list, each page in img_num_list has large and same count\n    \"\"\"\n    # Calculate number of non-empty elements in img_sz_list\n    count_img_sz_list_not_none = sum(1 for item in img_sz_list if item)\n    # Get top 80% elements\n    top_eighty_percent = get_top_percent_list(img_num_list, 0.8)\n    # Non-empty elements in img_sz_list <= 1, top 80% elements are all equal, and max value >= junk_limit_min\n    if count_img_sz_list_not_none <= 1 and len(set(top_eighty_percent)) == 1 and max(img_num_list) >= junk_limit_min:\n        return False  # If meets this condition, definitely not text PDF\n    else:\n        return True  # If doesn't meet these three conditions, might be text PDF, judge by other rules\n\n\ndef classify_by_text_layout(text_layout_per_page: list):\n    \"\"\"\n    Judge if text layout is mainly vertical.\n\n    Args:\n        text_layout_per_page (list): Text layout list, each element represents text layout of one page,\n                                     'vertical' means vertical layout, 'horizontal' means horizontal layout.\n\n    Returns:\n        bool: If text layout is mainly vertical, return False; otherwise return True.\n    \"\"\"\n    # Count vertical layouts in text_layout_per_page\n    count_vertical = sum(1 for item in text_layout_per_page if item == 'vertical')\n    # Count horizontal layouts in text_layout_per_page\n    count_horizontal = sum(1 for item in text_layout_per_page if item == 'horizontal')\n    # Calculate ratio of vertical layouts in text_layout_per_page\n    known_layout_cnt = count_vertical + count_horizontal\n    if known_layout_cnt != 0:\n        ratio = count_vertical / known_layout_cnt\n        if ratio >= 0.5:  # Threshold set to 0.5, adapt to cases like 2 out of 3 pages and 1 out of 2 pages\n            return False  # Text layout is mainly vertical, consider not text PDF\n        else:\n            return True  # Text layout is mainly horizontal, consider text PDF\n    else:\n        return False  # Text layout unknown, default consider not text PDF\n\n\ndef classify_by_img_narrow_strips(page_width, page_height, img_sz_list):\n    \"\"\"\n    Judge if a page consists of narrow strips, two conditions:\n    1. Image width or height reaches 90% of page width or height, and long side needs to be multiple times longer than short side\n    2. 80% or more of all images on the entire page meet condition 1\n\n    Args:\n        page_width (float): Page width\n        page_height (float): Page height\n        img_sz_list (list): Image size list, each element is a tuple representing image rectangle area and size, format (x0, y0, x1, y1, size), where (x0, y0) is top-left corner coordinate, (x1, y1) is bottom-right corner coordinate, size is image size\n\n    Returns:\n        bool: If ratio of pages meeting conditions < 0.5, return True, otherwise return False\n    \"\"\"\n\n    def is_narrow_strip(img):\n        x0, y0, x1, y1, _ = img\n        width, height = x1 - x0, y1 - y0\n        return any([\n            # Image width >= 90% of page width, and width >= 4 times height\n            width >= page_width * 0.9 and width >= height * 4,\n            # Image height >= 90% of page height, and height >= 4 times width\n            height >= page_height * 0.9 and height >= width * 4,\n        ])\n\n    # Initialize count of pages meeting conditions\n    narrow_strip_pages_count = 0\n\n    # Traverse all pages\n    for page_img_list in img_sz_list:\n        # Ignore empty pages\n        if not page_img_list:\n            continue\n\n        # Calculate total number of images on page\n        total_images = len(page_img_list)\n\n        # Calculate number of narrow strip images on page\n        narrow_strip_images_count = 0\n        for img in page_img_list:\n            if is_narrow_strip(img):\n                narrow_strip_images_count += 1\n        # If narrow strip image count < 5, skip\n        if narrow_strip_images_count < 5:\n            continue\n        else:\n            # If narrow strip image ratio >= 0.8, increase count of pages meeting conditions\n            if narrow_strip_images_count / total_images >= 0.8:\n                narrow_strip_pages_count += 1\n\n    # Calculate ratio of pages meeting conditions\n    narrow_strip_pages_ratio = narrow_strip_pages_count / len(img_sz_list)\n\n    return narrow_strip_pages_ratio < 0.5\n\n\ndef classify(total_page: int, page_width, page_height, img_sz_list: list, text_len_list: list, img_num_list: list,\n             text_layout_list: list, invalid_chars: bool):\n    \"\"\"\n    Image and page length units here are pts\n    \"\"\"\n    results = {\n        'by_image_area': classify_by_area(total_page, page_width, page_height, img_sz_list, text_len_list),\n        'by_text_len': classify_by_text_len(text_len_list, total_page),\n        'by_avg_words': classify_by_avg_words(text_len_list),\n        'by_img_num': classify_by_img_num(img_sz_list, img_num_list),\n        'by_text_layout': classify_by_text_layout(text_layout_list),\n        'by_img_narrow_strips': classify_by_img_narrow_strips(page_width, page_height, img_sz_list),\n        'by_invalid_chars': invalid_chars,\n    }\n\n    if all(results.values()):\n        return True, results\n    elif not any(results.values()):\n        return False, results\n    else:\n        logger.warning(\n            f\"pdf is not classified by area and text_len, by_image_area: {results['by_image_area']},\"\n            f\" by_text: {results['by_text_len']}, by_avg_words: {results['by_avg_words']}, by_img_num: {results['by_img_num']},\"\n            f\" by_text_layout: {results['by_text_layout']}, by_img_narrow_strips: {results['by_img_narrow_strips']},\"\n            f\" by_invalid_chars: {results['by_invalid_chars']}\",\n            file=sys.stderr)  # Use this situation to quickly find which PDFs are special, and fix classification algorithm accordingly\n        return False, results\n\n\n@click.command()\n@click.option(\"--json-file\", type=str, help=\"PDF information\")\ndef main(json_file):\n    if json_file is None:\n        print(\"json_file is None\", file=sys.stderr)\n        exit(0)\n    try:\n        with open(json_file, \"r\") as f:\n            for l in f:\n                if l.strip() == \"\":\n                    continue\n                o = json.loads(l)\n                total_page = o[\"total_page\"]\n                page_width = o[\"page_width_pts\"]\n                page_height = o[\"page_height_pts\"]\n                img_sz_list = o[\"image_info_per_page\"]\n                text_len_list = o['text_len_per_page']\n                text_layout_list = o['text_layout_per_page']\n                pdf_path = o['pdf_path']\n                is_encrypted = o['is_encrypted']\n                is_needs_password = o['is_needs_password']\n                if is_encrypted or total_page == 0 or is_needs_password:  # Encrypted, password-required, no pages - don't process\n                    continue\n                tag = classify(total_page, page_width, page_height, img_sz_list, text_len_list, text_layout_list)\n                o['is_text_pdf'] = tag\n                print(json.dumps(o, ensure_ascii=False))\n    except Exception as e:\n        print(\"ERROR: \", e, file=sys.stderr)"
  },
  {
    "file_name": "magic_pdf/filter/pdf_meta_scan.py",
    "file_contents": "from collections import Counter\n\nimport fitz\nfrom loguru import logger\n\nfrom magic_pdf.config.drop_reason import DropReason\nfrom magic_pdf.libs.commons import get_top_percent_list, mymax\nfrom magic_pdf.libs.language import detect_lang\nfrom magic_pdf.libs.pdf_check import detect_invalid_chars\n\nscan_max_page = 50\njunk_limit_min = 10\n\n\ndef calculate_max_image_area_per_page(result: list, page_width_pts, page_height_pts):\n    max_image_area_per_page = [\n        mymax([(x1 - x0) * (y1 - y0) for x0, y0, x1, y1, _ in page_img_sz])\n        for page_img_sz in result\n    ]\n    page_area = int(page_width_pts) * int(page_height_pts)\n    max_image_area_per_page = [area / page_area for area in max_image_area_per_page]\n    max_image_area_per_page = [area for area in max_image_area_per_page if area > 0.6]\n    return max_image_area_per_page\n\n\ndef process_image(page, junk_img_bojids=[]):\n    page_result = []\n    items = page.get_images()\n    dedup = set()\n    for img in items:\n\n        img_bojid = img[\n            0\n        ]\n        if img_bojid in junk_img_bojids:\n            continue\n        recs = page.get_image_rects(img, transform=True)\n        if recs:\n            rec = recs[0][0]\n            x0, y0, x1, y1 = map(int, rec)\n            width = x1 - x0\n            height = y1 - y0\n            if (\n                x0,\n                y0,\n                x1,\n                y1,\n                img_bojid,\n            ) in dedup:\n                continue\n            if not all(\n                [width, height]\n            ):\n                continue\n            dedup.add((x0, y0, x1, y1, img_bojid))\n            page_result.append([x0, y0, x1, y1, img_bojid])\n    return page_result\n\n\ndef get_image_info(doc: fitz.Document, page_width_pts, page_height_pts) -> list:\n\n    img_bojid_counter = Counter(img[0] for page in doc for img in page.get_images())\n\n\n    junk_limit = max(len(doc) * 0.5, junk_limit_min)\n\n    junk_img_bojids = [\n        img_bojid\n        for img_bojid, count in img_bojid_counter.items()\n        if count >= junk_limit\n    ]\n\n\n\n\n\n\n    imgs_len_list = [len(page.get_images()) for page in doc]\n\n    special_limit_pages = 10\n\n\n    result = []\n    break_loop = False\n    for i, page in enumerate(doc):\n        if break_loop:\n            break\n        if i >= special_limit_pages:\n            break\n        page_result = process_image(\n            page\n        )\n        result.append(page_result)\n        for item in result:\n            if not any(\n                item\n            ):\n                if (\n                    max(imgs_len_list) == min(imgs_len_list)\n                    and max(imgs_len_list) >= junk_limit_min\n                ):\n                    junk_img_bojids = []\n                else:\n                    pass\n                break_loop = True\n                break\n    if not break_loop:\n\n        top_eighty_percent = get_top_percent_list(imgs_len_list, 0.8)\n\n        if len(set(top_eighty_percent)) == 1 and max(imgs_len_list) >= junk_limit_min:\n\n            # if max(imgs_len_list) == min(imgs_len_list) and max(imgs_len_list) >= junk_limit_min:\n\n\n            max_image_area_per_page = calculate_max_image_area_per_page(\n                result, page_width_pts, page_height_pts\n            )\n            if (\n                len(max_image_area_per_page) < 0.8 * special_limit_pages\n            ):\n                junk_img_bojids = []\n            else:\n                pass\n        else:\n            junk_img_bojids = []\n\n\n    result = []\n    for i, page in enumerate(doc):\n        if i >= scan_max_page:\n            break\n        page_result = process_image(page, junk_img_bojids)\n        # logger.info(f\"page {i} img_len: {len(page_result)}\")\n        result.append(page_result)\n\n    return result, junk_img_bojids\n\n\ndef get_pdf_page_size_pts(doc: fitz.Document):\n    page_cnt = len(doc)\n    l: int = min(page_cnt, 50)\n\n    page_width_list = []\n    page_height_list = []\n    for i in range(l):\n        page = doc[i]\n        page_rect = page.rect\n        page_width_list.append(page_rect.width)\n        page_height_list.append(page_rect.height)\n\n    page_width_list.sort()\n    page_height_list.sort()\n\n    median_width = page_width_list[len(page_width_list) // 2]\n    median_height = page_height_list[len(page_height_list) // 2]\n\n    return median_width, median_height\n\n\ndef get_pdf_textlen_per_page(doc: fitz.Document):\n    text_len_lst = []\n    for page in doc:\n\n        # text_block = page.get_text(\"blocks\")\n\n        # text_block = page.get_text(\"words\")\n        # text_block_len = sum([len(t[4]) for t in text_block])\n\n        text_block = page.get_text('text')\n        text_block_len = len(text_block)\n        # logger.info(f\"page {page.number} text_block_len: {text_block_len}\")\n        text_len_lst.append(text_block_len)\n\n    return text_len_lst\n\n\ndef get_pdf_text_layout_per_page(doc: fitz.Document):\n    text_layout_list = []\n\n    for page_id, page in enumerate(doc):\n        if page_id >= scan_max_page:\n            break\n\n        vertical_count = 0\n        horizontal_count = 0\n        text_dict = page.get_text('dict')\n        if 'blocks' in text_dict:\n            for block in text_dict['blocks']:\n                if 'lines' in block:\n                    for line in block['lines']:\n\n                        x0, y0, x1, y1 = line['bbox']\n\n                        width = x1 - x0\n                        height = y1 - y0\n\n                        area = width * height\n                        font_sizes = []\n                        for span in line['spans']:\n                            if 'size' in span:\n                                font_sizes.append(span['size'])\n                        if len(font_sizes) > 0:\n                            average_font_size = sum(font_sizes) / len(font_sizes)\n                        else:\n                            average_font_size = (\n                                10\n                            )\n                        if (\n                            area <= average_font_size**2\n                        ):\n                            continue\n                        else:\n                            if 'wmode' in line:\n                                if line['wmode'] == 1:\n                                    vertical_count += 1\n                                elif line['wmode'] == 0:\n                                    horizontal_count += 1\n\n\n                        #         dir_value = line['dir']\n                        #         cosine, sine = dir_value\n\n                        #         angle = math.degrees(math.acos(cosine))\n                        #\n\n                        #         if abs(angle - 0) < 0.01 or abs(angle - 180) < 0.01:\n                        #             # line_text = ' '.join(span['text'] for span in line['spans'])\n                        #             # print('This line is horizontal:', line_text)\n                        #             horizontal_count += 1\n\n                        #         elif abs(angle - 90) < 0.01 or abs(angle - 270) < 0.01:\n                        #             # line_text = ' '.join(span['text'] for span in line['spans'])\n                        #             # print('This line is vertical:', line_text)\n                        #             vertical_count += 1\n        # print(f\"page_id: {page_id}, vertical_count: {vertical_count}, horizontal_count: {horizontal_count}\")\n\n        if vertical_count == 0 and horizontal_count == 0:\n            text_layout_list.append('unknow')\n            continue\n        else:\n            if vertical_count > horizontal_count:\n                text_layout_list.append('vertical')\n            else:\n                text_layout_list.append('horizontal')\n        # logger.info(f\"page_id: {page_id}, vertical_count: {vertical_count}, horizontal_count: {horizontal_count}\")\n    return text_layout_list\n\n\nclass PageSvgsTooManyError(Exception):\n    def __init__(self, message='Page SVGs are too many'):\n        self.message = message\n        super().__init__(self.message)\n\n\ndef get_svgs_per_page(doc: fitz.Document):\n    svgs_len_list = []\n    for page_id, page in enumerate(doc):\n        # svgs = page.get_drawings()\n        svgs = page.get_cdrawings()\n        len_svgs = len(svgs)\n        if len_svgs >= 3000:\n            raise PageSvgsTooManyError()\n        else:\n            svgs_len_list.append(len_svgs)\n        # logger.info(f\"page_id: {page_id}, svgs_len: {len(svgs)}\")\n    return svgs_len_list\n\n\ndef get_imgs_per_page(doc: fitz.Document):\n    imgs_len_list = []\n    for page_id, page in enumerate(doc):\n        imgs = page.get_images()\n        imgs_len_list.append(len(imgs))\n        # logger.info(f\"page_id: {page}, imgs_len: {len(imgs)}\")\n\n    return imgs_len_list\n\n\ndef get_language(doc: fitz.Document):\n    language_lst = []\n    for page_id, page in enumerate(doc):\n        if page_id >= scan_max_page:\n            break\n\n        text_block = page.get_text('text')\n        page_language = detect_lang(text_block)\n        language_lst.append(page_language)\n\n        # logger.info(f\"page_id: {page_id}, page_language: {page_language}\")\n\n\n    count_dict = Counter(language_lst)\n\n    language = max(count_dict, key=count_dict.get)\n    return language\n\n\ndef check_invalid_chars(pdf_bytes):\n    # return detect_invalid_chars_by_pymupdf(pdf_bytes)\n    return detect_invalid_chars(pdf_bytes)\n\n\ndef pdf_meta_scan(pdf_bytes: bytes):\n    doc = fitz.open('pdf', pdf_bytes)\n    is_needs_password = doc.needs_pass\n    is_encrypted = doc.is_encrypted\n    total_page = len(doc)\n    if total_page == 0:\n        logger.warning(f'drop this pdf, drop_reason: {DropReason.EMPTY_PDF}')\n        result = {'_need_drop': True, '_drop_reason': DropReason.EMPTY_PDF}\n        return result\n    else:\n        page_width_pts, page_height_pts = get_pdf_page_size_pts(doc)\n        # logger.info(f\"page_width_pts: {page_width_pts}, page_height_pts: {page_height_pts}\")\n\n        # svgs_per_page = get_svgs_per_page(doc)\n        # logger.info(f\"svgs_per_page: {svgs_per_page}\")\n        imgs_per_page = get_imgs_per_page(doc)\n        # logger.info(f\"imgs_per_page: {imgs_per_page}\")\n\n        image_info_per_page, junk_img_bojids = get_image_info(\n            doc, page_width_pts, page_height_pts\n        )\n        # logger.info(f\"image_info_per_page: {image_info_per_page}, junk_img_bojids: {junk_img_bojids}\")\n        text_len_per_page = get_pdf_textlen_per_page(doc)\n        # logger.info(f\"text_len_per_page: {text_len_per_page}\")\n        text_layout_per_page = get_pdf_text_layout_per_page(doc)\n        # logger.info(f\"text_layout_per_page: {text_layout_per_page}\")\n        text_language = get_language(doc)\n        # logger.info(f\"text_language: {text_language}\")\n        invalid_chars = check_invalid_chars(pdf_bytes)\n        # logger.info(f\"invalid_chars: {invalid_chars}\")\n\n\n        res = {\n            'is_needs_password': is_needs_password,\n            'is_encrypted': is_encrypted,\n            'total_page': total_page,\n            'page_width_pts': int(page_width_pts),\n            'page_height_pts': int(page_height_pts),\n            'image_info_per_page': image_info_per_page,\n            'text_len_per_page': text_len_per_page,\n            'text_layout_per_page': text_layout_per_page,\n            'text_language': text_language,\n            # \"svgs_per_page\": svgs_per_page,\n            'imgs_per_page': imgs_per_page,\n            'junk_img_bojids': junk_img_bojids,\n            'invalid_chars': invalid_chars,\n            'metadata': doc.metadata,\n        }\n        # logger.info(json.dumps(res, ensure_ascii=False))\n        return res\n\n\nif __name__ == '__main__':\n    pass\n\n\n    # \"D:\\project/20231108code-clean\\pdf_cost_time\\scihub\\scihub_86800000\\libgen.scimag86880000-86880999.zip_10.1021/acsami.1c03109.s002.pdf\"\n    # \"D:/project/20231108code-clean/pdf_cost_time/scihub/scihub_18600000/libgen.scimag18645000-18645999.zip_10.1021/om3006239.pdf\"\n    # file_content = read_file(\"D:/project/20231108code-clean/pdf_cost_time/scihub/scihub_31000000/libgen.scimag31098000-31098999.zip_10.1109/isit.2006.261791.pdf\",\"\")  # noqa: E501\n\n    # doc = fitz.open(\"pdf\", file_content)\n    # text_layout_lst = get_pdf_text_layout_per_page(doc)\n    # print(text_layout_lst)\n"
  },
  {
    "file_name": "magic_pdf/libs/boxbase.py",
    "file_contents": "import math\n\n\ndef _is_in_or_part_overlap(box1, box2) -> bool:\n    if box1 is None or box2 is None:\n        return False\n\n    x0_1, y0_1, x1_1, y1_1 = box1\n    x0_2, y0_2, x1_2, y1_2 = box2\n\n    return not (x1_1 < x0_2 or\n                x0_1 > x1_2 or\n                y1_1 < y0_2 or\n                y0_1 > y1_2)\n\n\ndef _is_in_or_part_overlap_with_area_ratio(box1,\n                                           box2,\n                                           area_ratio_threshold=0.6):\n    if box1 is None or box2 is None:\n        return False\n\n    x0_1, y0_1, x1_1, y1_1 = box1\n    x0_2, y0_2, x1_2, y1_2 = box2\n\n    if not _is_in_or_part_overlap(box1, box2):\n        return False\n\n\n    x_left = max(x0_1, x0_2)\n    y_top = max(y0_1, y0_2)\n    x_right = min(x1_1, x1_2)\n    y_bottom = min(y1_1, y1_2)\n    overlap_area = (x_right - x_left) * (y_bottom - y_top)\n\n\n    box1_area = (x1_1 - x0_1) * (y1_1 - y0_1)\n\n    return overlap_area / box1_area > area_ratio_threshold\n\n\ndef _is_in(box1, box2) -> bool:\n    x0_1, y0_1, x1_1, y1_1 = box1\n    x0_2, y0_2, x1_2, y1_2 = box2\n\n    return (x0_1 >= x0_2 and\n            y0_1 >= y0_2 and\n            x1_1 <= x1_2 and\n            y1_1 <= y1_2)\n\n\ndef _is_part_overlap(box1, box2) -> bool:\n    if box1 is None or box2 is None:\n        return False\n\n    return _is_in_or_part_overlap(box1, box2) and not _is_in(box1, box2)\n\n\ndef _left_intersect(left_box, right_box):\n    if left_box is None or right_box is None:\n        return False\n\n    x0_1, y0_1, x1_1, y1_1 = left_box\n    x0_2, y0_2, x1_2, y1_2 = right_box\n\n    return x1_1 > x0_2 and x0_1 < x0_2 and (y0_1 <= y0_2 <= y1_1\n                                            or y0_1 <= y1_2 <= y1_1)\n\n\ndef _right_intersect(left_box, right_box):\n    if left_box is None or right_box is None:\n        return False\n\n    x0_1, y0_1, x1_1, y1_1 = left_box\n    x0_2, y0_2, x1_2, y1_2 = right_box\n\n    return x0_1 < x1_2 and x1_1 > x1_2 and (y0_1 <= y0_2 <= y1_1\n                                            or y0_1 <= y1_2 <= y1_1)\n\n\ndef _is_vertical_full_overlap(box1, box2, x_tolerance=2):\n\n    x11, y11, x12, y12 = box1\n    x21, y21, x22, y22 = box2\n\n\n    contains_in_x = (x11 - x_tolerance <= x21 and x12 + x_tolerance >= x22) or (\n        x21 - x_tolerance <= x11 and x22 + x_tolerance >= x12)\n\n\n    overlap_in_y = not (y12 < y21 or y11 > y22)\n\n    return contains_in_x and overlap_in_y\n\n\ndef _is_bottom_full_overlap(box1, box2, y_tolerance=2):\n    if box1 is None or box2 is None:\n        return False\n\n    x0_1, y0_1, x1_1, y1_1 = box1\n    x0_2, y0_2, x1_2, y1_2 = box2\n    tolerance_margin = 2\n    is_xdir_full_overlap = (\n        (x0_1 - tolerance_margin <= x0_2 <= x1_1 + tolerance_margin\n         and x0_1 - tolerance_margin <= x1_2 <= x1_1 + tolerance_margin)\n        or (x0_2 - tolerance_margin <= x0_1 <= x1_2 + tolerance_margin\n            and x0_2 - tolerance_margin <= x1_1 <= x1_2 + tolerance_margin))\n\n    return y0_2 < y1_1 and 0 < (y1_1 -\n                                y0_2) < y_tolerance and is_xdir_full_overlap\n\n\ndef _is_left_overlap(\n    box1,\n    box2,\n):\n\n    def __overlap_y(Ay1, Ay2, By1, By2):\n        return max(0, min(Ay2, By2) - max(Ay1, By1))\n\n    if box1 is None or box2 is None:\n        return False\n\n    x0_1, y0_1, x1_1, y1_1 = box1\n    x0_2, y0_2, x1_2, y1_2 = box2\n\n    y_overlap_len = __overlap_y(y0_1, y1_1, y0_2, y1_2)\n    ratio_1 = 1.0 * y_overlap_len / (y1_1 - y0_1) if y1_1 - y0_1 != 0 else 0\n    ratio_2 = 1.0 * y_overlap_len / (y1_2 - y0_2) if y1_2 - y0_2 != 0 else 0\n    vertical_overlap_cond = ratio_1 >= 0.5 or ratio_2 >= 0.5\n\n    # vertical_overlap_cond = y0_1<=y0_2<=y1_1 or y0_1<=y1_2<=y1_1 or y0_2<=y0_1<=y1_2 or y0_2<=y1_1<=y1_2\n    return x0_1 <= x0_2 <= x1_1 and vertical_overlap_cond\n\n\ndef __is_overlaps_y_exceeds_threshold(bbox1,\n                                      bbox2,\n                                      overlap_ratio_threshold=0.8):\n    _, y0_1, _, y1_1 = bbox1\n    _, y0_2, _, y1_2 = bbox2\n\n    overlap = max(0, min(y1_1, y1_2) - max(y0_1, y0_2))\n    height1, height2 = y1_1 - y0_1, y1_2 - y0_2\n    # max_height = max(height1, height2)\n    min_height = min(height1, height2)\n\n    return (overlap / min_height) > overlap_ratio_threshold\n\n\ndef calculate_iou(bbox1, bbox2):\n    # Determine the coordinates of the intersection rectangle\n    x_left = max(bbox1[0], bbox2[0])\n    y_top = max(bbox1[1], bbox2[1])\n    x_right = min(bbox1[2], bbox2[2])\n    y_bottom = min(bbox1[3], bbox2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # The area of overlap area\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n\n    # The area of both rectangles\n    bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n    bbox2_area = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n\n    if any([bbox1_area == 0, bbox2_area == 0]):\n        return 0\n\n    # Compute the intersection over union by taking the intersection area\n    # and dividing it by the sum of both areas minus the intersection area\n    iou = intersection_area / float(bbox1_area + bbox2_area - intersection_area)\n\n    return iou\n\n\ndef calculate_overlap_area_2_minbox_area_ratio(bbox1, bbox2):\n    # Determine the coordinates of the intersection rectangle\n    x_left = max(bbox1[0], bbox2[0])\n    y_top = max(bbox1[1], bbox2[1])\n    x_right = min(bbox1[2], bbox2[2])\n    y_bottom = min(bbox1[3], bbox2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # The area of overlap area\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    min_box_area = min([(bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1]),\n                        (bbox2[3] - bbox2[1]) * (bbox2[2] - bbox2[0])])\n    if min_box_area == 0:\n        return 0\n    else:\n        return intersection_area / min_box_area\n\n\ndef calculate_overlap_area_in_bbox1_area_ratio(bbox1, bbox2):\n    # Determine the coordinates of the intersection rectangle\n    x_left = max(bbox1[0], bbox2[0])\n    y_top = max(bbox1[1], bbox2[1])\n    x_right = min(bbox1[2], bbox2[2])\n    y_bottom = min(bbox1[3], bbox2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # The area of overlap area\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n    if bbox1_area == 0:\n        return 0\n    else:\n        return intersection_area / bbox1_area\n\n\ndef get_minbox_if_overlap_by_ratio(bbox1, bbox2, ratio):\n    x1_min, y1_min, x1_max, y1_max = bbox1\n    x2_min, y2_min, x2_max, y2_max = bbox2\n    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n    overlap_ratio = calculate_overlap_area_2_minbox_area_ratio(bbox1, bbox2)\n    if overlap_ratio > ratio:\n        if area1 <= area2:\n            return bbox1\n        else:\n            return bbox2\n    else:\n        return None\n\n\ndef get_bbox_in_boundary(bboxes: list, boundary: tuple) -> list:\n    x0, y0, x1, y1 = boundary\n    new_boxes = [\n        box for box in bboxes\n        if box[0] >= x0 and box[1] >= y0 and box[2] <= x1 and box[3] <= y1\n    ]\n    return new_boxes\n\n\ndef is_vbox_on_side(bbox, width, height, side_threshold=0.2):\n    x0, x1 = bbox[0], bbox[2]\n    if x1 <= width * side_threshold or x0 >= width * (1 - side_threshold):\n        return True\n    return False\n\n\ndef find_top_nearest_text_bbox(pymu_blocks, obj_bbox):\n    tolerance_margin = 4\n    top_boxes = [\n        box for box in pymu_blocks\n        if obj_bbox[1] - box['bbox'][3] >= -tolerance_margin\n        and not _is_in(box['bbox'], obj_bbox)\n    ]\n\n    top_boxes = [\n        box for box in top_boxes if any([\n            obj_bbox[0] - tolerance_margin <= box['bbox'][0] <= obj_bbox[2] +\n            tolerance_margin, obj_bbox[0] -\n            tolerance_margin <= box['bbox'][2] <= obj_bbox[2] +\n            tolerance_margin, box['bbox'][0] -\n            tolerance_margin <= obj_bbox[0] <= box['bbox'][2] +\n            tolerance_margin, box['bbox'][0] -\n            tolerance_margin <= obj_bbox[2] <= box['bbox'][2] +\n            tolerance_margin\n        ])\n    ]\n\n\n    if len(top_boxes) > 0:\n        top_boxes.sort(key=lambda x: x['bbox'][3], reverse=True)\n        return top_boxes[0]\n    else:\n        return None\n\n\ndef find_bottom_nearest_text_bbox(pymu_blocks, obj_bbox):\n    bottom_boxes = [\n        box for box in pymu_blocks if box['bbox'][1] -\n        obj_bbox[3] >= -2 and not _is_in(box['bbox'], obj_bbox)\n    ]\n\n    bottom_boxes = [\n        box for box in bottom_boxes if any([\n            obj_bbox[0] - 2 <= box['bbox'][0] <= obj_bbox[2] + 2, obj_bbox[0] -\n            2 <= box['bbox'][2] <= obj_bbox[2] + 2, box['bbox'][0] -\n            2 <= obj_bbox[0] <= box['bbox'][2] + 2, box['bbox'][0] -\n            2 <= obj_bbox[2] <= box['bbox'][2] + 2\n        ])\n    ]\n\n\n    if len(bottom_boxes) > 0:\n        bottom_boxes.sort(key=lambda x: x['bbox'][1], reverse=False)\n        return bottom_boxes[0]\n    else:\n        return None\n\n\ndef find_left_nearest_text_bbox(pymu_blocks, obj_bbox):\n    left_boxes = [\n        box for box in pymu_blocks if obj_bbox[0] -\n        box['bbox'][2] >= -2 and not _is_in(box['bbox'], obj_bbox)\n    ]\n\n    left_boxes = [\n        box for box in left_boxes if any([\n            obj_bbox[1] - 2 <= box['bbox'][1] <= obj_bbox[3] + 2, obj_bbox[1] -\n            2 <= box['bbox'][3] <= obj_bbox[3] + 2, box['bbox'][1] -\n            2 <= obj_bbox[1] <= box['bbox'][3] + 2, box['bbox'][1] -\n            2 <= obj_bbox[3] <= box['bbox'][3] + 2\n        ])\n    ]\n\n\n    if len(left_boxes) > 0:\n        left_boxes.sort(key=lambda x: x['bbox'][2], reverse=True)\n        return left_boxes[0]\n    else:\n        return None\n\n\ndef find_right_nearest_text_bbox(pymu_blocks, obj_bbox):\n    right_boxes = [\n        box for box in pymu_blocks if box['bbox'][0] -\n        obj_bbox[2] >= -2 and not _is_in(box['bbox'], obj_bbox)\n    ]\n\n    right_boxes = [\n        box for box in right_boxes if any([\n            obj_bbox[1] - 2 <= box['bbox'][1] <= obj_bbox[3] + 2, obj_bbox[1] -\n            2 <= box['bbox'][3] <= obj_bbox[3] + 2, box['bbox'][1] -\n            2 <= obj_bbox[1] <= box['bbox'][3] + 2, box['bbox'][1] -\n            2 <= obj_bbox[3] <= box['bbox'][3] + 2\n        ])\n    ]\n\n\n    if len(right_boxes) > 0:\n        right_boxes.sort(key=lambda x: x['bbox'][0], reverse=False)\n        return right_boxes[0]\n    else:\n        return None\n\n\ndef bbox_relative_pos(bbox1, bbox2):\n    x1, y1, x1b, y1b = bbox1\n    x2, y2, x2b, y2b = bbox2\n\n    left = x2b < x1\n    right = x1b < x2\n    bottom = y2b < y1\n    top = y1b < y2\n    return left, right, bottom, top\n\n\ndef bbox_distance(bbox1, bbox2):\n\n    def dist(point1, point2):\n        return math.sqrt((point1[0] - point2[0])**2 +\n                         (point1[1] - point2[1])**2)\n\n    x1, y1, x1b, y1b = bbox1\n    x2, y2, x2b, y2b = bbox2\n\n    left, right, bottom, top = bbox_relative_pos(bbox1, bbox2)\n\n    if top and left:\n        return dist((x1, y1b), (x2b, y2))\n    elif left and bottom:\n        return dist((x1, y1), (x2b, y2b))\n    elif bottom and right:\n        return dist((x1b, y1), (x2, y2b))\n    elif right and top:\n        return dist((x1b, y1b), (x2, y2))\n    elif left:\n        return x1 - x2b\n    elif right:\n        return x2 - x1b\n    elif bottom:\n        return y1 - y2b\n    elif top:\n        return y2 - y1b\n    return 0.0\n\n\ndef box_area(bbox):\n    return (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n\n\ndef get_overlap_area(bbox1, bbox2):\n    # Determine the coordinates of the intersection rectangle\n    x_left = max(bbox1[0], bbox2[0])\n    y_top = max(bbox1[1], bbox2[1])\n    x_right = min(bbox1[2], bbox2[2])\n    y_bottom = min(bbox1[3], bbox2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # The area of overlap area\n    return (x_right - x_left) * (y_bottom - y_top)\n\n\ndef calculate_vertical_projection_overlap_ratio(block1, block2):\n    \"\"\"\n    Calculate the proportion of the x-axis covered by the vertical projection of two blocks.\n\n    Args:\n        block1 (tuple): Coordinates of the first block (x0, y0, x1, y1).\n        block2 (tuple): Coordinates of the second block (x0, y0, x1, y1).\n\n    Returns:\n        float: The proportion of the x-axis covered by the vertical projection of the two blocks.\n    \"\"\"\n    x0_1, _, x1_1, _ = block1\n    x0_2, _, x1_2, _ = block2\n\n    # Calculate the intersection of the x-coordinates\n    x_left = max(x0_1, x0_2)\n    x_right = min(x1_1, x1_2)\n\n    if x_right < x_left:\n        return 0.0\n\n    # Length of the intersection\n    intersection_length = x_right - x_left\n\n    # Length of the x-axis projection of the first block\n    block1_length = x1_1 - x0_1\n\n    if block1_length == 0:\n        return 0.0\n\n    # Proportion of the x-axis covered by the intersection\n    # logger.info(f\"intersection_length: {intersection_length}, block1_length: {block1_length}\")\n    return intersection_length / block1_length\n"
  },
  {
    "file_name": "magic_pdf/libs/clean_memory.py",
    "file_contents": "import torch\nimport gc\n\n\ndef clean_memory(device='cuda'):\n    if device == 'cuda':\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n    elif str(device).startswith(\"npu\"):\n        import torch_npu\n        if torch_npu.npu.is_available():\n            torch_npu.npu.empty_cache()\n    elif str(device).startswith(\"mps\"):\n        torch.mps.empty_cache()\n    gc.collect()"
  },
  {
    "file_name": "magic_pdf/libs/commons.py",
    "file_contents": "\ndef join_path(*args):\n    return '/'.join(str(s).rstrip('/') for s in args)\n\n\ndef get_top_percent_list(num_list, percent):\n    if len(num_list) == 0:\n        top_percent_list = []\n    else:\n\n        sorted_imgs_len_list = sorted(num_list, reverse=True)\n\n        top_percent_index = int(len(sorted_imgs_len_list) * percent)\n\n        top_percent_list = sorted_imgs_len_list[:top_percent_index]\n    return top_percent_list\n\n\ndef mymax(alist: list):\n    if len(alist) == 0:\n        return 0\n    else:\n        return max(alist)\n\n\ndef parse_bucket_key(s3_full_path: str):\n    s3_full_path = s3_full_path.strip()\n    if s3_full_path.startswith(\"s3://\"):\n        s3_full_path = s3_full_path[5:]\n    if s3_full_path.startswith(\"/\"):\n        s3_full_path = s3_full_path[1:]\n    bucket, key = s3_full_path.split(\"/\", 1)\n    return bucket, key\n"
  },
  {
    "file_name": "magic_pdf/libs/config_reader.py",
    "file_contents": "import os\n\nfrom loguru import logger\n\nfrom magic_pdf.libs.commons import parse_bucket_key\nimport yaml\n\n\nCONFIG_FILE_NAME = os.getenv('MONKEYOCR_MODEL_CONFIGS', 'model_configs.yaml')\n\ndef get_base_directory(path):\n    return os.path.dirname(os.path.dirname(os.path.dirname(path)))\n\ndef get_current_file_parent_parent_dir():\n    current_file = os.path.abspath(__file__)\n    return get_base_directory(current_file)\n\n\ndef read_config():\n    config_file = os.path.join(get_current_file_parent_parent_dir(), 'model_configs.yaml')\n\n    if not os.path.exists(config_file):\n        raise FileNotFoundError(f'{config_file} not found')\n\n    with open(config_file, 'r', encoding='utf-8') as f:\n        config = yaml.safe_load(f)\n    return config\n\n\ndef get_s3_config(bucket_name: str):\n    config = read_config()\n\n    bucket_info = config.get('bucket_info')\n    if bucket_name not in bucket_info:\n        access_key, secret_key, storage_endpoint = bucket_info['[default]']\n    else:\n        access_key, secret_key, storage_endpoint = bucket_info[bucket_name]\n\n    if access_key is None or secret_key is None or storage_endpoint is None:\n        raise Exception(f'ak, sk or endpoint not found in {CONFIG_FILE_NAME}')\n\n    # logger.info(f\"get_s3_config: ak={access_key}, sk={secret_key}, endpoint={storage_endpoint}\")\n\n    return access_key, secret_key, storage_endpoint\n\n\ndef get_s3_config_dict(path: str):\n    access_key, secret_key, storage_endpoint = get_s3_config(get_bucket_name(path))\n    return {'ak': access_key, 'sk': secret_key, 'endpoint': storage_endpoint}\n\n\ndef get_bucket_name(path):\n    bucket, key = parse_bucket_key(path)\n    return bucket\n\n\ndef get_local_models_dir():\n    config = read_config()\n    models_dir = config.get('models-dir')\n    if models_dir is None:\n        logger.warning(f\"'models-dir' not found in {CONFIG_FILE_NAME}, use '/tmp/models' as default\")\n        return '/tmp/models'\n    else:\n        return models_dir\n\n\ndef get_local_layoutreader_model_dir():\n    config = read_config()\n    layoutreader_model_dir = config.get('layoutreader-model-dir')\n    if layoutreader_model_dir is None or not os.path.exists(layoutreader_model_dir):\n        home_dir = os.path.expanduser('~')\n        layoutreader_at_modelscope_dir_path = os.path.join(home_dir, '.cache/modelscope/hub/ppaanngggg/layoutreader')\n        logger.warning(f\"'layoutreader-model-dir' not exists, use {layoutreader_at_modelscope_dir_path} as default\")\n        return layoutreader_at_modelscope_dir_path\n    else:\n        return layoutreader_model_dir\n\n\ndef get_device():\n    config = read_config()\n    device = config.get('device')\n    if device is None:\n        logger.warning(f\"'device' not found in {CONFIG_FILE_NAME}, use 'cpu' as default\")\n        return 'cpu'\n    else:\n        return device\n\n\nif __name__ == '__main__':\n    ak, sk, endpoint = get_s3_config('llm-raw')\n"
  },
  {
    "file_name": "magic_pdf/libs/convert_utils.py",
    "file_contents": "def dict_to_list(input_dict):\n    items_list = []\n    for _, item in input_dict.items():\n        items_list.append(item)\n    return items_list\n"
  },
  {
    "file_name": "magic_pdf/libs/coordinate_transform.py",
    "file_contents": "def get_scale_ratio(model_page_info, page):\n    pix = page.get_pixmap(dpi=72)\n    pymu_width = int(pix.w)\n    pymu_height = int(pix.h)\n    width_from_json = model_page_info['page_info']['width']\n    height_from_json = model_page_info['page_info']['height']\n    horizontal_scale_ratio = width_from_json / pymu_width\n    vertical_scale_ratio = height_from_json / pymu_height\n    return horizontal_scale_ratio, vertical_scale_ratio\n"
  },
  {
    "file_name": "magic_pdf/libs/draw_bbox.py",
    "file_contents": "import fitz\nfrom magic_pdf.config.constants import CROSS_PAGE\nfrom magic_pdf.config.ocr_content_type import (BlockType, CategoryId,\n                                               ContentType)\nfrom magic_pdf.data.dataset import Dataset\nfrom magic_pdf.model.magic_model import MagicModel\n\n\ndef draw_bbox_without_number(i, bbox_list, page, rgb_config, fill_config):\n    new_rgb = []\n    for item in rgb_config:\n        item = float(item) / 255\n        new_rgb.append(item)\n    page_data = bbox_list[i]\n    for bbox in page_data:\n        x0, y0, x1, y1 = bbox\n        rect_coords = fitz.Rect(x0, y0, x1, y1) * page.derotation_matrix  # Define the rectangle\n        if fill_config:\n            page.draw_rect(\n                rect_coords,\n                color=None,\n                fill=new_rgb,\n                fill_opacity=0.3,\n                width=0.5,\n                overlay=True,\n            )  # Draw the rectangle\n        else:\n            page.draw_rect(\n                rect_coords,\n                color=new_rgb,\n                fill=None,\n                fill_opacity=1,\n                width=0.5,\n                overlay=True,\n            )  # Draw the rectangle\n\n\ndef draw_bbox_with_number(i, bbox_list, page, rgb_config, fill_config, draw_bbox=True):\n    new_rgb = []\n    for item in rgb_config:\n        item = float(item) / 255\n        new_rgb.append(item)\n    page_data = bbox_list[i]\n    for j, bbox in enumerate(page_data):\n        x0, y0, x1, y1 = bbox\n        rect_coords = fitz.Rect(x0, y0, x1, y1) * page.derotation_matrix  # Define the rectangle\n        if draw_bbox:\n            if fill_config:\n                page.draw_rect(\n                    rect_coords,\n                    color=None,\n                    fill=new_rgb,\n                    fill_opacity=0.3,\n                    width=0.5,\n                    overlay=True,\n                )  # Draw the rectangle\n            else:\n                page.draw_rect(\n                    rect_coords,\n                    color=new_rgb,\n                    fill=None,\n                    fill_opacity=1,\n                    width=0.5,\n                    overlay=True,\n                )  # Draw the rectangle\n        page.insert_text(\n            (rect_coords.x1 + 2, rect_coords.y0 + 10), str(j + 1), fontsize=10, color=new_rgb, rotate=page.rotation,\n        )  # Insert the index in the top left corner of the rectangle\n\n\ndef draw_layout_bbox(pdf_info, pdf_bytes, out_path, filename):\n    dropped_bbox_list = []\n    tables_list, tables_body_list = [], []\n    tables_caption_list, tables_footnote_list = [], []\n    imgs_list, imgs_body_list, imgs_caption_list = [], [], []\n    imgs_footnote_list = []\n    titles_list = []\n    texts_list = []\n    interequations_list = []\n    lists_list = []\n    indexs_list = []\n    for page in pdf_info:\n\n        page_dropped_list = []\n        tables, tables_body, tables_caption, tables_footnote = [], [], [], []\n        imgs, imgs_body, imgs_caption, imgs_footnote = [], [], [], []\n        titles = []\n        texts = []\n        interequations = []\n        lists = []\n        indices = []\n\n        for dropped_bbox in page['discarded_blocks']:\n            page_dropped_list.append(dropped_bbox['bbox'])\n        dropped_bbox_list.append(page_dropped_list)\n        for block in page['para_blocks']:\n            bbox = block['bbox']\n            if block['type'] == BlockType.Table:\n                tables.append(bbox)\n                for nested_block in block['blocks']:\n                    bbox = nested_block['bbox']\n                    if nested_block['type'] == BlockType.TableBody:\n                        tables_body.append(bbox)\n                    elif nested_block['type'] == BlockType.TableCaption:\n                        tables_caption.append(bbox)\n                    elif nested_block['type'] == BlockType.TableFootnote:\n                        tables_footnote.append(bbox)\n            elif block['type'] == BlockType.Image:\n                imgs.append(bbox)\n                for nested_block in block['blocks']:\n                    bbox = nested_block['bbox']\n                    if nested_block['type'] == BlockType.ImageBody:\n                        imgs_body.append(bbox)\n                    elif nested_block['type'] == BlockType.ImageCaption:\n                        imgs_caption.append(bbox)\n                    elif nested_block['type'] == BlockType.ImageFootnote:\n                        imgs_footnote.append(bbox)\n            elif block['type'] == BlockType.Title:\n                titles.append(bbox)\n            elif block['type'] == BlockType.Text:\n                texts.append(bbox)\n            elif block['type'] == BlockType.InterlineEquation:\n                interequations.append(bbox)\n            elif block['type'] == BlockType.List:\n                lists.append(bbox)\n            elif block['type'] == BlockType.Index:\n                indices.append(bbox)\n\n        tables_list.append(tables)\n        tables_body_list.append(tables_body)\n        tables_caption_list.append(tables_caption)\n        tables_footnote_list.append(tables_footnote)\n        imgs_list.append(imgs)\n        imgs_body_list.append(imgs_body)\n        imgs_caption_list.append(imgs_caption)\n        imgs_footnote_list.append(imgs_footnote)\n        titles_list.append(titles)\n        texts_list.append(texts)\n        interequations_list.append(interequations)\n        lists_list.append(lists)\n        indexs_list.append(indices)\n\n    layout_bbox_list = []\n\n    table_type_order = {\n        'table_caption': 1,\n        'table_body': 2,\n        'table_footnote': 3\n    }\n    for page in pdf_info:\n        page_block_list = []\n        for block in page['para_blocks']:\n            if block['type'] in [\n                BlockType.Text,\n                BlockType.Title,\n                BlockType.InterlineEquation,\n                BlockType.List,\n                BlockType.Index,\n            ]:\n                bbox = block['bbox']\n                page_block_list.append(bbox)\n            elif block['type'] in [BlockType.Image]:\n                for sub_block in block['blocks']:\n                    bbox = sub_block['bbox']\n                    page_block_list.append(bbox)\n            elif block['type'] in [BlockType.Table]:\n                sorted_blocks = sorted(block['blocks'], key=lambda x: table_type_order[x['type']])\n                for sub_block in sorted_blocks:\n                    bbox = sub_block['bbox']\n                    page_block_list.append(bbox)\n\n        layout_bbox_list.append(page_block_list)\n\n    pdf_docs = fitz.open('pdf', pdf_bytes)\n\n    for i, page in enumerate(pdf_docs):\n\n        draw_bbox_without_number(i, dropped_bbox_list, page, [158, 158, 158], True)\n        # draw_bbox_without_number(i, tables_list, page, [153, 153, 0], True)  # color !\n        draw_bbox_without_number(i, tables_body_list, page, [204, 204, 0], True)\n        draw_bbox_without_number(i, tables_caption_list, page, [255, 255, 102], True)\n        draw_bbox_without_number(i, tables_footnote_list, page, [229, 255, 204], True)\n        # draw_bbox_without_number(i, imgs_list, page, [51, 102, 0], True)\n        draw_bbox_without_number(i, imgs_body_list, page, [153, 255, 51], True)\n        draw_bbox_without_number(i, imgs_caption_list, page, [102, 178, 255], True)\n        draw_bbox_without_number(i, imgs_footnote_list, page, [255, 178, 102], True),\n        draw_bbox_without_number(i, titles_list, page, [102, 102, 255], True)\n        draw_bbox_without_number(i, texts_list, page, [153, 0, 76], True)\n        draw_bbox_without_number(i, interequations_list, page, [0, 255, 0], True)\n        draw_bbox_without_number(i, lists_list, page, [40, 169, 92], True)\n        draw_bbox_without_number(i, indexs_list, page, [40, 169, 92], True)\n\n        draw_bbox_with_number(\n            i, layout_bbox_list, page, [255, 0, 0], False, draw_bbox=False\n        )\n\n    # Save the PDF\n    pdf_docs.save(f'{out_path}/{filename}')\n\n\ndef draw_span_bbox(pdf_info, pdf_bytes, out_path, filename):\n    text_list = []\n    inline_equation_list = []\n    interline_equation_list = []\n    image_list = []\n    table_list = []\n    dropped_list = []\n    next_page_text_list = []\n    next_page_inline_equation_list = []\n\n    def get_span_info(span):\n        if span['type'] == ContentType.Text:\n            if span.get(CROSS_PAGE, False):\n                next_page_text_list.append(span['bbox'])\n            else:\n                page_text_list.append(span['bbox'])\n        elif span['type'] == ContentType.InlineEquation:\n            if span.get(CROSS_PAGE, False):\n                next_page_inline_equation_list.append(span['bbox'])\n            else:\n                page_inline_equation_list.append(span['bbox'])\n        elif span['type'] == ContentType.InterlineEquation:\n            page_interline_equation_list.append(span['bbox'])\n        elif span['type'] == ContentType.Image:\n            page_image_list.append(span['bbox'])\n        elif span['type'] == ContentType.Table:\n            page_table_list.append(span['bbox'])\n\n    for page in pdf_info:\n        page_text_list = []\n        page_inline_equation_list = []\n        page_interline_equation_list = []\n        page_image_list = []\n        page_table_list = []\n        page_dropped_list = []\n\n\n        if len(next_page_text_list) > 0:\n            page_text_list.extend(next_page_text_list)\n            next_page_text_list.clear()\n        if len(next_page_inline_equation_list) > 0:\n            page_inline_equation_list.extend(next_page_inline_equation_list)\n            next_page_inline_equation_list.clear()\n\n\n        for block in page['discarded_blocks']:\n            if block['type'] == BlockType.Discarded:\n                for line in block['lines']:\n                    for span in line['spans']:\n                        page_dropped_list.append(span['bbox'])\n        dropped_list.append(page_dropped_list)\n\n\n        for block in page['preproc_blocks']:\n            if block['type'] in [\n                BlockType.Text,\n                BlockType.Title,\n                BlockType.InterlineEquation,\n                BlockType.List,\n                BlockType.Index,\n            ]:\n                for line in block['lines']:\n                    for span in line['spans']:\n                        get_span_info(span)\n            elif block['type'] in [BlockType.Image, BlockType.Table]:\n                for sub_block in block['blocks']:\n                    for line in sub_block['lines']:\n                        for span in line['spans']:\n                            get_span_info(span)\n        text_list.append(page_text_list)\n        inline_equation_list.append(page_inline_equation_list)\n        interline_equation_list.append(page_interline_equation_list)\n        image_list.append(page_image_list)\n        table_list.append(page_table_list)\n    pdf_docs = fitz.open('pdf', pdf_bytes)\n    for i, page in enumerate(pdf_docs):\n\n        draw_bbox_without_number(i, text_list, page, [255, 0, 0], False)\n        draw_bbox_without_number(i, inline_equation_list, page, [0, 255, 0], False)\n        draw_bbox_without_number(i, interline_equation_list, page, [0, 0, 255], False)\n        draw_bbox_without_number(i, image_list, page, [255, 204, 0], False)\n        draw_bbox_without_number(i, table_list, page, [204, 0, 255], False)\n        draw_bbox_without_number(i, dropped_list, page, [158, 158, 158], False)\n\n    # Save the PDF\n    pdf_docs.save(f'{out_path}/{filename}')\n\n\ndef draw_model_bbox(model_list, dataset: Dataset, out_path, filename):\n    dropped_bbox_list = []\n    tables_body_list, tables_caption_list, tables_footnote_list = [], [], []\n    imgs_body_list, imgs_caption_list, imgs_footnote_list = [], [], []\n    titles_list = []\n    texts_list = []\n    interequations_list = []\n    magic_model = MagicModel(model_list, dataset)\n    for i in range(len(model_list)):\n        page_dropped_list = []\n        tables_body, tables_caption, tables_footnote = [], [], []\n        imgs_body, imgs_caption, imgs_footnote = [], [], []\n        titles = []\n        texts = []\n        interequations = []\n        page_info = magic_model.get_model_list(i)\n        layout_dets = page_info['layout_dets']\n        for layout_det in layout_dets:\n            bbox = layout_det['bbox']\n            if layout_det['category_id'] == CategoryId.Text:\n                texts.append(bbox)\n            elif layout_det['category_id'] == CategoryId.Title:\n                titles.append(bbox)\n            elif layout_det['category_id'] == CategoryId.TableBody:\n                tables_body.append(bbox)\n            elif layout_det['category_id'] == CategoryId.TableCaption:\n                tables_caption.append(bbox)\n            elif layout_det['category_id'] == CategoryId.TableFootnote:\n                tables_footnote.append(bbox)\n            elif layout_det['category_id'] == CategoryId.ImageBody:\n                imgs_body.append(bbox)\n            elif layout_det['category_id'] == CategoryId.ImageCaption:\n                imgs_caption.append(bbox)\n            elif layout_det['category_id'] == CategoryId.InterlineEquation_YOLO:\n                interequations.append(bbox)\n            elif layout_det['category_id'] == CategoryId.Abandon:\n                page_dropped_list.append(bbox)\n            elif layout_det['category_id'] == CategoryId.ImageFootnote:\n                imgs_footnote.append(bbox)\n\n        tables_body_list.append(tables_body)\n        tables_caption_list.append(tables_caption)\n        tables_footnote_list.append(tables_footnote)\n        imgs_body_list.append(imgs_body)\n        imgs_caption_list.append(imgs_caption)\n        titles_list.append(titles)\n        texts_list.append(texts)\n        interequations_list.append(interequations)\n        dropped_bbox_list.append(page_dropped_list)\n        imgs_footnote_list.append(imgs_footnote)\n\n    for i in range(len(dataset)):\n        page = dataset.get_page(i)\n        draw_bbox_with_number(\n            i, dropped_bbox_list, page, [158, 158, 158], True\n        )  # color !\n        draw_bbox_with_number(i, tables_body_list, page, [204, 204, 0], True)\n        draw_bbox_with_number(i, tables_caption_list, page, [255, 255, 102], True)\n        draw_bbox_with_number(i, tables_footnote_list, page, [229, 255, 204], True)\n        draw_bbox_with_number(i, imgs_body_list, page, [153, 255, 51], True)\n        draw_bbox_with_number(i, imgs_caption_list, page, [102, 178, 255], True)\n        draw_bbox_with_number(i, imgs_footnote_list, page, [255, 178, 102], True)\n        draw_bbox_with_number(i, titles_list, page, [102, 102, 255], True)\n        draw_bbox_with_number(i, texts_list, page, [153, 0, 76], True)\n        draw_bbox_with_number(i, interequations_list, page, [0, 255, 0], True)\n\n    # Save the PDF\n    dataset.dump_to_file(f'{out_path}/{filename}')\n\n\ndef draw_line_sort_bbox(pdf_info, pdf_bytes, out_path, filename):\n    layout_bbox_list = []\n\n    for page in pdf_info:\n        page_line_list = []\n        for block in page['preproc_blocks']:\n            if block['type'] in [BlockType.Text]:\n                for line in block['lines']:\n                    bbox = line['bbox']\n                    index = line['index']\n                    page_line_list.append({'index': index, 'bbox': bbox})\n            elif block['type'] in [BlockType.Title, BlockType.InterlineEquation]:\n                if 'virtual_lines' in block:\n                    if len(block['virtual_lines']) > 0 and block['virtual_lines'][0].get('index', None) is not None:\n                        for line in block['virtual_lines']:\n                            bbox = line['bbox']\n                            index = line['index']\n                            page_line_list.append({'index': index, 'bbox': bbox})\n                else:\n                    for line in block['lines']:\n                        bbox = line['bbox']\n                        index = line['index']\n                        page_line_list.append({'index': index, 'bbox': bbox})\n            elif block['type'] in [BlockType.Image, BlockType.Table]:\n                for sub_block in block['blocks']:\n                    if sub_block['type'] in [BlockType.ImageBody, BlockType.TableBody]:\n                        if len(sub_block['virtual_lines']) > 0 and sub_block['virtual_lines'][0].get('index', None) is not None:\n                            for line in sub_block['virtual_lines']:\n                                bbox = line['bbox']\n                                index = line['index']\n                                page_line_list.append({'index': index, 'bbox': bbox})\n                        else:\n                            for line in sub_block['lines']:\n                                bbox = line['bbox']\n                                index = line['index']\n                                page_line_list.append({'index': index, 'bbox': bbox})\n                    elif sub_block['type'] in [BlockType.ImageCaption, BlockType.TableCaption, BlockType.ImageFootnote, BlockType.TableFootnote]:\n                        for line in sub_block['lines']:\n                            bbox = line['bbox']\n                            index = line['index']\n                            page_line_list.append({'index': index, 'bbox': bbox})\n        sorted_bboxes = sorted(page_line_list, key=lambda x: x['index'])\n        layout_bbox_list.append(sorted_bbox['bbox'] for sorted_bbox in sorted_bboxes)\n    pdf_docs = fitz.open('pdf', pdf_bytes)\n    for i, page in enumerate(pdf_docs):\n        draw_bbox_with_number(i, layout_bbox_list, page, [255, 0, 0], False)\n\n    pdf_docs.save(f'{out_path}/{filename}')\n\n\ndef draw_char_bbox(pdf_bytes, out_path, filename):\n    pdf_docs = fitz.open('pdf', pdf_bytes)\n    for i, page in enumerate(pdf_docs):\n        for block in page.get_text('rawdict', flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_MEDIABOX_CLIP)['blocks']:\n            for line in block['lines']:\n                for span in line['spans']:\n                    for char in span['chars']:\n                        char_bbox = char['bbox']\n                        page.draw_rect(char_bbox, color=[1, 0, 0], fill=None, fill_opacity=1, width=0.3, overlay=True,)\n    pdf_docs.save(f'{out_path}/{filename}')\n"
  },
  {
    "file_name": "magic_pdf/libs/hash_utils.py",
    "file_contents": "import hashlib\n\n\ndef compute_md5(file_bytes):\n    hasher = hashlib.md5()\n    hasher.update(file_bytes)\n    return hasher.hexdigest().upper()\n\n\ndef compute_sha256(input_string):\n    hasher = hashlib.sha256()\n\n    input_bytes = input_string.encode('utf-8')\n    hasher.update(input_bytes)\n    return hasher.hexdigest()\n"
  },
  {
    "file_name": "magic_pdf/libs/json_compressor.py",
    "file_contents": "import json\nimport brotli\nimport base64\n\nclass JsonCompressor:\n\n    @staticmethod\n    def compress_json(data):\n        \"\"\"\n        Compress a json object and encode it with base64\n        \"\"\"\n        json_str = json.dumps(data)\n        json_bytes = json_str.encode('utf-8')\n        compressed = brotli.compress(json_bytes, quality=6)\n        compressed_str = base64.b64encode(compressed).decode('utf-8')  # convert bytes to string\n        return compressed_str\n\n    @staticmethod\n    def decompress_json(compressed_str):\n        \"\"\"\n        Decode the base64 string and decompress the json object\n        \"\"\"\n        compressed = base64.b64decode(compressed_str.encode('utf-8'))  # convert string to bytes\n        decompressed_bytes = brotli.decompress(compressed)\n        json_str = decompressed_bytes.decode('utf-8')\n        data = json.loads(json_str)\n        return data\n"
  },
  {
    "file_name": "magic_pdf/libs/language.py",
    "file_contents": "import os\nimport unicodedata\n\nif not os.getenv(\"FTLANG_CACHE\"):\n    current_file_path = os.path.abspath(__file__)\n    current_dir = os.path.dirname(current_file_path)\n    root_dir = os.path.dirname(current_dir)\n    ftlang_cache_dir = os.path.join(root_dir, 'resources', 'fasttext-langdetect')\n    os.environ[\"FTLANG_CACHE\"] = str(ftlang_cache_dir)\n    # print(os.getenv(\"FTLANG_CACHE\"))\n\nfrom fast_langdetect import detect_language\n\n\ndef remove_invalid_surrogates(text):\n\n    return ''.join(c for c in text if not (0xD800 <= ord(c) <= 0xDFFF))\n\n\ndef detect_lang(text: str) -> str:\n\n    if len(text) == 0:\n        return \"\"\n\n    text = text.replace(\"\\n\", \"\")\n    text = remove_invalid_surrogates(text)\n\n    # print(text)\n    try:\n        lang_upper = detect_language(text)\n    except:\n        html_no_ctrl_chars = ''.join([l for l in text if unicodedata.category(l)[0] not in ['C', ]])\n        lang_upper = detect_language(html_no_ctrl_chars)\n\n    try:\n        lang = lang_upper.lower()\n    except:\n        lang = \"\"\n    return lang"
  },
  {
    "file_name": "magic_pdf/libs/local_math.py",
    "file_contents": "def float_gt(a, b):\n    if 0.0001 >= abs(a -b):\n        return False\n    return a > b\n    \ndef float_equal(a, b):\n    if 0.0001 >= abs(a-b):\n        return True\n    return False"
  },
  {
    "file_name": "magic_pdf/libs/markdown_utils.py",
    "file_contents": "\ndef ocr_escape_special_markdown_char(content):\n    special_chars = [\"*\", \"`\", \"~\", \"$\"]\n    for char in special_chars:\n        content = content.replace(char, \"\\\\\" + char)\n\n    return content\n"
  },
  {
    "file_name": "magic_pdf/libs/path_utils.py",
    "file_contents": "\n\ndef remove_non_official_s3_args(s3path):\n    \"\"\"\n    example: s3://abc/xxxx.json?bytes=0,81350 ==> s3://abc/xxxx.json\n    \"\"\"\n    arr = s3path.split(\"?\")\n    return arr[0]\n\ndef parse_s3path(s3path: str):\n    # from s3pathlib import S3Path\n    # p = S3Path(remove_non_official_s3_args(s3path))\n    # return p.bucket, p.key\n    s3path = remove_non_official_s3_args(s3path).strip()\n    if s3path.startswith(('s3://', 's3a://')):\n        prefix, path = s3path.split('://', 1)\n        bucket_name, key = path.split('/', 1)\n        return bucket_name, key\n    elif s3path.startswith('/'):\n        raise ValueError(\"The provided path starts with '/'. This does not conform to a valid S3 path format.\")\n    else:\n        raise ValueError(\"Invalid S3 path format. Expected 's3://bucket-name/key' or 's3a://bucket-name/key'.\")\n\n\ndef parse_s3_range_params(s3path: str):\n    \"\"\"\n    example: s3://abc/xxxx.json?bytes=0,81350 ==> [0, 81350]\n    \"\"\"\n    arr = s3path.split(\"?bytes=\")\n    if len(arr) == 1:\n        return None\n    return arr[1].split(\",\")\n"
  },
  {
    "file_name": "magic_pdf/libs/pdf_check.py",
    "file_contents": "import fitz\nimport numpy as np\nfrom loguru import logger\nimport re\nfrom io import BytesIO\nfrom pdfminer.high_level import extract_text\n\n\ndef calculate_sample_count(total_page: int):\n    select_page_cnt = min(10, total_page)\n    return select_page_cnt\n\n\ndef extract_pages(src_pdf_bytes: bytes) -> fitz.Document:\n    pdf_docs = fitz.open(\"pdf\", src_pdf_bytes)\n    total_page = len(pdf_docs)\n    if total_page == 0:\n\n        logger.warning(\"PDF is empty, return empty document\")\n        return fitz.Document()\n    select_page_cnt = calculate_sample_count(total_page)\n\n    page_num = np.random.choice(total_page, select_page_cnt, replace=False)\n    sample_docs = fitz.Document()\n    try:\n        for index in page_num:\n            sample_docs.insert_pdf(pdf_docs, from_page=int(index), to_page=int(index))\n    except Exception as e:\n        logger.exception(e)\n    return sample_docs\n\n\ndef detect_invalid_chars(src_pdf_bytes: bytes) -> bool:\n    sample_docs = extract_pages(src_pdf_bytes)\n    sample_pdf_bytes = sample_docs.tobytes()\n    sample_pdf_file_like_object = BytesIO(sample_pdf_bytes)\n    text = extract_text(sample_pdf_file_like_object)\n    text = text.replace(\"\\n\", \"\")\n    # logger.info(text)\n    cid_pattern = re.compile(r'\\(cid:\\d+\\)')\n    matches = cid_pattern.findall(text)\n    cid_count = len(matches)\n    cid_len = sum(len(match) for match in matches)\n    text_len = len(text)\n    if text_len == 0:\n        cid_chars_radio = 0\n    else:\n        cid_chars_radio = cid_count/(cid_count + text_len - cid_len)\n    logger.info(f\"cid_count: {cid_count}, text_len: {text_len}, cid_chars_radio: {cid_chars_radio}\")\n    if cid_chars_radio > 0.05:\n        return False\n    else:\n        return True\n\n\ndef count_replacement_characters(text: str) -> int:\n    return text.count('\\ufffd')\n\n\ndef detect_invalid_chars_by_pymupdf(src_pdf_bytes: bytes) -> bool:\n    sample_docs = extract_pages(src_pdf_bytes)\n    doc_text = \"\"\n    for page in sample_docs:\n        page_text = page.get_text('text', flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_MEDIABOX_CLIP)\n        doc_text += page_text\n    text_len = len(doc_text)\n    uffd_count = count_replacement_characters(doc_text)\n    if text_len == 0:\n        uffd_chars_radio = 0\n    else:\n        uffd_chars_radio = uffd_count / text_len\n    logger.info(f\"uffd_count: {uffd_count}, text_len: {text_len}, uffd_chars_radio: {uffd_chars_radio}\")\n    if uffd_chars_radio > 0.01:\n        return False\n    else:\n        return True"
  },
  {
    "file_name": "magic_pdf/libs/pdf_image_tools.py",
    "file_contents": "from io import BytesIO\nimport cv2\nimport fitz\nimport numpy as np\nfrom PIL import Image\nfrom magic_pdf.data.data_reader_writer import DataWriter\nfrom magic_pdf.libs.commons import join_path\nfrom magic_pdf.libs.hash_utils import compute_sha256\n\n\ndef cut_image(bbox: tuple, page_num: int, page: fitz.Page, return_path, imageWriter: DataWriter):\n\n    filename = f'{page_num}_{int(bbox[0])}_{int(bbox[1])}_{int(bbox[2])}_{int(bbox[3])}'\n\n\n    img_path = join_path(return_path, filename) if return_path is not None else None\n\n\n    img_hash256_path = f'{compute_sha256(img_path)}.jpg'\n\n\n    rect = fitz.Rect(*bbox)\n\n    zoom = fitz.Matrix(3, 3)\n\n    pix = page.get_pixmap(clip=rect, matrix=zoom)\n\n    byte_data = pix.tobytes(output='jpeg', jpg_quality=95)\n\n    imageWriter.write(img_hash256_path, byte_data)\n\n    return img_hash256_path\n\n\ndef cut_image_to_pil_image(bbox: tuple, page: fitz.Page, mode=\"pillow\"):\n\n\n    rect = fitz.Rect(*bbox)\n\n    zoom = fitz.Matrix(3, 3)\n\n    pix = page.get_pixmap(clip=rect, matrix=zoom)\n\n\n    image_file = BytesIO(pix.tobytes(output='png'))\n\n    pil_image = Image.open(image_file)\n    if mode == \"cv2\":\n        image_result = cv2.cvtColor(np.asarray(pil_image), cv2.COLOR_RGB2BGR)\n    elif mode == \"pillow\":\n        image_result = pil_image\n    else:\n        raise ValueError(f\"mode: {mode} is not supported.\")\n\n    return image_result"
  },
  {
    "file_name": "magic_pdf/libs/safe_filename.py",
    "file_contents": "import os\n\n\ndef sanitize_filename(filename, replacement=\"_\"):\n    if os.name == 'nt':\n        invalid_chars = '<>:\"|?*'\n\n        for char in invalid_chars:\n            filename = filename.replace(char, replacement)\n\n    return filename\n"
  },
  {
    "file_name": "magic_pdf/libs/version.py",
    "file_contents": "__version__ = \"1.1.0\"\n"
  },
  {
    "file_name": "magic_pdf/model/__init__.py",
    "file_contents": "__use_inside_model__ = True\n__model_mode__ = 'full'"
  },
  {
    "file_name": "magic_pdf/model/async_vllm.py",
    "file_contents": "import os\nimport time\nimport asyncio\nfrom typing import List\nimport torch\nfrom loguru import logger\nfrom magic_pdf.utils.load_image import load_image\n\n\nclass MonkeyChat_vLLM_async:\n    def __init__(self, model_path, tp=1):\n        try:\n            from vllm import AsyncLLMEngine, SamplingParams\n            from vllm.engine.arg_utils import AsyncEngineArgs\n        except ImportError:\n            raise ImportError(\"vLLM is not installed. Please install it: \"\n                              \"https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda.md\")\n\n        self.model_name = os.path.basename(model_path)\n\n        engine_args = AsyncEngineArgs(\n            model=model_path,\n            max_seq_len_to_capture=10240,\n            mm_processor_kwargs={'use_fast': True},\n            gpu_memory_utilization=self._auto_gpu_mem_ratio(0.9),\n            disable_log_stats=True,\n            enable_prefix_caching=True,\n            tensor_parallel_size=tp\n        )\n\n        self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n        logger.info(f\"vLLM Async engine initialized: {self.model_name}\")\n\n        self.gen_config = SamplingParams(\n            max_tokens=4096,\n            temperature=0,\n            repetition_penalty=1.05,\n        )\n\n        self.request_timeout = 600\n    \n    def _auto_gpu_mem_ratio(self, ratio):\n        mem_free, mem_total = torch.cuda.mem_get_info()\n        ratio = ratio * mem_free / mem_total\n        return ratio\n\n    async def async_batch_inference(self, images: List[str], questions: List[str]) -> List[str]:\n        if len(images) != len(questions):\n            raise ValueError(\"Images and questions must have the same length\")\n\n        semaphore = asyncio.Semaphore(min(64, max(1, len(images))))\n        timeout_s = 300\n\n        async def infer_one(img_path: str, q: str, req_id: str) -> str:\n            placeholder = \"<|image_pad|>\"\n            prompt = (\n                \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n                f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n                f\"{q}<|im_end|>\\n\"\n                \"<|im_start|>assistant\\n\"\n            )\n\n            inputs = {\n                \"prompt\": prompt,\n                \"multi_modal_data\": {\n                    \"image\": [load_image(img_path, max_size=1600)],\n                }\n            }\n\n            start = time.time()\n            final_output = None\n            async for out in self.engine.generate(inputs, self.gen_config, req_id):\n                if time.time() - start > timeout_s:\n                    try:\n                        abort_res = self.engine.abort(req_id)\n                        if asyncio.iscoroutine(abort_res):\n                            await abort_res\n                        logger.info(f\"{req_id} aborted due to timeout\")\n                    except Exception as abort_err:\n                        logger.warning(f\"Abort failed for {req_id}: {abort_err}\")\n                    return \"Error: Request timed out\"\n                final_output = out\n                if getattr(out, \"finished\", False):\n                    break\n\n            if final_output and getattr(final_output, \"outputs\", None):\n                return final_output.outputs[0].text\n            return \"Error: No output generated\"\n\n        async def bounded(img: str, q: str, idx: int):\n            async with semaphore:\n                req_id = f\"batch_req_{idx}_{int(time.time()*1000)}\"\n                try:\n                    return await infer_one(img, q, req_id)\n                except Exception as e:\n                    logger.error(f\"Task {idx} failed: {e}\")\n                    return f\"Error: {str(e)}\"\n\n        tasks = [bounded(img, q, i) for i, (img, q) in enumerate(zip(images, questions))]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        out = []\n        for i, r in enumerate(results):\n            if isinstance(r, Exception):\n                logger.error(f\"Task {i} exception: {r}\")\n                out.append(f\"Error: {str(r)}\")\n            else:\n                out.append(r)\n        return out\n\n    def batch_inference(self, images: List[str], questions: List[str]) -> List[str]:\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            return asyncio.run(self.async_batch_inference(images, questions))\n\n        import concurrent.futures\n\n        def run_in_thread():\n            new_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(new_loop)\n            try:\n                return new_loop.run_until_complete(self.async_batch_inference(images, questions))\n            finally:\n                new_loop.close()\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:\n            fut = ex.submit(run_in_thread)\n            try:\n                return fut.result(timeout=self.request_timeout)\n            except Exception as e:\n                logger.error(f\"Synchronous batch inference failed: {e}\")\n                return [f\"Error: {str(e)}\"] * len(images)"
  },
  {
    "file_name": "magic_pdf/model/batch_analyze_llm.py",
    "file_contents": "import base64\nimport copy\nimport time\n\nfrom loguru import logger\n\nfrom magic_pdf.config.constants import MODEL_NAME\nfrom io import BytesIO\nfrom PIL import Image\nfrom magic_pdf.model.sub_modules.model_utils import (\n    clean_vram, crop_img)\n\nYOLO_LAYOUT_BASE_BATCH_SIZE = 8\n\nclass BatchAnalyzeLLM:\n    def __init__(self, model):\n        self.model = model\n\n    def __call__(self, images: list, split_pages: bool = False, pred_abandon: bool = False) -> list:\n        images_layout_res = []\n\n        layout_start_time = time.time()\n        if self.model.layout_model_name == MODEL_NAME.DocLayout_YOLO:\n            # doclayout_yolo\n            layout_images = []\n            for image_index, image in enumerate(images):\n                pil_img = Image.fromarray(image)\n                layout_images.append(pil_img)\n\n            images_layout_res += self.model.layout_model.batch_predict(\n                # layout_images, self.batch_ratio * YOLO_LAYOUT_BASE_BATCH_SIZE\n                layout_images, YOLO_LAYOUT_BASE_BATCH_SIZE\n            )\n                            \n        elif self.model.layout_model_name == MODEL_NAME.PaddleXLayoutModel:\n            # PP-DocLayout_plus-L\n            paddlex_layout_images = []\n            for image_index, image in enumerate(images):\n                pil_img = Image.fromarray(image)\n                paddlex_layout_images.append(pil_img)\n            layout_results = self.model.layout_model.batch_predict(\n                paddlex_layout_images, YOLO_LAYOUT_BASE_BATCH_SIZE \n            )\n            \n            images_layout_res += layout_results\n        else: \n            logger.error(f\"Unsupported layout model name: {self.model.layout_model_name}\")\n            raise ValueError(f\"Unsupported layout model name: {self.model.layout_model_name}\")\n\n        logger.info(\n            f'layout time: {round(time.time() - layout_start_time, 2)}, image num: {len(images)}'\n        )\n\n        if pred_abandon:\n            for index in range(len(images)):\n                layout_res = images_layout_res[index]\n                for res in layout_res:\n                    if res['category_id'] == 2:\n                        res['category_id'] = 1\n\n        clean_vram(self.model.device, vram_threshold=8)\n\n        lmm_ocr_start = time.time()\n        logger.info('LMM OCR start (done/total text blocks):')\n        # Check if split_pages is True and handle pages without valid cids\n        if split_pages or len(images) == 1:\n            cid2instruction = [0, 1, 4, 5, 6, 7, 8, 14, 101]\n            \n            pages_to_process_directly = []\n            for index in range(len(images)):\n                layout_res = images_layout_res[index]\n                # Check if this page has any valid cids\n                has_valid_cid = any(res['category_id'] in cid2instruction for res in layout_res)\n                \n                if not has_valid_cid:\n                    pages_to_process_directly.append(index)\n                    logger.info(f'Page {index} has no valid layout elements, will process directly')\n            \n            # Process pages without valid cids directly\n            if pages_to_process_directly:\n                direct_images = []\n                direct_messages = []\n                for page_idx in pages_to_process_directly:\n                    pil_img = Image.fromarray(images[page_idx])\n                    direct_images.append(pil_img)\n                    direct_messages.append(f'''Please output the text content from the image.''')\n                \n                # Get direct recognition results\n                direct_results = self.model.chat_model.batch_inference(direct_images, direct_messages)\n                \n                # Replace layout results for these pages\n                for i, page_idx in enumerate(pages_to_process_directly):\n                    # Create a single result covering the whole page\n                    height, width = images[page_idx].shape[:2]\n                    pre_res = {\n                        'category_id': 200,\n                        'score': 1.0,\n                        'poly': [0, 0, width, 0, width, height, 0, height]\n                    }\n                    single_res = {\n                        'category_id': 15,\n                        'score': 1.0,\n                        'text': direct_results[i],\n                        'poly': [0, 0, width, 0, width, height, 0, height]\n                    }\n                    images_layout_res[page_idx] = [pre_res, single_res]\n\n        new_images_all = []\n        cids_all = []\n        page_idxs = []\n        for index in range(len(images)):\n            layout_res = images_layout_res[index]\n            pil_img = Image.fromarray(images[index])\n            new_images = []\n            cids = []\n            for res in layout_res:\n                pad_size = 0 if res['category_id'] == 5 else 50\n                new_image, useful_list = crop_img(\n                    res, pil_img, crop_paste_x=pad_size, crop_paste_y=pad_size\n                )\n                new_images.append(new_image)\n                cids.append(res['category_id'])\n            new_images_all.extend(new_images)\n            cids_all.extend(cids)\n            page_idxs.append(len(new_images_all) - len(new_images))\n        ocr_result = self.batch_lmm_ocr(new_images_all, cids_all)\n        for index in range(len(images)):\n            ocr_results = []\n            layout_res = images_layout_res[index]\n            for i in range(len(layout_res)):\n                res = layout_res[i]\n                ocr = ocr_result[page_idxs[index]+i]\n                if res['category_id'] in [8, 14]:\n                    temp_res = copy.deepcopy(res)\n                    temp_res['category_id'] = 14\n                    temp_res['score'] = 1.0\n                    temp_res['latex'] = ocr\n                    ocr_results.append(temp_res)\n                elif res['category_id'] in [0, 1, 2, 4, 6, 7, 101]:\n                    temp_res = copy.deepcopy(res)\n                    temp_res['category_id'] = 15\n                    temp_res['score'] = 1.0\n                    temp_res['text'] = ocr\n                    ocr_results.append(temp_res)\n                elif res['category_id'] == 5:\n                    res['score'] = 1.0\n                    res['html'] = ocr\n                elif res['category_id'] == 15:\n                    # This is already a direct recognition result, keep it as is\n                    pass\n                elif res['category_id'] == 200:\n                    res['category_id'] = 1\n            layout_res.extend(ocr_results)\n        logger.info(\n            f'LMM ocr time: {round(time.time() - lmm_ocr_start, 2)}, image num: {len(images)}'\n        )\n\n        return images_layout_res\n\n    def batch_lmm_ocr(self, images, cat_ids, version='lmdeploy'):\n        def sanitize_md(output):\n            return output.replace('<md>', '').replace('</md>', '').replace('md\\n','').strip()\n        def sanitize_mf(output:str):\n            return output.replace('$$', '').strip('$').strip()\n        def sanitize_html(output):\n            output = output.replace('```html','').replace('```','').replace('<html>','').replace('</html>','').strip()\n            if not output.endswith('</table>'):\n                output += '</table>'\n            return output.strip()\n        assert len(images) == len(cat_ids)\n        instruction = f'''Please output the text content from the image.'''\n        instruction_mf = f'''Please write out the expression of the formula in the image using LaTeX format.'''\n        instruction_table = f'''This is the image of a table. Please output the table in html format.'''\n        cid2instruction = {\n            0: instruction,\n            1: instruction,\n            # 2: instruction,\n            4: instruction,\n            5: instruction_table,\n            6: instruction,\n            7: instruction,\n            8: instruction_mf,\n            # 9: instruction,\n            14: instruction_mf,\n            101: instruction,\n        }\n        new_images = []\n        messages = []\n        ignore_idx = []\n        outs = []\n        for i in range(len(images)):\n            if cat_ids[i] not in cid2instruction:\n                ignore_idx.append(i)\n                continue\n            new_images.append(images[i])\n            messages.append(cid2instruction[cat_ids[i]])\n        if len(new_images) == 0:\n            return [''] * len(images)\n        out = self.model.chat_model.batch_inference(new_images, messages)\n        outs.extend(out)\n        for j in ignore_idx:\n            outs.insert(j, '')\n        messages.clear()\n        ignore_idx.clear()\n        for j in range(len(outs)):\n            if cat_ids[j] in cid2instruction:\n                if cat_ids[j] == 5:\n                    outs[j] = sanitize_html(outs[j])\n                elif cat_ids[j] in [8, 14]:\n                    outs[j] = sanitize_mf(outs[j])\n                else:\n                    outs[j] = sanitize_md(outs[j])\n        return outs"
  },
  {
    "file_name": "magic_pdf/model/custom_model.py",
    "file_contents": "import os\nimport time\nimport torch\nfrom magic_pdf.config.constants import *\nfrom magic_pdf.model.sub_modules.model_init import AtomModelSingleton\nfrom magic_pdf.model.model_list import AtomicModel\nfrom magic_pdf.model.async_vllm import MonkeyChat_vLLM_async\nfrom magic_pdf.utils.load_image import load_image, encode_image_base64\nfrom transformers import LayoutLMv3ForTokenClassification\nfrom loguru import logger\nimport yaml\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import Image\nfrom typing import List, Union\nfrom openai import OpenAI\nimport asyncio\nimport uuid\n\n\nclass MonkeyOCR:\n    def __init__(self, config_path):\n        current_file_path = os.path.abspath(__file__)\n\n        current_dir = os.path.dirname(current_file_path)\n\n        root_dir = os.path.dirname(current_dir)\n\n        with open(config_path, 'r', encoding='utf-8') as f:\n            self.configs = yaml.load(f, Loader=yaml.FullLoader)\n        logger.info('using configs: {}'.format(self.configs))\n\n        self.device = self.configs.get('device', 'cpu')\n        logger.info('using device: {}'.format(self.device))\n\n        bf16_supported = False\n        if self.device.startswith(\"cuda\"):\n            bf16_supported = torch.cuda.is_bf16_supported()\n        elif self.device.startswith(\"mps\"):\n            bf16_supported = True\n        \n        models_dir = self.configs.get(\n            'models_dir', os.path.join(root_dir, 'model_weight')\n        )\n\n        logger.info('using models_dir: {}'.format(models_dir))\n        if not os.path.exists(models_dir):\n            raise FileNotFoundError(\n                f\"Model directory '{models_dir}' not found. \"\n                \"Please run 'python tools/download_model.py' to download the required models.\"\n            )\n        \n        self.layout_config = self.configs.get('layout_config')\n        self.layout_model_name = self.layout_config.get(\n            'model', MODEL_NAME.DocLayout_YOLO\n        )\n\n        atom_model_manager = AtomModelSingleton()\n        if self.layout_model_name == MODEL_NAME.DocLayout_YOLO:\n            layout_model_path = os.path.join(models_dir, self.configs['weights'][self.layout_model_name])\n            if not os.path.exists(layout_model_path):\n                raise FileNotFoundError(\n                    f\"Layout model file not found at '{layout_model_path}'. \"\n                    \"Please run 'python tools/download_model.py' to download the required models.\"\n                )\n            self.layout_model = atom_model_manager.get_atom_model(\n                atom_model_name=AtomicModel.Layout,\n                layout_model_name=MODEL_NAME.DocLayout_YOLO,\n                doclayout_yolo_weights=layout_model_path,\n                device=self.device,\n            )\n        elif self.layout_model_name == MODEL_NAME.PaddleXLayoutModel:\n            layout_model_path = None\n            if self.layout_model_name in self.configs['weights']:\n                layout_model_path = os.path.join(models_dir, self.configs['weights'][self.layout_model_name])\n                if not os.path.exists(layout_model_path):\n                    raise FileNotFoundError(\n                        f\"Layout model file not found at '{layout_model_path}'. \"\n                        \"Please run 'python tools/download_model.py' to download the required models.\"\n                    )\n            self.layout_model = atom_model_manager.get_atom_model(\n                atom_model_name=AtomicModel.Layout,\n                layout_model_name=MODEL_NAME.PaddleXLayoutModel,\n                paddlexlayout_model_dir=layout_model_path,\n                device=self.device,\n            )\n        logger.info(f'layout model loaded: {self.layout_model_name}')\n\n\n        layout_reader_config = self.layout_config.get('reader')\n        self.layout_reader_name = layout_reader_config.get('name')\n        if self.layout_reader_name == 'layoutreader':\n            layoutreader_model_dir = os.path.join(models_dir, self.configs['weights'][self.layout_reader_name])\n            if os.path.exists(layoutreader_model_dir):\n                model = LayoutLMv3ForTokenClassification.from_pretrained(\n                    layoutreader_model_dir\n                )\n            else:\n                raise FileNotFoundError(\n                    f\"Reading Order model file not found at '{layoutreader_model_dir}'. \"\n                    \"Please run 'python tools/download_model.py' to download the required models.\"\n                )\n\n            if bf16_supported:\n                model.to(self.device).eval().bfloat16()\n            else:\n                model.to(self.device).eval()\n        else:\n            logger.error('model name not allow')\n        self.layoutreader_model = model\n        logger.info(f'layoutreader model loaded: {self.layout_reader_name}')\n\n        self.chat_config = self.configs.get('chat_config', {})\n        chat_backend = self.chat_config.get('backend', 'lmdeploy')\n        chat_path = self.chat_config.get('weight_path', 'model_weight/Recognition')\n        if not os.path.exists(chat_path):\n            chat_path = os.path.join(models_dir, chat_path)\n            if not os.path.exists(chat_path):\n                raise FileNotFoundError(\n                    f\"Chat model file not found at '{chat_path}'. \"\n                    \"Please run 'python tools/download_model.py' to download the required models.\"\n                )\n        if chat_backend == 'lmdeploy':\n            logger.info('using backend: LMDeploy')\n            dp = self.chat_config.get('data_parallelism', 1)\n            tp = self.chat_config.get('model_parallelism', 1)\n            self.chat_model = MonkeyChat_LMDeploy(chat_path, dp=dp, tp=tp)\n        elif chat_backend == 'lmdeploy_queue':\n            logger.info('using backend: LMDeploy Queue')\n            dp = self.chat_config.get('data_parallelism', 1)\n            tp = self.chat_config.get('model_parallelism', 1)\n            queue_config = self.chat_config.get('queue_config', {})\n            self.chat_model = MonkeyChat_LMDeploy_queue(chat_path, dp=dp, tp=tp, **queue_config)\n        elif chat_backend == 'vllm':\n            logger.info('using backend: vLLM')\n            tp = self.chat_config.get('model_parallelism', 1)\n            self.chat_model = MonkeyChat_vLLM(chat_path, tp=tp)\n        elif chat_backend == 'vllm_queue':\n            logger.info('using backend: vLLM Queue')\n            tp = self.chat_config.get('model_parallelism', 1)\n            queue_config = self.chat_config.get('queue_config', {})\n            self.chat_model = MonkeyChat_vLLM_queue(chat_path, tp=tp, **queue_config)\n        elif chat_backend == 'vllm_async':\n            logger.info('using backend: vLLM Async')\n            tp = self.chat_config.get('model_parallelism', 1)\n            self.chat_model = MonkeyChat_vLLM_async(chat_path, tp=tp)\n        elif chat_backend == 'transformers':\n            logger.info('using backend: transformers')\n            batch_size = self.chat_config.get('batch_size', 5)\n            self.chat_model = MonkeyChat_transformers(chat_path, batch_size, device=self.device)\n        elif chat_backend == 'api':\n            logger.info('using backend: API')\n            api_config = self.configs.get('api_config', {})\n            if not api_config:\n                raise ValueError(\"API configuration is required for API backend.\")\n            self.chat_model = MonkeyChat_OpenAIAPI(\n                url=api_config.get('url'),\n                model_name=api_config.get('model_name'),\n                api_key=api_config.get('api_key', None)\n            )\n        else:\n            logger.warning('using backend: LMDeploy (default)')\n            self.chat_model = MonkeyChat_LMDeploy(chat_path)\n        logger.info(f'LMM loaded: {self.chat_model.model_name}')\n\nclass MonkeyChat_LMDeploy:\n    def __init__(self, model_path, dp=1, tp=1): \n        try:\n            from lmdeploy import pipeline, GenerationConfig, ChatTemplateConfig\n        except ImportError:\n            raise ImportError(\"LMDeploy is not installed. Please install it following: \"\n                              \"https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda_pp.md \"\n                              \"to use MonkeyChat_LMDeploy.\")\n        self.model_name = os.path.basename(model_path)\n        self.engine_config = self._auto_config_dtype(dp=dp, tp=tp)\n        self.pipe = pipeline(model_path,\n                             backend_config=self.engine_config,\n                             chat_template_config=ChatTemplateConfig('qwen2d5-vl'),\n                             log_level='ERROR')\n        self.gen_config=GenerationConfig(max_new_tokens=4096,do_sample=True,temperature=0,repetition_penalty=1.05)\n\n    def _auto_config_dtype(self, dp=1, tp=1):\n        from lmdeploy import PytorchEngineConfig\n        engine_config = PytorchEngineConfig(session_len=10240, dp=dp, tp=tp)\n        dtype = \"bfloat16\"\n        if torch.cuda.is_available():\n            device = torch.cuda.current_device()\n            capability = torch.cuda.get_device_capability(device)\n            sm_version = capability[0] * 10 + capability[1]  # e.g. sm75 = 7.5\n            \n            # use float16 if computing capability <= sm75 (7.5)\n            if sm_version <= 75:\n                dtype = \"float16\"\n        engine_config.dtype = dtype\n        return engine_config\n    \n    def batch_inference(self, images, questions):\n        inputs = [(question, load_image(image, max_size=1600)) for image, question in zip(images, questions)]\n        outputs = self.pipe(inputs, gen_config=self.gen_config, use_tqdm=True)\n        return [output.text for output in outputs]\n    \nclass MonkeyChat_vLLM:\n    def __init__(self, model_path, tp=1):\n        try:\n            from vllm import LLM, SamplingParams\n        except ImportError:\n            raise ImportError(\"vLLM is not installed. Please install it following: \"\n                              \"https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda_pp.md \"\n                               \"to use MonkeyChat_vLLM.\")\n        self.model_name = os.path.basename(model_path)\n        self.pipe = LLM(model=model_path,\n                        max_seq_len_to_capture=10240,\n                        mm_processor_kwargs={'use_fast': True},\n                        gpu_memory_utilization=self._auto_gpu_mem_ratio(0.9),\n                        tensor_parallel_size=tp)\n        self.gen_config = SamplingParams(max_tokens=4096,temperature=0,repetition_penalty=1.05)\n    \n    def _auto_gpu_mem_ratio(self, ratio):\n        mem_free, mem_total = torch.cuda.mem_get_info()\n        ratio = ratio * mem_free / mem_total\n        return ratio\n\n    def batch_inference(self, images, questions):\n        placeholder = \"<|image_pad|>\"\n        prompts = [\n            (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\") for question in questions\n        ]\n        inputs = [{\n            \"prompt\": prompts[i],\n            \"multi_modal_data\": {\n                \"image\": load_image(images[i], max_size=1600),\n            }\n        } for i in range(len(prompts))]\n        outputs = self.pipe.generate(inputs, sampling_params=self.gen_config)\n        return [o.outputs[0].text for o in outputs]\n\nclass MonkeyChat_transformers:\n    def __init__(self, model_path: str, max_batch_size: int = 10, max_new_tokens=4096, device: str = None):\n        try:\n            from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n        except ImportError:\n            raise ImportError(\"transformers is not installed. Please install it following: \"\n                              \"https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda_pp.md \"\n                              \"to use MonkeyChat_transformers.\")\n        self.model_name = os.path.basename(model_path)\n        self.max_batch_size = max_batch_size\n        self.max_new_tokens = max_new_tokens\n        \n        if device is None:\n            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        else:\n            self.device = device\n        \n        bf16_supported = False\n        if self.device.startswith(\"cuda\"):\n            bf16_supported = torch.cuda.is_bf16_supported()\n        elif self.device.startswith(\"mps\"):\n            bf16_supported = True\n            \n        logger.info(f\"Loading Qwen2.5VL model from: {model_path}\")\n        logger.info(f\"Using device: {self.device}\")\n        logger.info(f\"Max batch size: {self.max_batch_size}\")\n        \n        try:\n            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                        model_path,\n                        torch_dtype=torch.bfloat16 if bf16_supported else torch.float16,\n                        attn_implementation=\"flash_attention_2\" if self.device.startswith(\"cuda\") else 'sdpa',\n                        device_map=self.device,\n                    )\n                \n            self.processor = AutoProcessor.from_pretrained(\n                model_path,\n                trust_remote_code=True\n            )\n            self.processor.tokenizer.padding_side = \"left\"\n            \n            self.model.eval()\n            logger.info(\"Qwen2.5VL model loaded successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            raise e\n    \n    def prepare_messages(self, images: List[Union[str, Image.Image]], questions: List[str]) -> List[List[dict]]:\n        if len(images) != len(questions):\n            raise ValueError(\"Images and questions must have the same length\")\n        \n        all_messages = []\n        for image, question in zip(images, questions):\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"image\",\n                            \"image\": load_image(image, max_size=1600),\n                        },\n                        {\"type\": \"text\", \"text\": question},\n                    ],\n                }\n            ]\n            all_messages.append(messages)\n        \n        return all_messages\n    \n    def batch_inference(self, images: List[Union[str, Image.Image]], questions: List[str]) -> List[str]:\n        if len(images) != len(questions):\n            raise ValueError(\"Images and questions must have the same length\")\n        \n        results = []\n        total_items = len(images)\n        \n        for i in range(0, total_items, self.max_batch_size):\n            batch_end = min(i + self.max_batch_size, total_items)\n            batch_images = images[i:batch_end]\n            batch_questions = questions[i:batch_end]\n            \n            logger.info(f\"Processing batch {i//self.max_batch_size + 1}/{(total_items-1)//self.max_batch_size + 1} \"\n                       f\"(items {i+1}-{batch_end})\")\n            \n            try:\n                batch_results = self._process_batch(batch_images, batch_questions)\n                results.extend(batch_results)\n            except Exception as e:\n                logger.error(f\"Batch processing failed for items {i+1}-{batch_end}: {e}\")\n                logger.info(\"Falling back to single processing...\")\n                for img, q in zip(batch_images, batch_questions):\n                    try:\n                        single_result = self._process_single(img, q)\n                        results.append(single_result)\n                    except Exception as single_e:\n                        logger.error(f\"Single processing also failed: {single_e}\")\n                        results.append(f\"Error: {str(single_e)}\")\n            \n            if self.device == 'cuda':\n                torch.cuda.empty_cache()\n        \n        return results\n    \n    def _process_batch(self, batch_images: List[Union[str, Image.Image]], batch_questions: List[str]) -> List[str]:\n        all_messages = self.prepare_messages(batch_images, batch_questions)\n        \n        texts = []\n        image_inputs = []\n        \n        for messages in all_messages:\n            text = self.processor.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            texts.append(text)\n            \n            image_inputs.append(process_vision_info(messages)[0])\n        \n        inputs = self.processor(\n            text=texts,\n            images=image_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        ).to(self.device)\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **inputs,\n                max_new_tokens=self.max_new_tokens,\n                do_sample=True,\n                temperature=0.1,\n                repetition_penalty=1.05,\n                pad_token_id=self.processor.tokenizer.pad_token_id,\n            )\n        \n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        \n        output_texts = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n        \n        return [text.strip() for text in output_texts]\n    \n    def _process_single(self, image: Union[str, Image.Image], question: str) -> str:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"image\": image,\n                    },\n                    {\"type\": \"text\", \"text\": question},\n                ],\n            }\n        ]\n        \n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        \n        image_inputs, video_inputs = process_vision_info(messages)\n        \n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        ).to(self.device)\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                **inputs,\n                max_new_tokens=1024,\n                do_sample=True,\n                temperature=0.1,\n                repetition_penalty=1.05,\n            )\n        \n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        \n        output_text = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n        \n        return output_text.strip()\n    \n    def single_inference(self, image: Union[str, Image.Image], question: str) -> str:\n        return self._process_single(image, question)\n    \nclass MonkeyChat_OpenAIAPI:\n    def __init__(self, url: str, model_name: str, api_key: str = None):\n        self.model_name = model_name\n        self.client = OpenAI(\n            api_key=api_key,\n            base_url=url\n        )\n        if not self.validate_connection():\n            raise ValueError(\"Invalid API URL or API key. Please check your configuration.\")\n\n    def validate_connection(self) -> bool:\n        \"\"\"\n        Validate the effectiveness of API URL and key\n        \"\"\"\n        try:\n            # Try to get model list to validate connection\n            response = self.client.models.list()\n            logger.info(\"API connection validation successful\")\n            return True\n        except Exception as e:\n            logger.error(f\"API connection validation failed: {e}\")\n            return False\n    \n    def img2base64(self, image: Union[str, Image.Image]) -> tuple[str, str]:\n        if hasattr(image, 'format') and image.format:\n            img_format = image.format\n        else:\n            # Default to PNG if format is not specified\n            img_format = \"PNG\"\n        image = encode_image_base64(image)\n        return image, img_format.lower()\n\n    def batch_inference(self, images: List[Union[str, Image.Image]], questions: List[str]) -> List[str]:\n        results = []\n        for image, question in zip(images, questions):\n            try:\n                # Load and resize image\n                image = load_image(image, max_size=1600)\n                img, img_type = self.img2base64(image)\n\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"input_image\",\n                            \"image_url\": f\"data:image/{img_type};base64,{img}\"\n                        },\n                        {\n                            \"type\": \"input_text\", \n                            \"text\": question\n                        }\n                    ],\n                }]\n                response = self.client.chat.completions.create(\n                    model=self.model_name,\n                    messages=messages\n                )\n                results.append(response.choices[0].message.content)\n            except Exception as e:\n                results.append(f\"Error: {e}\")\n        return results\n\nclass MonkeyChat_LMDeploy_queue:\n    \"\"\"\n    Hybrid architecture: Combines synchronous batch processing with asynchronous concurrency for LMDeploy\n    Designed for multi-user large-batch concurrent inference scenarios using LMDeploy backend\n    \n    Features:\n    1. Uses request queue to collect requests from multiple users\n    2. Dynamic batch merging to maximize GPU utilization\n    3. Supports multi-user concurrency, each user can submit large batch tasks\n    4. Achieves inference speed close to MonkeyChat_LMDeploy\n    5. Uses LMDeploy's efficient pipeline for batch processing\n    \"\"\"\n    \n    def __init__(self, model_path, dp=1, tp=1, max_batch_size=32, queue_timeout=0.1, max_queue_size=1000):\n        try:\n            from lmdeploy import pipeline, GenerationConfig, ChatTemplateConfig\n            import asyncio\n            import threading\n            from collections import deque\n        except ImportError:\n            raise ImportError(\"LMDeploy is not installed. Please install it following: \"\n                              \"https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda_pp.md\")\n        \n        self.model_name = os.path.basename(model_path)\n        self.max_batch_size = max_batch_size\n        self.queue_timeout = queue_timeout\n        self.max_queue_size = max_queue_size\n        \n        # Clear GPU memory before initialization\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        # Initialize LMDeploy pipeline (for efficient batch processing)\n        self.engine_config = self._auto_config_dtype(dp=dp, tp=tp)\n        self.pipe = pipeline(model_path,\n                             backend_config=self.engine_config,\n                             chat_template_config=ChatTemplateConfig('qwen2d5-vl'),\n                             log_level='ERROR')\n        \n        self.gen_config = GenerationConfig(\n            max_new_tokens=4096,\n            do_sample=True,\n            temperature=0,\n            repetition_penalty=1.05\n        )\n        \n        # Request queue and processing related\n        self.request_queue = deque()\n        self.result_futures = {}\n        self.queue_lock = threading.Lock()\n        self.processing = False\n        self.shutdown_flag = False\n        \n        # Start background processing thread\n        self.processor_thread = threading.Thread(target=self._background_processor, daemon=True)\n        self.processor_thread.start()\n        \n        logger.info(f\"LMDeploy MultiUser engine initialized for model: {self.model_name}\")\n        logger.info(f\"Max batch size: {max_batch_size}, Queue timeout: {queue_timeout}s\")\n    \n    def _auto_config_dtype(self, dp=1, tp=1):\n        \"\"\"Auto configure dtype based on GPU capability\"\"\"\n        from lmdeploy import PytorchEngineConfig\n        engine_config = PytorchEngineConfig(session_len=10240, dp=dp, tp=tp)\n        dtype = \"bfloat16\"\n        if torch.cuda.is_available():\n            device = torch.cuda.current_device()\n            capability = torch.cuda.get_device_capability(device)\n            sm_version = capability[0] * 10 + capability[1]  # e.g. sm75 = 7.5\n            \n            # use float16 if computing capability <= sm75 (7.5)\n            if sm_version <= 75:\n                dtype = \"float16\"\n        engine_config.dtype = dtype\n        return engine_config\n    \n    def _background_processor(self):\n        \"\"\"Background thread: continuously process request queue\"\"\"\n        import time\n        \n        while not self.shutdown_flag:\n            try:\n                # Collect a batch of requests\n                batch_requests = self._collect_batch_requests()\n                \n                if batch_requests:\n                    # Process batch requests\n                    self._process_batch_requests(batch_requests)\n                else:\n                    # Sleep briefly when no requests\n                    time.sleep(0.01)\n                    \n            except Exception as e:\n                logger.error(f\"Background processor error: {e}\")\n                time.sleep(0.1)\n    \n    def _collect_batch_requests(self):\n        \"\"\"Collect a batch of requests, supports dynamic batch size\"\"\"\n        import time\n        \n        batch_requests = []\n        start_time = time.time()\n        \n        with self.queue_lock:\n            # Collect requests until batch size reached or timeout\n            while (len(batch_requests) < self.max_batch_size and \n                   time.time() - start_time < self.queue_timeout and\n                   self.request_queue):\n                \n                request = self.request_queue.popleft()\n                batch_requests.append(request)\n        \n        return batch_requests\n    \n    def _process_batch_requests(self, batch_requests):\n        \"\"\"Process batch requests using LMDeploy pipeline\"\"\"\n        try:\n            # Prepare batch inputs for LMDeploy\n            inputs = []\n            request_ids = []\n            \n            for request in batch_requests:\n                request_id, image_path, question, future = request\n                \n                # Load image and prepare input tuple for LMDeploy\n                image = load_image(image_path, max_size=1600)\n                inputs.append((question, image))\n                request_ids.append(request_id)\n            \n            # Batch inference using LMDeploy pipeline\n            start_time = time.time()\n            outputs = self.pipe(inputs, gen_config=self.gen_config)\n            processing_time = time.time() - start_time\n            \n            logger.info(f\"Processed batch of {len(batch_requests)} requests in {processing_time:.2f}s \"\n                       f\"({len(batch_requests)/processing_time:.1f} req/s)\")\n            \n            # Distribute results to corresponding futures\n            for i, output in enumerate(outputs):\n                request_id = request_ids[i]\n                result_text = output.text\n                \n                # Get corresponding future from batch_requests and set result\n                request = batch_requests[i]\n                _, _, _, future = request\n                \n                try:\n                    if not future.done():\n                        # Need to set future result in correct event loop\n                        if hasattr(future, '_loop') and future._loop is not None:\n                            future._loop.call_soon_threadsafe(future.set_result, result_text)\n                        else:\n                            future.set_result(result_text)\n                    \n                    # Clean from dictionary\n                    if request_id in self.result_futures:\n                        del self.result_futures[request_id]\n                        \n                except Exception as e:\n                    logger.error(f\"Failed to set result for request {request_id}: {e}\")\n                    # Try to set error result\n                    try:\n                        if not future.done():\n                            if hasattr(future, '_loop') and future._loop is not None:\n                                future._loop.call_soon_threadsafe(future.set_result, f\"Error: {str(e)}\")\n                            else:\n                                future.set_result(f\"Error: {str(e)}\")\n                    except Exception:\n                        pass\n                    \n        except Exception as e:\n            logger.error(f\"Batch processing failed: {e}\")\n            # Set all requests to error state\n            for i, request in enumerate(batch_requests):\n                request_id, _, _, future = request\n                \n                try:\n                    if not future.done():\n                        error_msg = f\"Error: {str(e)}\"\n                        if hasattr(future, '_loop') and future._loop is not None:\n                            future._loop.call_soon_threadsafe(future.set_result, error_msg)\n                        else:\n                            future.set_result(error_msg)\n                    \n                    # Clean from dictionary\n                    if request_id in self.result_futures:\n                        del self.result_futures[request_id]\n                        \n                except Exception as set_error:\n                    logger.error(f\"Failed to set error result for request {request_id}: {set_error}\")\n    \n    async def async_single_inference(self, image: str, question: str) -> str:\n        \"\"\"Asynchronous single inference\"\"\"\n        request_id = f\"lmdeploy_multiuser_{uuid.uuid4().hex[:8]}\"\n        \n        # Create future to receive result\n        loop = asyncio.get_event_loop()\n        future = loop.create_future()\n        \n        # Add request to queue\n        with self.queue_lock:\n            if len(self.request_queue) >= self.max_queue_size:\n                logger.warning(f\"Request queue full, rejecting request {request_id}\")\n                return \"Error: Request queue full\"\n            \n            self.request_queue.append((request_id, image, question, future))\n            self.result_futures[request_id] = future\n        \n        try:\n            # Wait for result with timeout\n            result = await future\n            return result\n        except asyncio.TimeoutError:\n            logger.error(f\"Request {request_id} timed out\")\n            # Clean up timed out request\n            with self.queue_lock:\n                if request_id in self.result_futures:\n                    del self.result_futures[request_id]\n            return \"Error: Request timeout\"\n        except asyncio.CancelledError:\n            logger.info(f\"Request {request_id} was cancelled\")\n            # Clean up cancelled request\n            with self.queue_lock:\n                if request_id in self.result_futures:\n                    del self.result_futures[request_id]\n            raise\n        except Exception as e:\n            logger.error(f\"Request {request_id} failed with exception: {e}\")\n            # Clean up failed request\n            with self.queue_lock:\n                if request_id in self.result_futures:\n                    del self.result_futures[request_id]\n            return f\"Error: {str(e)}\"\n    \n    def single_inference(self, image: str, question: str) -> str:\n        \"\"\"Synchronous single inference (wraps async method)\"\"\"\n        try:\n            try:\n                loop = asyncio.get_running_loop()\n                # Already in async context, use thread executor\n                import concurrent.futures\n                \n                def run_async_in_thread():\n                    new_loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(new_loop)\n                    try:\n                        return new_loop.run_until_complete(\n                            self.async_single_inference(image, question)\n                        )\n                    finally:\n                        new_loop.close()\n                \n                with concurrent.futures.ThreadPoolExecutor() as executor:\n                    future = executor.submit(run_async_in_thread)\n                    return future.result()\n                    \n            except RuntimeError:\n                # No running event loop\n                return asyncio.run(self.async_single_inference(image, question))\n                \n        except Exception as e:\n            logger.error(f\"Single inference failed: {e}\")\n            return f\"Error: {str(e)}\"\n    \n    async def async_batch_inference(self, images: List[str], questions: List[str]) -> List[str]:\n        \"\"\"Asynchronous batch inference (decompose large batches into multiple concurrent requests)\"\"\"\n        if len(images) != len(questions):\n            raise ValueError(\"Images and questions must have the same length\")\n        \n        # Create concurrent tasks\n        tasks = []\n        for image, question in zip(images, questions):\n            task = self.async_single_inference(image, question)\n            tasks.append(task)\n        \n        # Execute all tasks concurrently\n        logger.info(f\"Processing {len(tasks)} requests concurrently\")\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Handle exception results\n        processed_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                processed_results.append(f\"Error: {str(result)}\")\n            else:\n                processed_results.append(result)\n        \n        return processed_results\n    \n    def batch_inference(self, images: List[str], questions: List[str]) -> List[str]:\n        \"\"\"Synchronous batch inference\"\"\"\n        try:\n            try:\n                loop = asyncio.get_running_loop()\n                # Already in async context\n                import concurrent.futures\n                \n                def run_async_in_thread():\n                    new_loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(new_loop)\n                    try:\n                        return new_loop.run_until_complete(\n                            self.async_batch_inference(images, questions)\n                        )\n                    finally:\n                        new_loop.close()\n                \n                with concurrent.futures.ThreadPoolExecutor() as executor:\n                    future = executor.submit(run_async_in_thread)\n                    return future.result()\n                    \n            except RuntimeError:\n                # No running event loop\n                return asyncio.run(self.async_batch_inference(images, questions))\n                \n        except Exception as e:\n            logger.error(f\"Batch inference failed: {e}\")\n            return [f\"Error: {str(e)}\"] * len(images)\n    \n    def get_queue_status(self):\n        \"\"\"Get queue status (for monitoring)\"\"\"\n        with self.queue_lock:\n            return {\n                \"queue_size\": len(self.request_queue),\n                \"pending_results\": len(self.result_futures),\n                \"max_queue_size\": self.max_queue_size,\n                \"processing\": self.processing,\n                \"processor_thread_alive\": self.processor_thread.is_alive(),\n                \"shutdown_flag\": self.shutdown_flag\n            }\n    \n    def shutdown(self):\n        \"\"\"Shutdown service\"\"\"\n        self.shutdown_flag = True\n        \n        # Wait for background thread to finish\n        if self.processor_thread.is_alive():\n            self.processor_thread.join(timeout=5)\n        \n        # Clean up unfinished requests\n        with self.queue_lock:\n            for request_id, future in self.result_futures.items():\n                if not future.done():\n                    future.set_result(\"Error: Service shutdown\")\n            self.result_futures.clear()\n            self.request_queue.clear()\n        \n        # Clean up pipeline and GPU memory\n        try:\n            if hasattr(self, 'pipe') and self.pipe is not None:\n                del self.pipe\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n        except Exception as e:\n            logger.warning(f\"Error during cleanup: {e}\")\n        \n        logger.info(\"LMDeploy MultiUser engine shutdown completed\")\n    \n    def __del__(self):\n        \"\"\"Destructor\"\"\"\n        try:\n            self.shutdown()\n        except Exception:\n            pass\n\nclass MonkeyChat_vLLM_queue:\n    \"\"\"\n    Hybrid architecture: Combines synchronous batch processing with asynchronous concurrency\n    Designed for multi-user large-batch concurrent inference scenarios\n    \n    Features:\n    1. Uses request queue to collect requests from multiple users\n    2. Dynamic batch merging to maximize GPU utilization\n    3. Supports multi-user concurrency, each user can submit large batch tasks\n    4. Achieves inference speed close to MonkeyChat_vLLM\n    \"\"\"\n    \n    def __init__(self, model_path, tp=1, max_batch_size=64, queue_timeout=0.1, max_queue_size=1000):\n        try:\n            from vllm import LLM, SamplingParams\n            import threading\n            from collections import deque\n        except ImportError:\n            raise ImportError(\"vLLM is not installed. Please install it following: \"\n                              \"https://github.com/Yuliang-Liu/MonkeyOCR/blob/main/docs/install_cuda_pp.md\")\n        \n        self.model_name = os.path.basename(model_path)\n        self.max_batch_size = max_batch_size\n        self.queue_timeout = queue_timeout\n        self.max_queue_size = max_queue_size\n        \n        # Initialize synchronous vLLM engine (for efficient batch processing)\n        self.engine = LLM(\n            model=model_path,\n            max_seq_len_to_capture=10240,\n            mm_processor_kwargs={'use_fast': True},\n            gpu_memory_utilization=self._auto_gpu_mem_ratio(0.9),\n            max_num_seqs=max_batch_size * 2,  # Allow larger sequence numbers\n            tensor_parallel_size=tp\n        )\n        \n        self.gen_config = SamplingParams(\n            max_tokens=4096, \n            temperature=0, \n            repetition_penalty=1.05\n        )\n        \n        # Request queue and processing related\n        self.request_queue = deque()\n        self.result_futures = {}\n        self.queue_lock = threading.Lock()\n        self.processing = False\n        self.shutdown_flag = False\n        \n        # Start background processing thread\n        self.processor_thread = threading.Thread(target=self._background_processor, daemon=True)\n        self.processor_thread.start()\n        \n        logger.info(f\"vLLM MultiUser engine initialized for model: {self.model_name}\")\n        logger.info(f\"Max batch size: {max_batch_size}, Queue timeout: {queue_timeout}s\")\n    \n    def _auto_gpu_mem_ratio(self, ratio):\n        mem_free, mem_total = torch.cuda.mem_get_info()\n        ratio = ratio * mem_free / mem_total\n        return ratio\n    \n    def _background_processor(self):\n        \"\"\"Background thread: continuously process request queue\"\"\"\n        import time\n        \n        while not self.shutdown_flag:\n            try:\n                # Collect a batch of requests\n                batch_requests = self._collect_batch_requests()\n                \n                if batch_requests:\n                    # Process batch requests\n                    self._process_batch_requests(batch_requests)\n                else:\n                    # Sleep briefly when no requests\n                    time.sleep(0.01)\n                    \n            except Exception as e:\n                logger.error(f\"Background processor error: {e}\")\n                time.sleep(0.1)\n    \n    def _collect_batch_requests(self):\n        \"\"\"Collect a batch of requests, supports dynamic batch size\"\"\"\n        import time\n        \n        batch_requests = []\n        start_time = time.time()\n        \n        with self.queue_lock:\n            # Collect requests until batch size reached or timeout\n            while (len(batch_requests) < self.max_batch_size and \n                   time.time() - start_time < self.queue_timeout and\n                   self.request_queue):\n                \n                request = self.request_queue.popleft()\n                batch_requests.append(request)\n        \n        return batch_requests\n    \n    def _process_batch_requests(self, batch_requests):\n        \"\"\"Process batch requests using synchronous engine\"\"\"\n        try:\n            # Prepare batch inputs\n            placeholder = \"<|image_pad|>\"\n            inputs = []\n            request_ids = []\n            \n            for request in batch_requests:\n                request_id, image_path, question, future = request\n                \n                prompt = (\n                    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n                    f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n                    f\"{question}<|im_end|>\\n\"\n                    \"<|im_start|>assistant\\n\"\n                )\n                \n                inputs.append({\n                    \"prompt\": prompt,\n                    \"multi_modal_data\": {\n                        \"image\": load_image(image_path, max_size=1600),\n                    }\n                })\n                request_ids.append(request_id)\n            \n            # Batch inference (using high-efficiency batch processing of synchronous engine)\n            start_time = time.time()\n            outputs = self.engine.generate(inputs, sampling_params=self.gen_config)\n            processing_time = time.time() - start_time\n            \n            logger.info(f\"Processed batch of {len(batch_requests)} requests in {processing_time:.2f}s \"\n                       f\"({len(batch_requests)/processing_time:.1f} req/s)\")\n            \n            # Distribute results to corresponding futures\n            for i, output in enumerate(outputs):\n                request_id = request_ids[i]\n                result_text = output.outputs[0].text\n                \n                # Get corresponding future from batch_requests and set result\n                request = batch_requests[i]\n                _, _, _, future = request\n                \n                try:\n                    if not future.done():\n                        # Need to set future result in correct event loop\n                        if hasattr(future, '_loop') and future._loop is not None:\n                            future._loop.call_soon_threadsafe(future.set_result, result_text)\n                        else:\n                            future.set_result(result_text)\n                    \n                    # Clean from dictionary\n                    if request_id in self.result_futures:\n                        del self.result_futures[request_id]\n                        \n                except Exception as e:\n                    logger.error(f\"Failed to set result for request {request_id}: {e}\")\n                    # Try to set error result\n                    try:\n                        if not future.done():\n                            if hasattr(future, '_loop') and future._loop is not None:\n                                future._loop.call_soon_threadsafe(future.set_result, f\"Error: {str(e)}\")\n                            else:\n                                future.set_result(f\"Error: {str(e)}\")\n                    except Exception:\n                        pass\n                    \n        except Exception as e:\n            logger.error(f\"Batch processing failed: {e}\")\n            # Set all requests to error state\n            for i, request in enumerate(batch_requests):\n                request_id, _, _, future = request\n                \n                try:\n                    if not future.done():\n                        error_msg = f\"Error: {str(e)}\"\n                        if hasattr(future, '_loop') and future._loop is not None:\n                            future._loop.call_soon_threadsafe(future.set_result, error_msg)\n                        else:\n                            future.set_result(error_msg)\n                    \n                    # Clean from dictionary\n                    if request_id in self.result_futures:\n                        del self.result_futures[request_id]\n                        \n                except Exception as set_error:\n                    logger.error(f\"Failed to set error result for request {request_id}: {set_error}\")\n                \n    async def async_single_inference(self, image: str, question: str) -> str:\n        \"\"\"Asynchronous single inference\"\"\"\n        request_id = f\"vllm_multiuser_{uuid.uuid4().hex[:8]}\"\n        \n        # Create future to receive result\n        loop = asyncio.get_event_loop()\n        future = loop.create_future()\n        \n        # Add request to queue\n        with self.queue_lock:\n            if len(self.request_queue) >= self.max_queue_size:\n                logger.warning(f\"Request queue full, rejecting request {request_id}\")\n                return \"Error: Request queue full\"\n            \n            self.request_queue.append((request_id, image, question, future))\n            self.result_futures[request_id] = future\n        \n        try:\n            # Wait for result without timeout\n            result = await future\n            return result\n        except asyncio.TimeoutError:\n            logger.error(f\"Request {request_id} timed out\")\n            # Clean up timed out request\n            with self.queue_lock:\n                if request_id in self.result_futures:\n                    del self.result_futures[request_id]\n            return \"Error: Request timeout\"\n        except asyncio.CancelledError:\n            logger.info(f\"Request {request_id} was cancelled\")\n            # Clean up cancelled request\n            with self.queue_lock:\n                if request_id in self.result_futures:\n                    del self.result_futures[request_id]\n            raise\n        except Exception as e:\n            logger.error(f\"Request {request_id} failed with exception: {e}\")\n            # Clean up failed request\n            with self.queue_lock:\n                if request_id in self.result_futures:\n                    del self.result_futures[request_id]\n            return f\"Error: {str(e)}\"\n    \n    def single_inference(self, image: str, question: str) -> str:\n        \"\"\"Synchronous single inference (wraps async method)\"\"\"\n        try:\n            try:\n                loop = asyncio.get_running_loop()\n                # Already in async context, use thread executor\n                import concurrent.futures\n                \n                def run_async_in_thread():\n                    new_loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(new_loop)\n                    try:\n                        return new_loop.run_until_complete(\n                            self.async_single_inference(image, question)\n                        )\n                    finally:\n                        new_loop.close()\n                \n                with concurrent.futures.ThreadPoolExecutor() as executor:\n                    future = executor.submit(run_async_in_thread)\n                    return future.result()\n                    \n            except RuntimeError:\n                # No running event loop\n                return asyncio.run(self.async_single_inference(image, question))\n                \n        except Exception as e:\n            logger.error(f\"Single inference failed: {e}\")\n            return f\"Error: {str(e)}\"\n    \n    async def async_batch_inference(self, images: List[str], questions: List[str]) -> List[str]:\n        \"\"\"Asynchronous batch inference (decompose large batches into multiple concurrent requests)\"\"\"\n        if len(images) != len(questions):\n            raise ValueError(\"Images and questions must have the same length\")\n        \n        # Create concurrent tasks\n        tasks = []\n        for image, question in zip(images, questions):\n            task = self.async_single_inference(image, question)\n            tasks.append(task)\n        \n        # Execute all tasks concurrently\n        logger.info(f\"Processing {len(tasks)} requests concurrently\")\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Handle exception results\n        processed_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                processed_results.append(f\"Error: {str(result)}\")\n            else:\n                processed_results.append(result)\n        \n        return processed_results\n    \n    def batch_inference(self, images: List[str], questions: List[str]) -> List[str]:\n        \"\"\"Synchronous batch inference\"\"\"\n        try:\n            try:\n                loop = asyncio.get_running_loop()\n                # Already in async context\n                import concurrent.futures\n                \n                def run_async_in_thread():\n                    new_loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(new_loop)\n                    try:\n                        return new_loop.run_until_complete(\n                            self.async_batch_inference(images, questions)\n                        )\n                    finally:\n                        new_loop.close()\n                \n                with concurrent.futures.ThreadPoolExecutor() as executor:\n                    future = executor.submit(run_async_in_thread)\n                    return future.result()\n                    \n            except RuntimeError:\n                # No running event loop\n                return asyncio.run(self.async_batch_inference(images, questions))\n                \n        except Exception as e:\n            logger.error(f\"Batch inference failed: {e}\")\n            return [f\"Error: {str(e)}\"] * len(images)\n    \n    def get_queue_status(self):\n        \"\"\"Get queue status (for monitoring)\"\"\"\n        with self.queue_lock:\n            return {\n                \"queue_size\": len(self.request_queue),\n                \"pending_results\": len(self.result_futures),\n                \"max_queue_size\": self.max_queue_size,\n                \"processing\": self.processing,\n                \"processor_thread_alive\": self.processor_thread.is_alive(),\n                \"shutdown_flag\": self.shutdown_flag\n            }\n    \n    def shutdown(self):\n        \"\"\"Shutdown service\"\"\"\n        self.shutdown_flag = True\n        \n        # Wait for background thread to finish\n        if self.processor_thread.is_alive():\n            self.processor_thread.join(timeout=5)\n        \n        # Clean up unfinished requests\n        with self.queue_lock:\n            for request_id, future in self.result_futures.items():\n                if not future.done():\n                    future.set_result(\"Error: Service shutdown\")\n            self.result_futures.clear()\n            self.request_queue.clear()\n        \n        # Clean up engine and GPU memory\n        try:\n            if hasattr(self, 'engine') and self.engine is not None:\n                del self.engine\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n        except Exception as e:\n            logger.warning(f\"Error during cleanup: {e}\")\n        \n        logger.info(\"vLLM MultiUser engine shutdown completed\")\n    \n    def __del__(self):\n        \"\"\"Destructor\"\"\"\n        try:\n            self.shutdown()\n        except Exception:\n            pass\n"
  },
  {
    "file_name": "magic_pdf/model/doc_analyze_by_custom_model_llm.py",
    "file_contents": "import time\nfrom loguru import logger\nfrom magic_pdf.model.batch_analyze_llm import BatchAnalyzeLLM\nfrom magic_pdf.data.dataset import Dataset, MultiFileDataset\nfrom magic_pdf.libs.clean_memory import clean_memory\nfrom magic_pdf.operators.models_llm import InferenceResultLLM\nfrom magic_pdf.data.dataset import ImageDataset\nfrom io import BytesIO\nfrom PIL import Image\n\n\ndef doc_analyze_llm(\n    dataset: Dataset,\n    MonkeyOCR_model,\n    start_page_id=0,\n    end_page_id=None,\n    split_pages=False,\n    split_files=False,\n    pred_abandon=False,\n) -> InferenceResultLLM:\n\n    end_page_id = end_page_id if end_page_id else len(dataset) - 1\n\n    device = MonkeyOCR_model.device\n\n    batch_model = BatchAnalyzeLLM(model=MonkeyOCR_model)\n\n    model_json = []\n    doc_analyze_start = time.time()\n\n    image_dicts = []\n    images = []\n    for index in range(len(dataset)):\n        if start_page_id <= index <= end_page_id:\n            page_data = dataset.get_page(index)\n            img_dict = page_data.get_image()\n            image_dicts.append(img_dict)\n            images.append(img_dict['img'])\n    \n    logger.info(f'images load time: {round(time.time() - doc_analyze_start, 2)}')\n    analyze_result = batch_model(images, split_pages=split_pages or split_files, pred_abandon=pred_abandon)\n\n    # Handle MultiFileDataset with split_files\n    if split_files and isinstance(dataset, MultiFileDataset):\n        file_results = []\n        for file_index in range(len(dataset.file_info)):\n            file_info = dataset.file_info[file_index]\n            file_start_page = file_info['start_page']\n            file_end_page = file_info['end_page']\n            file_page_count = file_info['page_count']\n            \n            # Create file-specific dataset\n            file_dataset = dataset.export_file_as_dataset(file_index)\n            \n            # Collect results for this file\n            file_model_json = []\n            for page_idx in range(file_page_count):\n                global_page_idx = file_start_page + page_idx\n                if start_page_id <= global_page_idx <= end_page_id:\n                    result = analyze_result.pop(0)\n                else:\n                    result = []\n                \n                img_dict = image_dicts[global_page_idx]\n                page_width = img_dict['width']\n                page_height = img_dict['height']\n                \n                if split_pages:\n                    # For split_pages, create individual InferenceResultLLM for each page\n                    page_info = {'page_no': 0, 'height': page_height, 'width': page_width}\n                    page_dict = {'layout_dets': result, 'page_info': page_info}\n                    \n                    # For ImageDataset, we can reuse the file_dataset directly since it's already single-page\n                    if isinstance(file_dataset, ImageDataset) and file_page_count == 1:\n                        page_inference_result = InferenceResultLLM([page_dict], file_dataset)\n                    else:\n                        # For multi-page files (PDFs), convert page to bytes\n                        img_bytes = BytesIO()\n                        img = Image.fromarray(img_dict['img'])\n                        img.save(img_bytes, format='PNG')\n                        img_ds = ImageDataset(img_bytes.getvalue())\n                        page_inference_result = InferenceResultLLM([page_dict], img_ds)\n                    \n                    # Initialize file_results structure if needed\n                    if len(file_results) <= file_index:\n                        file_results.extend([[] for _ in range(file_index + 1 - len(file_results))])\n                    if not isinstance(file_results[file_index], list):\n                        file_results[file_index] = []\n                    file_results[file_index].append(page_inference_result)\n                else:\n                    # For file-level results, use relative page numbers starting from 0\n                    page_info = {'page_no': page_idx, 'height': page_height, 'width': page_width}\n                    page_dict = {'layout_dets': result, 'page_info': page_info}\n                    file_model_json.append(page_dict)\n            \n            if not split_pages:\n                # Create one InferenceResultLLM per file\n                file_inference_result = InferenceResultLLM(file_model_json, file_dataset)\n                file_results.append(file_inference_result)\n        \n        inference_results = file_results\n    else:\n        # Original logic for non-split_files cases\n        inference_results = []\n        for index in range(len(dataset)):\n            img_dict = image_dicts[index]\n            page_width = img_dict['width']\n            page_height = img_dict['height']\n            if start_page_id <= index <= end_page_id:\n                result = analyze_result.pop(0)\n            else:\n                result = []\n\n            if split_pages:\n                # If split_pages is True, we create a separate entry for each page\n                page_info = {'page_no': 0, 'height': page_height, 'width': page_width}\n                page_dict = {'layout_dets': result, 'page_info': page_info}\n                # Convert PIL image to bytes\n                img_bytes = BytesIO()\n                img = Image.fromarray(img_dict['img'])\n                img.save(img_bytes, format='PNG')\n                img_ds = ImageDataset(img_bytes.getvalue())\n                inference_result = InferenceResultLLM([page_dict], img_ds)\n                inference_results.append(inference_result)\n            else:\n                page_info = {'page_no': index, 'height': page_height, 'width': page_width}\n                page_dict = {'layout_dets': result, 'page_info': page_info}\n                model_json.append(page_dict)\n        if not split_pages:\n            inference_results = InferenceResultLLM(model_json, dataset)\n\n    gc_start = time.time()\n    clean_memory(device)\n    gc_time = round(time.time() - gc_start, 2)\n    logger.info(f'gc time: {gc_time}')\n\n    doc_analyze_time = round(time.time() - doc_analyze_start, 2)\n    doc_analyze_speed = round((end_page_id + 1 - start_page_id) / doc_analyze_time, 2)\n    logger.info(\n        f'doc analyze time: {round(time.time() - doc_analyze_start, 2)},'\n        f'speed: {doc_analyze_speed} pages/second'\n    )\n\n    return inference_results\n"
  },
  {
    "file_name": "magic_pdf/model/magic_model.py",
    "file_contents": "import enum\n\nfrom magic_pdf.config.model_block_type import ModelBlockTypeEnum\nfrom magic_pdf.config.ocr_content_type import CategoryId, ContentType\nfrom magic_pdf.data.dataset import Dataset\nfrom magic_pdf.libs.boxbase import (_is_in, bbox_distance, bbox_relative_pos,\n                                    calculate_iou)\nfrom magic_pdf.libs.coordinate_transform import get_scale_ratio\nfrom magic_pdf.pre_proc.remove_bbox_overlap import _remove_overlap_between_bbox\n\nCAPATION_OVERLAP_AREA_RATIO = 0.6\nMERGE_BOX_OVERLAP_AREA_RATIO = 1.1\n\n\nclass PosRelationEnum(enum.Enum):\n    LEFT = 'left'\n    RIGHT = 'right'\n    UP = 'up'\n    BOTTOM = 'bottom'\n    ALL = 'all'\n\n\nclass MagicModel:\n\n    def __fix_axis(self):\n        for model_page_info in self.__model_list:\n            need_remove_list = []\n            page_no = model_page_info['page_info']['page_no']\n            horizontal_scale_ratio, vertical_scale_ratio = get_scale_ratio(\n                model_page_info, self.__docs.get_page(page_no)\n            )\n            layout_dets = model_page_info['layout_dets']\n            for layout_det in layout_dets:\n\n                if layout_det.get('bbox') is not None:\n\n                    x0, y0, x1, y1 = layout_det['bbox']\n                else:\n\n                    x0, y0, _, _, x1, y1, _, _ = layout_det['poly']\n\n                bbox = [\n                    int(x0 / horizontal_scale_ratio),\n                    int(y0 / vertical_scale_ratio),\n                    int(x1 / horizontal_scale_ratio),\n                    int(y1 / vertical_scale_ratio),\n                ]\n                layout_det['bbox'] = bbox\n\n                if bbox[2] - bbox[0] <= 0 or bbox[3] - bbox[1] <= 0:\n                    need_remove_list.append(layout_det)\n            for need_remove in need_remove_list:\n                layout_dets.remove(need_remove)\n\n    def __fix_by_remove_low_confidence(self):\n        for model_page_info in self.__model_list:\n            need_remove_list = []\n            layout_dets = model_page_info['layout_dets']\n            for layout_det in layout_dets:\n                if layout_det['score'] <= 0.05:\n                    need_remove_list.append(layout_det)\n                else:\n                    continue\n            for need_remove in need_remove_list:\n                layout_dets.remove(need_remove)\n\n    def __fix_by_remove_high_iou_and_low_confidence(self):\n        for model_page_info in self.__model_list:\n            need_remove_list = []\n            layout_dets = model_page_info['layout_dets']\n            for i in range(len(layout_dets)):\n                for j in range(i + 1, len(layout_dets)):\n                    layout_det1 = layout_dets[i]\n                    layout_det2 = layout_dets[j]\n                    if layout_det1['category_id'] in [\n                        0,\n                        1,\n                        2,\n                        3,\n                        4,\n                        5,\n                        6,\n                        7,\n                        8,\n                        9,\n                    ] and layout_det2['category_id'] in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n                        if (\n                            calculate_iou(layout_det1['bbox'], layout_det2['bbox'])\n                            > 0.9\n                        ):\n                            if layout_det1['score'] < layout_det2['score']:\n                                layout_det_need_remove = layout_det1\n                            else:\n                                layout_det_need_remove = layout_det2\n\n                            if layout_det_need_remove not in need_remove_list:\n                                need_remove_list.append(layout_det_need_remove)\n                        else:\n                            continue\n                    else:\n                        continue\n            for need_remove in need_remove_list:\n                layout_dets.remove(need_remove)\n\n    def __init__(self, model_list: list, docs: Dataset):\n        self.__model_list = model_list\n        self.__docs = docs\n        self.__fix_axis()\n        self.__fix_by_remove_low_confidence()\n        self.__fix_by_remove_high_iou_and_low_confidence()\n        self.__fix_footnote()\n\n    def _bbox_distance(self, bbox1, bbox2):\n        left, right, bottom, top = bbox_relative_pos(bbox1, bbox2)\n        flags = [left, right, bottom, top]\n        count = sum([1 if v else 0 for v in flags])\n        if count > 1:\n            return float('inf')\n        if left or right:\n            l1 = bbox1[3] - bbox1[1]\n            l2 = bbox2[3] - bbox2[1]\n        else:\n            l1 = bbox1[2] - bbox1[0]\n            l2 = bbox2[2] - bbox2[0]\n\n        if l2 > l1 and (l2 - l1) / l1 > 0.3:\n            return float('inf')\n\n        return bbox_distance(bbox1, bbox2)\n\n    def __fix_footnote(self):\n        # 3: figure, 5: table, 7: footnote\n        for model_page_info in self.__model_list:\n            footnotes = []\n            figures = []\n            tables = []\n\n            for obj in model_page_info['layout_dets']:\n                if obj['category_id'] == 7:\n                    footnotes.append(obj)\n                elif obj['category_id'] == 3:\n                    figures.append(obj)\n                elif obj['category_id'] == 5:\n                    tables.append(obj)\n                if len(footnotes) * len(figures) == 0:\n                    continue\n            dis_figure_footnote = {}\n            dis_table_footnote = {}\n\n            for i in range(len(footnotes)):\n                for j in range(len(figures)):\n                    pos_flag_count = sum(\n                        list(\n                            map(\n                                lambda x: 1 if x else 0,\n                                bbox_relative_pos(\n                                    footnotes[i]['bbox'], figures[j]['bbox']\n                                ),\n                            )\n                        )\n                    )\n                    if pos_flag_count > 1:\n                        continue\n                    dis_figure_footnote[i] = min(\n                        self._bbox_distance(figures[j]['bbox'], footnotes[i]['bbox']),\n                        dis_figure_footnote.get(i, float('inf')),\n                    )\n            for i in range(len(footnotes)):\n                for j in range(len(tables)):\n                    pos_flag_count = sum(\n                        list(\n                            map(\n                                lambda x: 1 if x else 0,\n                                bbox_relative_pos(\n                                    footnotes[i]['bbox'], tables[j]['bbox']\n                                ),\n                            )\n                        )\n                    )\n                    if pos_flag_count > 1:\n                        continue\n\n                    dis_table_footnote[i] = min(\n                        self._bbox_distance(tables[j]['bbox'], footnotes[i]['bbox']),\n                        dis_table_footnote.get(i, float('inf')),\n                    )\n            for i in range(len(footnotes)):\n                if i not in dis_figure_footnote:\n                    continue\n                if dis_table_footnote.get(i, float('inf')) > dis_figure_footnote[i]:\n                    footnotes[i]['category_id'] = CategoryId.ImageFootnote\n\n    def __reduct_overlap(self, bboxes):\n        N = len(bboxes)\n        keep = [True] * N\n        for i in range(N):\n            for j in range(N):\n                if i == j:\n                    continue\n                if _is_in(bboxes[i]['bbox'], bboxes[j]['bbox']):\n                    keep[i] = False\n        return [bboxes[i] for i in range(N) if keep[i]]\n\n    def __tie_up_category_by_distance_v2(\n        self,\n        page_no: int,\n        subject_category_id: int,\n        object_category_id: int,\n        priority_pos: PosRelationEnum,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            page_no (int): _description_\n            subject_category_id (int): _description_\n            object_category_id (int): _description_\n            priority_pos (PosRelationEnum): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        AXIS_MULPLICITY = 0.5\n        subjects = self.__reduct_overlap(\n            list(\n                map(\n                    lambda x: {'bbox': x['bbox'], 'score': x['score']},\n                    filter(\n                        lambda x: x['category_id'] == subject_category_id,\n                        self.__model_list[page_no]['layout_dets'],\n                    ),\n                )\n            )\n        )\n\n        objects = self.__reduct_overlap(\n            list(\n                map(\n                    lambda x: {'bbox': x['bbox'], 'score': x['score']},\n                    filter(\n                        lambda x: x['category_id'] == object_category_id,\n                        self.__model_list[page_no]['layout_dets'],\n                    ),\n                )\n            )\n        )\n        M = len(objects)\n\n        subjects.sort(key=lambda x: x['bbox'][0] ** 2 + x['bbox'][1] ** 2)\n        objects.sort(key=lambda x: x['bbox'][0] ** 2 + x['bbox'][1] ** 2)\n\n        sub_obj_map_h = {i: [] for i in range(len(subjects))}\n\n        dis_by_directions = {\n            'top': [[-1, float('inf')]] * M,\n            'bottom': [[-1, float('inf')]] * M,\n            'left': [[-1, float('inf')]] * M,\n            'right': [[-1, float('inf')]] * M,\n        }\n\n        for i, obj in enumerate(objects):\n            l_x_axis, l_y_axis = (\n                obj['bbox'][2] - obj['bbox'][0],\n                obj['bbox'][3] - obj['bbox'][1],\n            )\n            axis_unit = min(l_x_axis, l_y_axis)\n            for j, sub in enumerate(subjects):\n\n                bbox1, bbox2, _ = _remove_overlap_between_bbox(\n                    objects[i]['bbox'], subjects[j]['bbox']\n                )\n                left, right, bottom, top = bbox_relative_pos(bbox1, bbox2)\n                flags = [left, right, bottom, top]\n                if sum([1 if v else 0 for v in flags]) > 1:\n                    continue\n\n                if left:\n                    if dis_by_directions['left'][i][1] > bbox_distance(\n                        obj['bbox'], sub['bbox']\n                    ):\n                        dis_by_directions['left'][i] = [\n                            j,\n                            bbox_distance(obj['bbox'], sub['bbox']),\n                        ]\n                if right:\n                    if dis_by_directions['right'][i][1] > bbox_distance(\n                        obj['bbox'], sub['bbox']\n                    ):\n                        dis_by_directions['right'][i] = [\n                            j,\n                            bbox_distance(obj['bbox'], sub['bbox']),\n                        ]\n                if bottom:\n                    if dis_by_directions['bottom'][i][1] > bbox_distance(\n                        obj['bbox'], sub['bbox']\n                    ):\n                        dis_by_directions['bottom'][i] = [\n                            j,\n                            bbox_distance(obj['bbox'], sub['bbox']),\n                        ]\n                if top:\n                    if dis_by_directions['top'][i][1] > bbox_distance(\n                        obj['bbox'], sub['bbox']\n                    ):\n                        dis_by_directions['top'][i] = [\n                            j,\n                            bbox_distance(obj['bbox'], sub['bbox']),\n                        ]\n\n            if (\n                dis_by_directions['top'][i][1] != float('inf')\n                and dis_by_directions['bottom'][i][1] != float('inf')\n                and priority_pos in (PosRelationEnum.BOTTOM, PosRelationEnum.UP)\n            ):\n                RATIO = 3\n                if (\n                    abs(\n                        dis_by_directions['top'][i][1]\n                        - dis_by_directions['bottom'][i][1]\n                    )\n                    < RATIO * axis_unit\n                ):\n\n                    if priority_pos == PosRelationEnum.BOTTOM:\n                        sub_obj_map_h[dis_by_directions['bottom'][i][0]].append(i)\n                    else:\n                        sub_obj_map_h[dis_by_directions['top'][i][0]].append(i)\n                    continue\n\n            if dis_by_directions['left'][i][1] != float('inf') or dis_by_directions[\n                'right'\n            ][i][1] != float('inf'):\n                if dis_by_directions['left'][i][1] != float(\n                    'inf'\n                ) and dis_by_directions['right'][i][1] != float('inf'):\n                    if AXIS_MULPLICITY * axis_unit >= abs(\n                        dis_by_directions['left'][i][1]\n                        - dis_by_directions['right'][i][1]\n                    ):\n                        left_sub_bbox = subjects[dis_by_directions['left'][i][0]][\n                            'bbox'\n                        ]\n                        right_sub_bbox = subjects[dis_by_directions['right'][i][0]][\n                            'bbox'\n                        ]\n\n                        left_sub_bbox_y_axis = left_sub_bbox[3] - left_sub_bbox[1]\n                        right_sub_bbox_y_axis = right_sub_bbox[3] - right_sub_bbox[1]\n\n                        if (\n                            abs(left_sub_bbox_y_axis - l_y_axis)\n                            + dis_by_directions['left'][i][0]\n                            > abs(right_sub_bbox_y_axis - l_y_axis)\n                            + dis_by_directions['right'][i][0]\n                        ):\n                            left_or_right = dis_by_directions['right'][i]\n                        else:\n                            left_or_right = dis_by_directions['left'][i]\n                    else:\n                        left_or_right = dis_by_directions['left'][i]\n                        if left_or_right[1] > dis_by_directions['right'][i][1]:\n                            left_or_right = dis_by_directions['right'][i]\n                else:\n                    left_or_right = dis_by_directions['left'][i]\n                    if left_or_right[1] == float('inf'):\n                        left_or_right = dis_by_directions['right'][i]\n            else:\n                left_or_right = [-1, float('inf')]\n\n            if dis_by_directions['top'][i][1] != float('inf') or dis_by_directions[\n                'bottom'\n            ][i][1] != float('inf'):\n                if dis_by_directions['top'][i][1] != float('inf') and dis_by_directions[\n                    'bottom'\n                ][i][1] != float('inf'):\n                    if AXIS_MULPLICITY * axis_unit >= abs(\n                        dis_by_directions['top'][i][1]\n                        - dis_by_directions['bottom'][i][1]\n                    ):\n                        top_bottom = subjects[dis_by_directions['bottom'][i][0]]['bbox']\n                        bottom_top = subjects[dis_by_directions['top'][i][0]]['bbox']\n\n                        top_bottom_x_axis = top_bottom[2] - top_bottom[0]\n                        bottom_top_x_axis = bottom_top[2] - bottom_top[0]\n                        if (\n                            abs(top_bottom_x_axis - l_x_axis)\n                            + dis_by_directions['bottom'][i][1]\n                            > abs(bottom_top_x_axis - l_x_axis)\n                            + dis_by_directions['top'][i][1]\n                        ):\n                            top_or_bottom = dis_by_directions['top'][i]\n                        else:\n                            top_or_bottom = dis_by_directions['bottom'][i]\n                    else:\n                        top_or_bottom = dis_by_directions['top'][i]\n                        if top_or_bottom[1] > dis_by_directions['bottom'][i][1]:\n                            top_or_bottom = dis_by_directions['bottom'][i]\n                else:\n                    top_or_bottom = dis_by_directions['top'][i]\n                    if top_or_bottom[1] == float('inf'):\n                        top_or_bottom = dis_by_directions['bottom'][i]\n            else:\n                top_or_bottom = [-1, float('inf')]\n\n            if left_or_right[1] != float('inf') or top_or_bottom[1] != float('inf'):\n                if left_or_right[1] != float('inf') and top_or_bottom[1] != float(\n                    'inf'\n                ):\n                    if AXIS_MULPLICITY * axis_unit >= abs(\n                        left_or_right[1] - top_or_bottom[1]\n                    ):\n                        y_axis_bbox = subjects[left_or_right[0]]['bbox']\n                        x_axis_bbox = subjects[top_or_bottom[0]]['bbox']\n\n                        if (\n                            abs((x_axis_bbox[2] - x_axis_bbox[0]) - l_x_axis) / l_x_axis\n                            > abs((y_axis_bbox[3] - y_axis_bbox[1]) - l_y_axis)\n                            / l_y_axis\n                        ):\n                            sub_obj_map_h[left_or_right[0]].append(i)\n                        else:\n                            sub_obj_map_h[top_or_bottom[0]].append(i)\n                    else:\n                        if left_or_right[1] > top_or_bottom[1]:\n                            sub_obj_map_h[top_or_bottom[0]].append(i)\n                        else:\n                            sub_obj_map_h[left_or_right[0]].append(i)\n                else:\n                    if left_or_right[1] != float('inf'):\n                        sub_obj_map_h[left_or_right[0]].append(i)\n                    else:\n                        sub_obj_map_h[top_or_bottom[0]].append(i)\n        ret = []\n        for i in sub_obj_map_h.keys():\n            ret.append(\n                {\n                    'sub_bbox': {\n                        'bbox': subjects[i]['bbox'],\n                        'score': subjects[i]['score'],\n                    },\n                    'obj_bboxes': [\n                        {'score': objects[j]['score'], 'bbox': objects[j]['bbox']}\n                        for j in sub_obj_map_h[i]\n                    ],\n                    'sub_idx': i,\n                }\n            )\n        return ret\n\n    def get_imgs_v2(self, page_no: int):\n        with_captions = self.__tie_up_category_by_distance_v2(\n            page_no, 3, 4, PosRelationEnum.BOTTOM\n        )\n        with_footnotes = self.__tie_up_category_by_distance_v2(\n            page_no, 3, CategoryId.ImageFootnote, PosRelationEnum.ALL\n        )\n        ret = []\n        for v in with_captions:\n            record = {\n                'image_body': v['sub_bbox'],\n                'image_caption_list': v['obj_bboxes'],\n            }\n            filter_idx = v['sub_idx']\n            d = next(filter(lambda x: x['sub_idx'] == filter_idx, with_footnotes))\n            record['image_footnote_list'] = d['obj_bboxes']\n            ret.append(record)\n        return ret\n\n    def get_tables_v2(self, page_no: int) -> list:\n        with_captions = self.__tie_up_category_by_distance_v2(\n            page_no, 5, 6, PosRelationEnum.UP\n        )\n        with_footnotes = self.__tie_up_category_by_distance_v2(\n            page_no, 5, 7, PosRelationEnum.ALL\n        )\n        ret = []\n        for v in with_captions:\n            record = {\n                'table_body': v['sub_bbox'],\n                'table_caption_list': v['obj_bboxes'],\n            }\n            filter_idx = v['sub_idx']\n            d = next(filter(lambda x: x['sub_idx'] == filter_idx, with_footnotes))\n            record['table_footnote_list'] = d['obj_bboxes']\n            ret.append(record)\n        return ret\n\n    def get_imgs(self, page_no: int):\n        return self.get_imgs_v2(page_no)\n\n    def get_tables(\n        self, page_no: int\n    ) -> list:\n        return self.get_tables_v2(page_no)\n\n    def get_equations(self, page_no: int) -> list:\n        inline_equations = self.__get_blocks_by_type(\n            ModelBlockTypeEnum.EMBEDDING.value, page_no, ['latex']\n        )\n        interline_equations = self.__get_blocks_by_type(\n            ModelBlockTypeEnum.ISOLATED.value, page_no, ['latex']\n        )\n        interline_equations_blocks = self.__get_blocks_by_type(\n            ModelBlockTypeEnum.ISOLATE_FORMULA.value, page_no\n        )\n        return inline_equations, interline_equations, interline_equations_blocks\n\n    def get_discarded(self, page_no: int) -> list:\n        blocks = self.__get_blocks_by_type(ModelBlockTypeEnum.ABANDON.value, page_no)\n        return blocks\n\n    def get_text_blocks(self, page_no: int) -> list:\n        blocks = self.__get_blocks_by_type(ModelBlockTypeEnum.PLAIN_TEXT.value, page_no)\n        return blocks\n\n    def get_title_blocks(self, page_no: int) -> list:\n        blocks = self.__get_blocks_by_type(ModelBlockTypeEnum.TITLE.value, page_no)\n        return blocks\n\n    def get_ocr_text(self, page_no: int) -> list:\n        text_spans = []\n        model_page_info = self.__model_list[page_no]\n        layout_dets = model_page_info['layout_dets']\n        for layout_det in layout_dets:\n            if layout_det['category_id'] == '15':\n                span = {\n                    'bbox': layout_det['bbox'],\n                    'content': layout_det['text'],\n                }\n                text_spans.append(span)\n        return text_spans\n\n    def get_all_spans(self, page_no: int) -> list:\n\n        def remove_duplicate_spans(spans):\n            new_spans = []\n            for span in spans:\n                if not any(span == existing_span for existing_span in new_spans):\n                    new_spans.append(span)\n            return new_spans\n\n        all_spans = []\n        model_page_info = self.__model_list[page_no]\n        layout_dets = model_page_info['layout_dets']\n        allow_category_id_list = [3, 5, 13, 14, 15]\n\n\n\n\n\n        for layout_det in layout_dets:\n            category_id = layout_det['category_id']\n            if category_id in allow_category_id_list:\n                span = {'bbox': layout_det['bbox'], 'score': layout_det['score']}\n                if category_id == 3:\n                    span['type'] = ContentType.Image\n                elif category_id == 5:\n\n                    latex = layout_det.get('latex', None)\n                    html = layout_det.get('html', None)\n                    if latex:\n                        span['latex'] = latex\n                    elif html:\n                        span['html'] = html\n                    span['type'] = ContentType.Table\n                elif category_id == 13:\n                    span['content'] = layout_det['latex']\n                    span['type'] = ContentType.InlineEquation\n                elif category_id == 14:\n                    span['content'] = layout_det['latex']\n                    span['type'] = ContentType.InterlineEquation\n                elif category_id == 15:\n                    span['content'] = layout_det['text']\n                    span['type'] = ContentType.Text\n                all_spans.append(span)\n        return remove_duplicate_spans(all_spans)\n\n    def get_page_size(self, page_no: int):\n\n        page = self.__docs.get_page(page_no).get_page_info()\n\n        page_w = page.w\n        page_h = page.h\n        return page_w, page_h\n\n    def __get_blocks_by_type(\n        self, type: int, page_no: int, extra_col: list[str] = []\n    ) -> list:\n        blocks = []\n        for page_dict in self.__model_list:\n            layout_dets = page_dict.get('layout_dets', [])\n            page_info = page_dict.get('page_info', {})\n            page_number = page_info.get('page_no', -1)\n            if page_no != page_number:\n                continue\n            for item in layout_dets:\n                category_id = item.get('category_id', -1)\n                bbox = item.get('bbox', None)\n\n                if category_id == type:\n                    block = {\n                        'bbox': bbox,\n                        'score': item.get('score'),\n                    }\n                    for col in extra_col:\n                        block[col] = item.get(col, None)\n                    blocks.append(block)\n        return blocks\n\n    def get_model_list(self, page_no):\n        return self.__model_list[page_no]\n"
  },
  {
    "file_name": "magic_pdf/model/model_list.py",
    "file_contents": "class AtomicModel:\n    Layout = \"layout\"\n"
  },
  {
    "file_name": "magic_pdf/operators/__init__.py",
    "file_contents": "from abc import ABC, abstractmethod\nfrom typing import Callable\n\nfrom magic_pdf.data.data_reader_writer import DataWriter\nfrom magic_pdf.data.dataset import Dataset\nfrom magic_pdf.operators.pipes_llm import PipeResultLLM\n\n\nclass InferenceResultBase(ABC):\n\n    @abstractmethod\n    def __init__(self, inference_results: list, dataset: Dataset):\n        \"\"\"Initialized method.\n\n        Args:\n            inference_results (list): the inference result generated by model\n            dataset (Dataset): the dataset related with model inference result\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def draw_model(self, file_path: str) -> None:\n        \"\"\"Draw model inference result.\n\n        Args:\n            file_path (str): the output file path\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dump_model(self, writer: DataWriter, file_path: str):\n        \"\"\"Dump model inference result to file.\n\n        Args:\n            writer (DataWriter): writer handle\n            file_path (str): the location of target file\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_infer_res(self):\n        \"\"\"Get the inference result.\n\n        Returns:\n            list: the inference result generated by model\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(inference_result, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        pass\n\n    def pipe_txt_mode(\n        self,\n        imageWriter: DataWriter,\n        start_page_id=0,\n        end_page_id=None,\n        debug_mode=False,\n        lang=None,\n    ) -> PipeResultLLM:\n        \"\"\"Post-proc the model inference result, Extract the text using the\n        third library, such as `pymupdf`\n\n        Args:\n            imageWriter (DataWriter): the image writer handle\n            start_page_id (int, optional): Defaults to 0. Let user select some pages He/She want to process\n            end_page_id (int, optional):  Defaults to the last page index of dataset. Let user select some pages He/She want to process\n            debug_mode (bool, optional): Defaults to False. will dump more log if enabled\n            lang (str, optional): Defaults to None.\n\n        Returns:\n            PipeResult: the result\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def pipe_ocr_mode(\n        self,\n        imageWriter: DataWriter,\n        start_page_id=0,\n        end_page_id=None,\n        debug_mode=False,\n        lang=None,\n    ) -> PipeResultLLM:\n        pass\n"
  },
  {
    "file_name": "magic_pdf/operators/models_llm.py",
    "file_contents": "import copy\nimport json\nimport os\nfrom typing import Callable\n\nfrom magic_pdf.config.constants import PARSE_TYPE_OCR\nfrom magic_pdf.config.enums import SupportedPdfParseMethod\nfrom magic_pdf.data.data_reader_writer import DataWriter\nfrom magic_pdf.data.dataset import Dataset\nfrom magic_pdf.libs.draw_bbox import draw_model_bbox\nfrom magic_pdf.libs.version import __version__\nfrom magic_pdf.operators.pipes_llm import PipeResultLLM\nfrom magic_pdf.pdf_parse_union_core_v2_llm import pdf_parse_union\nfrom magic_pdf.operators import InferenceResultBase\n\nclass InferenceResultLLM(InferenceResultBase):\n    def __init__(self, inference_results: list, dataset: Dataset):\n        \"\"\"Initialized method.\n\n        Args:\n            inference_results (list): the inference result generated by model\n            dataset (Dataset): the dataset related with model inference result\n        \"\"\"\n        self._infer_res = inference_results\n        self._dataset = dataset\n\n    def draw_model(self, file_path: str) -> None:\n        \"\"\"Draw model inference result.\n\n        Args:\n            file_path (str): the output file path\n        \"\"\"\n        dir_name = os.path.dirname(file_path)\n        base_name = os.path.basename(file_path)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name, exist_ok=True)\n        draw_model_bbox(\n            copy.deepcopy(self._infer_res), self._dataset, dir_name, base_name\n        )\n\n    def dump_model(self, writer: DataWriter, file_path: str):\n        \"\"\"Dump model inference result to file.\n\n        Args:\n            writer (DataWriter): writer handle\n            file_path (str): the location of target file\n        \"\"\"\n        writer.write_string(\n            file_path, json.dumps(self._infer_res, ensure_ascii=False, indent=4)\n        )\n\n    def get_infer_res(self):\n        \"\"\"Get the inference result.\n\n        Returns:\n            list: the inference result generated by model\n        \"\"\"\n        return self._infer_res\n\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(inference_result, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        return proc(copy.deepcopy(self._infer_res), *args, **kwargs)\n\n    def pipe_ocr_mode(\n        self,\n        imageWriter: DataWriter,\n        MonkeyOCR_model,\n        start_page_id=0,\n        end_page_id=None,\n        debug_mode=False,\n        lang=None,\n    ) -> PipeResultLLM:\n        \"\"\"Post-proc the model inference result, Extract the text using `OCR`\n        technical.\n\n        Args:\n            imageWriter (DataWriter): the image writer handle\n            start_page_id (int, optional): Defaults to 0. Let user select some pages He/She want to process\n            end_page_id (int, optional):  Defaults to the last page index of dataset. Let user select some pages He/She want to process\n            debug_mode (bool, optional): Defaults to False. will dump more log if enabled\n            lang (str, optional): Defaults to None.\n\n        Returns:\n            PipeResultLLM: the result\n        \"\"\"\n\n        def proc(*args, **kwargs) -> PipeResultLLM:\n            res = pdf_parse_union(*args, **kwargs)\n            res['_parse_type'] = PARSE_TYPE_OCR\n            res['_version_name'] = __version__\n            if 'lang' in kwargs and kwargs['lang'] is not None:\n                res['lang'] = kwargs['lang']\n            return PipeResultLLM(res, self._dataset)\n\n        res = self.apply(\n            proc,\n            self._dataset,\n            imageWriter,\n            SupportedPdfParseMethod.OCR,\n            start_page_id=start_page_id,\n            end_page_id=end_page_id,\n            debug_mode=debug_mode,\n            lang=lang,\n            MonkeyOCR_model=MonkeyOCR_model\n        )\n        return res"
  },
  {
    "file_name": "magic_pdf/operators/pipes_llm.py",
    "file_contents": "import copy\nimport json\nimport os\nfrom typing import Callable\n\nfrom magic_pdf.config.make_content_config import DropMode, MakeMode\nfrom magic_pdf.data.data_reader_writer import DataWriter\nfrom magic_pdf.data.dataset import Dataset\nfrom magic_pdf.dict2md.ocr_mkcontent import union_make\nfrom magic_pdf.libs.draw_bbox import (draw_layout_bbox, draw_line_sort_bbox,\n                                      draw_span_bbox)\nfrom magic_pdf.libs.json_compressor import JsonCompressor\n\n\nclass PipeResultLLM:\n    def __init__(self, pipe_res, dataset: Dataset):\n        \"\"\"Initialized.\n\n        Args:\n            pipe_res (list[dict]): the pipeline processed result of model inference result\n            dataset (Dataset): the dataset associated with pipe_res\n        \"\"\"\n        self._pipe_res = pipe_res\n        self._dataset = dataset\n\n    def get_markdown(\n        self,\n        img_dir_or_bucket_prefix: str,\n        drop_mode=DropMode.NONE,\n        md_make_mode=MakeMode.MM_MD,\n    ) -> str:\n        \"\"\"Get markdown content.\n\n        Args:\n            img_dir_or_bucket_prefix (str): The s3 bucket prefix or local file directory which used to store the figure\n            drop_mode (str, optional): Drop strategy when some page which is corrupted or inappropriate. Defaults to DropMode.NONE.\n            md_make_mode (str, optional): The content Type of Markdown be made. Defaults to MakeMode.MM_MD.\n\n        Returns:\n            str: return markdown content\n        \"\"\"\n        pdf_info_list = self._pipe_res['pdf_info']\n        md_content = union_make(\n            pdf_info_list, md_make_mode, drop_mode, img_dir_or_bucket_prefix\n        )\n        return md_content.replace('\\\\$', '$').replace('\\\\*', '*').replace('<seg>', r'\\<seg\\>').replace('<sos>', r'\\<sos\\>').replace('<eos>', r'\\<eos\\>').replace('<pad>', r'\\<pad\\>').replace('<unk>', r'\\<unk\\>').replace('<sep>', r'\\<sep\\>').replace('<cls>', r'\\<cls\\>')\n\n    def dump_md(\n        self,\n        writer: DataWriter,\n        file_path: str,\n        img_dir_or_bucket_prefix: str,\n        drop_mode=DropMode.NONE,\n        md_make_mode=MakeMode.MM_MD,\n    ):\n        \"\"\"Dump The Markdown.\n\n        Args:\n            writer (DataWriter): File writer handle\n            file_path (str): The file location of markdown\n            img_dir_or_bucket_prefix (str): The s3 bucket prefix or local file directory which used to store the figure\n            drop_mode (str, optional): Drop strategy when some page which is corrupted or inappropriate. Defaults to DropMode.NONE.\n            md_make_mode (str, optional): The content Type of Markdown be made. Defaults to MakeMode.MM_MD.\n        \"\"\"\n\n        md_content = self.get_markdown(\n            img_dir_or_bucket_prefix, drop_mode=drop_mode, md_make_mode=md_make_mode\n        )\n        writer.write_string(file_path, md_content)\n\n    def get_content_list(\n        self,\n        image_dir_or_bucket_prefix: str,\n        drop_mode=DropMode.NONE,\n    ) -> str:\n        \"\"\"Get Content List.\n\n        Args:\n            image_dir_or_bucket_prefix (str): The s3 bucket prefix or local file directory which used to store the figure\n            drop_mode (str, optional): Drop strategy when some page which is corrupted or inappropriate. Defaults to DropMode.NONE.\n\n        Returns:\n            str: content list content\n        \"\"\"\n        pdf_info_list = self._pipe_res['pdf_info']\n        content_list = union_make(\n            pdf_info_list,\n            MakeMode.STANDARD_FORMAT,\n            drop_mode,\n            image_dir_or_bucket_prefix,\n        )\n        return content_list\n\n    def dump_content_list(\n        self,\n        writer: DataWriter,\n        file_path: str,\n        image_dir_or_bucket_prefix: str,\n        drop_mode=DropMode.NONE,\n    ):\n        \"\"\"Dump Content List.\n\n        Args:\n            writer (DataWriter): File writer handle\n            file_path (str): The file location of content list\n            image_dir_or_bucket_prefix (str): The s3 bucket prefix or local file directory which used to store the figure\n            drop_mode (str, optional): Drop strategy when some page which is corrupted or inappropriate. Defaults to DropMode.NONE.\n        \"\"\"\n        content_list = self.get_content_list(\n            image_dir_or_bucket_prefix, drop_mode=drop_mode,\n        )\n        writer.write_string(\n            file_path, json.dumps(content_list, ensure_ascii=False, indent=4)\n        )\n\n    def get_middle_json(self) -> str:\n        \"\"\"Get middle json.\n\n        Returns:\n            str: The content of middle json\n        \"\"\"\n        return json.dumps(self._pipe_res, ensure_ascii=False, indent=4)\n\n    def dump_middle_json(self, writer: DataWriter, file_path: str):\n        \"\"\"Dump the result of pipeline.\n\n        Args:\n            writer (DataWriter): File writer handler\n            file_path (str): The file location of middle json\n        \"\"\"\n        middle_json = self.get_middle_json()\n        writer.write_string(file_path, middle_json)\n\n    def draw_layout(self, file_path: str) -> None:\n        \"\"\"Draw the layout.\n\n        Args:\n            file_path (str): The file location of layout result file\n        \"\"\"\n        dir_name = os.path.dirname(file_path)\n        base_name = os.path.basename(file_path)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name, exist_ok=True)\n        pdf_info = self._pipe_res['pdf_info']\n        draw_layout_bbox(pdf_info, self._dataset.data_bits(), dir_name, base_name)\n\n    def draw_span(self, file_path: str):\n        \"\"\"Draw the Span.\n\n        Args:\n            file_path (str): The file location of span result file\n        \"\"\"\n        dir_name = os.path.dirname(file_path)\n        base_name = os.path.basename(file_path)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name, exist_ok=True)\n        pdf_info = self._pipe_res['pdf_info']\n        draw_span_bbox(pdf_info, self._dataset.data_bits(), dir_name, base_name)\n\n    def draw_line_sort(self, file_path: str):\n        \"\"\"Draw line sort.\n\n        Args:\n            file_path (str): The file location of line sort result file\n        \"\"\"\n        dir_name = os.path.dirname(file_path)\n        base_name = os.path.basename(file_path)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name, exist_ok=True)\n        pdf_info = self._pipe_res['pdf_info']\n        draw_line_sort_bbox(pdf_info, self._dataset.data_bits(), dir_name, base_name)\n\n    def get_compress_pdf_mid_data(self):\n        \"\"\"Compress the pipeline result.\n\n        Returns:\n            str: compress the pipeline result and return\n        \"\"\"\n        return JsonCompressor.compress_json(self._pipe_res)\n\n    def apply(self, proc: Callable, *args, **kwargs):\n        \"\"\"Apply callable method which.\n\n        Args:\n            proc (Callable): invoke proc as follows:\n                proc(pipeline_result, *args, **kwargs)\n\n        Returns:\n            Any: return the result generated by proc\n        \"\"\"\n        return proc(copy.deepcopy(self._pipe_res), *args, **kwargs)\n"
  },
  {
    "file_name": "magic_pdf/post_proc/para_split_v3.py",
    "file_contents": "import copy\nimport os\n\nfrom magic_pdf.config.constants import CROSS_PAGE, LINES_DELETED\nfrom magic_pdf.config.ocr_content_type import BlockType, ContentType\nfrom magic_pdf.libs.language import detect_lang\n\nLINE_STOP_FLAG = (\n    '.',\n    '!',\n    '?',\n    '„ÄÇ',\n    'ÔºÅ',\n    'Ôºü',\n    ')',\n    'Ôºâ',\n    '\"',\n    '‚Äù',\n    ':',\n    'Ôºö',\n    ';',\n    'Ôºõ',\n)\nLIST_END_FLAG = ('.', '„ÄÇ', ';', 'Ôºõ')\n\n\nclass ListLineTag:\n    IS_LIST_START_LINE = 'is_list_start_line'\n    IS_LIST_END_LINE = 'is_list_end_line'\n\n\ndef __process_blocks(blocks):\n    result = []\n    current_group = []\n\n    for i in range(len(blocks)):\n        current_block = blocks[i]\n\n\n        if current_block['type'] == 'text':\n            current_block['bbox_fs'] = copy.deepcopy(current_block['bbox'])\n            if 'lines' in current_block and len(current_block['lines']) > 0:\n                current_block['bbox_fs'] = [\n                    min([line['bbox'][0] for line in current_block['lines']]),\n                    min([line['bbox'][1] for line in current_block['lines']]),\n                    max([line['bbox'][2] for line in current_block['lines']]),\n                    max([line['bbox'][3] for line in current_block['lines']]),\n                ]\n            current_group.append(current_block)\n\n\n        if i + 1 < len(blocks):\n            next_block = blocks[i + 1]\n\n            if next_block['type'] in ['title', 'interline_equation']:\n                result.append(current_group)\n                current_group = []\n\n\n    if current_group:\n        result.append(current_group)\n\n    return result\n\n\ndef __is_list_or_index_block(block):\n    if len(block['lines']) >= 2:\n        first_line = block['lines'][0]\n        line_height = first_line['bbox'][3] - first_line['bbox'][1]\n        block_weight = block['bbox_fs'][2] - block['bbox_fs'][0]\n        block_height = block['bbox_fs'][3] - block['bbox_fs'][1]\n        page_weight, page_height = block['page_size']\n\n        left_close_num = 0\n        left_not_close_num = 0\n        right_not_close_num = 0\n        right_close_num = 0\n        lines_text_list = []\n        center_close_num = 0\n        external_sides_not_close_num = 0\n        multiple_para_flag = False\n        last_line = block['lines'][-1]\n\n        if page_weight == 0:\n            block_weight_ratio = 0\n        else:\n            block_weight_ratio = block_weight / page_weight\n        # logger.info(f\"block_weight_ratio: {block_weight_ratio}\")\n\n\n        if (\n            first_line['bbox'][0] - block['bbox_fs'][0] > line_height / 2\n            and abs(last_line['bbox'][0] - block['bbox_fs'][0]) < line_height / 2\n            and block['bbox_fs'][2] - last_line['bbox'][2] > line_height\n        ):\n            multiple_para_flag = True\n\n        for line in block['lines']:\n            line_mid_x = (line['bbox'][0] + line['bbox'][2]) / 2\n            block_mid_x = (block['bbox_fs'][0] + block['bbox_fs'][2]) / 2\n            if (\n                line['bbox'][0] - block['bbox_fs'][0] > 0.7 * line_height\n                and block['bbox_fs'][2] - line['bbox'][2] > 0.7 * line_height\n            ):\n                external_sides_not_close_num += 1\n            if abs(line_mid_x - block_mid_x) < line_height / 2:\n                center_close_num += 1\n\n            line_text = ''\n\n            for span in line['spans']:\n                span_type = span['type']\n                if span_type == ContentType.Text:\n                    line_text += span['content'].strip()\n\n\n            lines_text_list.append(line_text)\n            block_text = ''.join(lines_text_list)\n            block_lang = detect_lang(block_text)\n            # logger.info(f\"block_lang: {block_lang}\")\n\n\n            if abs(block['bbox_fs'][0] - line['bbox'][0]) < line_height / 2:\n                left_close_num += 1\n            elif line['bbox'][0] - block['bbox_fs'][0] > line_height:\n                left_not_close_num += 1\n\n\n            if abs(block['bbox_fs'][2] - line['bbox'][2]) < line_height:\n                right_close_num += 1\n            else:\n\n                if block_lang in ['zh', 'ja', 'ko']:\n                    closed_area = 0.26 * block_weight\n                else:\n\n\n                    if block_weight_ratio >= 0.5:\n                        closed_area = 0.26 * block_weight\n                    else:\n                        closed_area = 0.36 * block_weight\n                if block['bbox_fs'][2] - line['bbox'][2] > closed_area:\n                    right_not_close_num += 1\n\n\n        line_end_flag = False\n\n        line_num_flag = False\n        num_start_count = 0\n        num_end_count = 0\n        flag_end_count = 0\n\n        if len(lines_text_list) > 0:\n            for line_text in lines_text_list:\n                if len(line_text) > 0:\n                    if line_text[-1] in LIST_END_FLAG:\n                        flag_end_count += 1\n                    if line_text[0].isdigit():\n                        num_start_count += 1\n                    if line_text[-1].isdigit():\n                        num_end_count += 1\n\n            if (\n                num_start_count / len(lines_text_list) >= 0.8\n                or num_end_count / len(lines_text_list) >= 0.8\n            ):\n                line_num_flag = True\n            if flag_end_count / len(lines_text_list) >= 0.8:\n                line_end_flag = True\n\n\n        if (\n            left_close_num / len(block['lines']) >= 0.8\n            or right_close_num / len(block['lines']) >= 0.8\n        ) and line_num_flag:\n            for line in block['lines']:\n                line[ListLineTag.IS_LIST_START_LINE] = True\n            return BlockType.Index\n\n\n\n        elif (\n            external_sides_not_close_num >= 2\n            and center_close_num == len(block['lines'])\n            and external_sides_not_close_num / len(block['lines']) >= 0.5\n            and block_height / block_weight > 0.4\n        ):\n            for line in block['lines']:\n                line[ListLineTag.IS_LIST_START_LINE] = True\n            return BlockType.List\n\n        elif (\n            left_close_num >= 2\n            and (right_not_close_num >= 2 or line_end_flag or left_not_close_num >= 2)\n            and not multiple_para_flag\n            # and block_weight_ratio > 0.27\n        ):\n\n            if left_close_num / len(block['lines']) > 0.8:\n\n                if flag_end_count == 0 and right_close_num / len(block['lines']) < 0.5:\n                    for line in block['lines']:\n                        if abs(block['bbox_fs'][0] - line['bbox'][0]) < line_height / 2:\n                            line[ListLineTag.IS_LIST_START_LINE] = True\n\n                elif line_end_flag:\n                    for i, line in enumerate(block['lines']):\n                        if (\n                            len(lines_text_list[i]) > 0\n                            and lines_text_list[i][-1] in LIST_END_FLAG\n                        ):\n                            line[ListLineTag.IS_LIST_END_LINE] = True\n                            if i + 1 < len(block['lines']):\n                                block['lines'][i + 1][\n                                    ListLineTag.IS_LIST_START_LINE\n                                ] = True\n\n                else:\n                    line_start_flag = False\n                    for i, line in enumerate(block['lines']):\n                        if line_start_flag:\n                            line[ListLineTag.IS_LIST_START_LINE] = True\n                            line_start_flag = False\n\n                        if (\n                            abs(block['bbox_fs'][2] - line['bbox'][2])\n                            > 0.1 * block_weight\n                        ):\n                            line[ListLineTag.IS_LIST_END_LINE] = True\n                            line_start_flag = True\n\n            elif num_start_count >= 2 and num_start_count == flag_end_count:\n                for i, line in enumerate(block['lines']):\n                    if len(lines_text_list[i]) > 0:\n                        if lines_text_list[i][0].isdigit():\n                            line[ListLineTag.IS_LIST_START_LINE] = True\n                        if lines_text_list[i][-1] in LIST_END_FLAG:\n                            line[ListLineTag.IS_LIST_END_LINE] = True\n            else:\n\n                for line in block['lines']:\n                    if abs(block['bbox_fs'][0] - line['bbox'][0]) < line_height / 2:\n                        line[ListLineTag.IS_LIST_START_LINE] = True\n                    if abs(block['bbox_fs'][2] - line['bbox'][2]) > line_height:\n                        line[ListLineTag.IS_LIST_END_LINE] = True\n\n            return BlockType.List\n        else:\n            return BlockType.Text\n    else:\n        return BlockType.Text\n\n\ndef __merge_2_text_blocks(block1, block2):\n    if len(block1['lines']) > 0:\n        first_line = block1['lines'][0]\n        line_height = first_line['bbox'][3] - first_line['bbox'][1]\n        block1_weight = block1['bbox'][2] - block1['bbox'][0]\n        block2_weight = block2['bbox'][2] - block2['bbox'][0]\n        min_block_weight = min(block1_weight, block2_weight)\n        if abs(block1['bbox_fs'][0] - first_line['bbox'][0]) < line_height / 2:\n            last_line = block2['lines'][-1]\n            if len(last_line['spans']) > 0:\n                last_span = last_line['spans'][-1]\n                line_height = last_line['bbox'][3] - last_line['bbox'][1]\n                if len(first_line['spans']) > 0:\n                    first_span = first_line['spans'][0]\n                    if len(first_span['content']) > 0:\n                        span_start_with_num = first_span['content'][0].isdigit()\n                        span_start_with_big_char = first_span['content'][0].isupper()\n                        if (\n\n                            abs(block2['bbox_fs'][2] - last_line['bbox'][2]) < line_height\n\n                            and not last_span['content'].endswith(LINE_STOP_FLAG)\n\n                            and abs(block1_weight - block2_weight) < min_block_weight\n\n                            and not span_start_with_num\n\n                            and not span_start_with_big_char\n                        ):\n                            if block1['page_num'] != block2['page_num']:\n                                for line in block1['lines']:\n                                    for span in line['spans']:\n                                        span[CROSS_PAGE] = True\n                            block2['lines'].extend(block1['lines'])\n                            block1['lines'] = []\n                            block1[LINES_DELETED] = True\n\n    return block1, block2\n\n\ndef __merge_2_list_blocks(block1, block2):\n    if block1['page_num'] != block2['page_num']:\n        for line in block1['lines']:\n            for span in line['spans']:\n                span[CROSS_PAGE] = True\n    block2['lines'].extend(block1['lines'])\n    block1['lines'] = []\n    block1[LINES_DELETED] = True\n\n    return block1, block2\n\n\ndef __is_list_group(text_blocks_group):\n    for block in text_blocks_group:\n        if len(block['lines']) > 3:\n            return False\n    return True\n\n\ndef __is_list_group_llm(text_blocks_group):\n    for block in text_blocks_group:\n        if len(block['lines'][0]['spans'][0]['content']) > 400:\n            return False\n    return True\n\n\ndef __para_merge_page(blocks):\n    page_text_blocks_groups = __process_blocks(blocks)\n    for text_blocks_group in page_text_blocks_groups:\n        if len(text_blocks_group) > 0:\n\n            for block in text_blocks_group:\n                block_type = __is_list_or_index_block(block)\n                block['type'] = block_type\n                # logger.info(f\"{block['type']}:{block}\")\n\n        if len(text_blocks_group) > 1:\n\n            is_list_group = False #__is_list_group_llm(text_blocks_group)\n\n\n            for i in range(len(text_blocks_group) - 1, -1, -1):\n                current_block = text_blocks_group[i]\n\n\n                if i - 1 >= 0:\n                    prev_block = text_blocks_group[i - 1]\n\n                    if (\n                        current_block['type'] == 'text'\n                        and prev_block['type'] == 'text'\n                        and not is_list_group\n                    ):\n                        __merge_2_text_blocks(current_block, prev_block)\n                    elif (\n                        current_block['type'] == BlockType.List\n                        and prev_block['type'] == BlockType.List\n                    ) or (\n                        current_block['type'] == BlockType.Index\n                        and prev_block['type'] == BlockType.Index\n                    ):\n                        __merge_2_list_blocks(current_block, prev_block)\n\n        else:\n            continue\n\n\ndef para_split(pdf_info_dict):\n    all_blocks = []\n    for page_num, page in pdf_info_dict.items():\n        blocks = copy.deepcopy(page['preproc_blocks'])\n        for block in blocks:\n            block['page_num'] = page_num\n            block['page_size'] = page['page_size']\n        all_blocks.extend(blocks)\n\n    if os.getenv(\"MERGE_BLOCKS\", \"0\") == \"1\":\n        __para_merge_page(all_blocks)\n    for page_num, page in pdf_info_dict.items():\n        page['para_blocks'] = []\n        for block in all_blocks:\n            if block['page_num'] == page_num:\n                page['para_blocks'].append(block)\n\n\nif __name__ == '__main__':\n    input_blocks = []\n\n    groups = __process_blocks(input_blocks)\n    for group_index, group in enumerate(groups):\n        print(f'Group {group_index}: {group}')\n"
  },
  {
    "file_name": "magic_pdf/pre_proc/construct_page_dict.py",
    "file_contents": "\ndef ocr_construct_page_component_v2(blocks, layout_bboxes, page_id, page_w, page_h, layout_tree,\n                                    images, tables, interline_equations, discarded_blocks, need_drop, drop_reason):\n    return_dict = {\n        'preproc_blocks': blocks,\n        'layout_bboxes': layout_bboxes,\n        'page_idx': page_id,\n        'page_size': [page_w, page_h],\n        '_layout_tree': layout_tree,\n        'images': images,\n        'tables': tables,\n        'interline_equations': interline_equations,\n        'discarded_blocks': discarded_blocks,\n        'need_drop': need_drop,\n        'drop_reason': drop_reason,\n    }\n    return return_dict\n"
  },
  {
    "file_name": "magic_pdf/pre_proc/cut_image.py",
    "file_contents": "from loguru import logger\n\nfrom magic_pdf.config.ocr_content_type import ContentType\nfrom magic_pdf.libs.commons import join_path\nfrom magic_pdf.libs.pdf_image_tools import cut_image\n\n\ndef ocr_cut_image_and_table(spans, page, page_id, pdf_bytes_md5, imageWriter):\n    def return_path(type):\n        return join_path(pdf_bytes_md5, type)\n    \n    if not imageWriter: \n        return spans\n\n    for span in spans:\n        span_type = span['type']\n        if span_type == ContentType.Image:\n            if not check_img_bbox(span['bbox']):\n                continue\n            span['image_path'] = cut_image(span['bbox'], page_id, page, return_path=return_path('images'),\n                                           imageWriter=imageWriter)\n        elif span_type == ContentType.Table:\n            if not check_img_bbox(span['bbox']):\n                continue\n            span['image_path'] = cut_image(span['bbox'], page_id, page, return_path=return_path('tables'),\n                                           imageWriter=imageWriter)\n\n    return spans\n\n\ndef check_img_bbox(bbox) -> bool:\n    if any([bbox[0] >= bbox[2], bbox[1] >= bbox[3]]):\n        logger.warning(f'image_bboxes: wrong box, {bbox}')\n        return False\n    return True\n"
  },
  {
    "file_name": "magic_pdf/pre_proc/ocr_detect_all_bboxes.py",
    "file_contents": "from magic_pdf.config.ocr_content_type import BlockType\nfrom magic_pdf.libs.boxbase import (\n    calculate_iou,\n    calculate_overlap_area_in_bbox1_area_ratio,\n    calculate_vertical_projection_overlap_ratio,\n    get_minbox_if_overlap_by_ratio\n)\n\n\ndef add_bboxes(blocks, block_type, bboxes):\n    for block in blocks:\n        x0, y0, x1, y1 = block['bbox']\n        if block_type in [\n            BlockType.ImageBody,\n            BlockType.ImageCaption,\n            BlockType.ImageFootnote,\n            BlockType.TableBody,\n            BlockType.TableCaption,\n            BlockType.TableFootnote,\n        ]:\n            bboxes.append(\n                [\n                    x0,\n                    y0,\n                    x1,\n                    y1,\n                    None,\n                    None,\n                    None,\n                    block_type,\n                    None,\n                    None,\n                    None,\n                    None,\n                    block['score'],\n                    block['group_id'],\n                ]\n            )\n        else:\n            bboxes.append(\n                [\n                    x0,\n                    y0,\n                    x1,\n                    y1,\n                    None,\n                    None,\n                    None,\n                    block_type,\n                    None,\n                    None,\n                    None,\n                    None,\n                    block['score'],\n                ]\n            )\n\n\ndef ocr_prepare_bboxes_for_layout_split_v2(\n    img_body_blocks,\n    img_caption_blocks,\n    img_footnote_blocks,\n    table_body_blocks,\n    table_caption_blocks,\n    table_footnote_blocks,\n    discarded_blocks,\n    text_blocks,\n    title_blocks,\n    interline_equation_blocks,\n    page_w,\n    page_h,\n):\n    all_bboxes = []\n\n    add_bboxes(img_body_blocks, BlockType.ImageBody, all_bboxes)\n    add_bboxes(img_caption_blocks, BlockType.ImageCaption, all_bboxes)\n    add_bboxes(img_footnote_blocks, BlockType.ImageFootnote, all_bboxes)\n    add_bboxes(table_body_blocks, BlockType.TableBody, all_bboxes)\n    add_bboxes(table_caption_blocks, BlockType.TableCaption, all_bboxes)\n    add_bboxes(table_footnote_blocks, BlockType.TableFootnote, all_bboxes)\n    add_bboxes(text_blocks, BlockType.Text, all_bboxes)\n    add_bboxes(title_blocks, BlockType.Title, all_bboxes)\n    add_bboxes(interline_equation_blocks, BlockType.InterlineEquation, all_bboxes)\n\n    all_bboxes = fix_text_overlap_title_blocks(all_bboxes)\n    all_bboxes = remove_need_drop_blocks(all_bboxes, discarded_blocks)\n\n\n    all_bboxes = fix_interline_equation_overlap_text_blocks_with_hi_iou(all_bboxes)\n\n\n    \"\"\"discarded_blocks\"\"\"\n    all_discarded_blocks = []\n    add_bboxes(discarded_blocks, BlockType.Discarded, all_discarded_blocks)\n\n    footnote_blocks = []\n    for discarded in discarded_blocks:\n        x0, y0, x1, y1 = discarded['bbox']\n        if (x1 - x0) > (page_w / 3) and (y1 - y0) > 10 and y0 > (page_h / 2):\n            footnote_blocks.append([x0, y0, x1, y1])\n\n    need_remove_blocks = find_blocks_under_footnote(all_bboxes, footnote_blocks)\n    if len(need_remove_blocks) > 0:\n        for block in need_remove_blocks:\n            all_bboxes.remove(block)\n            all_discarded_blocks.append(block)\n\n    all_bboxes = remove_overlaps_min_blocks(all_bboxes)\n    all_discarded_blocks = remove_overlaps_min_blocks(all_discarded_blocks)\n    # all_bboxes, drop_reasons = remove_overlap_between_bbox_for_block(all_bboxes)\n    all_bboxes.sort(key=lambda x: x[0]+x[1])\n    return all_bboxes, all_discarded_blocks\n\n\ndef find_blocks_under_footnote(all_bboxes, footnote_blocks):\n    need_remove_blocks = []\n    for block in all_bboxes:\n        block_x0, block_y0, block_x1, block_y1 = block[:4]\n        for footnote_bbox in footnote_blocks:\n            footnote_x0, footnote_y0, footnote_x1, footnote_y1 = footnote_bbox\n\n            if (\n                block_y0 >= footnote_y1\n                and calculate_vertical_projection_overlap_ratio(\n                    (block_x0, block_y0, block_x1, block_y1), footnote_bbox\n                )\n                >= 0.8\n            ):\n                if block not in need_remove_blocks:\n                    need_remove_blocks.append(block)\n                    break\n    return need_remove_blocks\n\n\ndef fix_interline_equation_overlap_text_blocks_with_hi_iou(all_bboxes):\n\n    text_blocks = []\n    for block in all_bboxes:\n        if block[7] == BlockType.Text:\n            text_blocks.append(block)\n    interline_equation_blocks = []\n    for block in all_bboxes:\n        if block[7] == BlockType.InterlineEquation:\n            interline_equation_blocks.append(block)\n\n    need_remove = []\n\n    for interline_equation_block in interline_equation_blocks:\n        for text_block in text_blocks:\n            interline_equation_block_bbox = interline_equation_block[:4]\n            text_block_bbox = text_block[:4]\n            if calculate_iou(interline_equation_block_bbox, text_block_bbox) > 0.8:\n                if text_block not in need_remove:\n                    need_remove.append(text_block)\n\n    if len(need_remove) > 0:\n        for block in need_remove:\n            all_bboxes.remove(block)\n\n    return all_bboxes\n\n\ndef fix_text_overlap_title_blocks(all_bboxes):\n\n    text_blocks = []\n    for block in all_bboxes:\n        if block[7] == BlockType.Text:\n            text_blocks.append(block)\n    title_blocks = []\n    for block in all_bboxes:\n        if block[7] == BlockType.Title:\n            title_blocks.append(block)\n\n    need_remove = []\n\n    for text_block in text_blocks:\n        for title_block in title_blocks:\n            text_block_bbox = text_block[:4]\n            title_block_bbox = title_block[:4]\n            if calculate_iou(text_block_bbox, title_block_bbox) > 0.8:\n                if title_block not in need_remove:\n                    need_remove.append(title_block)\n\n    if len(need_remove) > 0:\n        for block in need_remove:\n            all_bboxes.remove(block)\n\n    return all_bboxes\n\n\ndef remove_need_drop_blocks(all_bboxes, discarded_blocks):\n    need_remove = []\n    for block in all_bboxes:\n        for discarded_block in discarded_blocks:\n            block_bbox = block[:4]\n            if (\n                calculate_overlap_area_in_bbox1_area_ratio(\n                    block_bbox, discarded_block['bbox']\n                )\n                > 0.6\n            ):\n                if block not in need_remove:\n                    need_remove.append(block)\n                    break\n\n    if len(need_remove) > 0:\n        for block in need_remove:\n            all_bboxes.remove(block)\n    return all_bboxes\n\n\ndef remove_overlaps_min_blocks(all_bboxes):\n\n\n    need_remove = []\n    for block1 in all_bboxes:\n        for block2 in all_bboxes:\n            if block1 != block2:\n                block1_bbox = block1[:4]\n                block2_bbox = block2[:4]\n                overlap_box = get_minbox_if_overlap_by_ratio(\n                    block1_bbox, block2_bbox, 0.8\n                )\n                if overlap_box is not None:\n                    block_to_remove = next(\n                        (block for block in all_bboxes if block[:4] == overlap_box),\n                        None,\n                    )\n                    if (\n                        block_to_remove is not None\n                        and block_to_remove not in need_remove\n                    ):\n                        large_block = block1 if block1 != block_to_remove else block2\n                        x1, y1, x2, y2 = large_block[:4]\n                        sx1, sy1, sx2, sy2 = block_to_remove[:4]\n                        x1 = min(x1, sx1)\n                        y1 = min(y1, sy1)\n                        x2 = max(x2, sx2)\n                        y2 = max(y2, sy2)\n                        large_block[:4] = [x1, y1, x2, y2]\n                        need_remove.append(block_to_remove)\n\n    if len(need_remove) > 0:\n        for block in need_remove:\n            all_bboxes.remove(block)\n\n    return all_bboxes\n"
  },
  {
    "file_name": "magic_pdf/pre_proc/ocr_dict_merge.py",
    "file_contents": "from magic_pdf.config.ocr_content_type import BlockType, ContentType\nfrom magic_pdf.libs.boxbase import __is_overlaps_y_exceeds_threshold, calculate_overlap_area_in_bbox1_area_ratio\n\n\n\ndef line_sort_spans_by_left_to_right(lines):\n    line_objects = []\n    for line in lines:\n\n        line.sort(key=lambda span: span['bbox'][0])\n        line_bbox = [\n            min(span['bbox'][0] for span in line),  # x0\n            min(span['bbox'][1] for span in line),  # y0\n            max(span['bbox'][2] for span in line),  # x1\n            max(span['bbox'][3] for span in line),  # y1\n        ]\n        line_objects.append({\n            'bbox': line_bbox,\n            'spans': line,\n        })\n    return line_objects\n\n\ndef merge_spans_to_line(spans, threshold=0.6):\n    if len(spans) == 0:\n        return []\n    else:\n\n        spans.sort(key=lambda span: span['bbox'][1])\n\n        lines = []\n        current_line = [spans[0]]\n        for span in spans[1:]:\n\n\n            if span['type'] in [\n                    ContentType.InterlineEquation, ContentType.Image,\n                    ContentType.Table\n            ] or any(s['type'] in [\n                    ContentType.InterlineEquation, ContentType.Image,\n                    ContentType.Table\n            ] for s in current_line):\n\n                lines.append(current_line)\n                current_line = [span]\n                continue\n\n\n            if __is_overlaps_y_exceeds_threshold(span['bbox'], current_line[-1]['bbox'], threshold):\n                current_line.append(span)\n            else:\n\n                lines.append(current_line)\n                current_line = [span]\n\n\n        if current_line:\n            lines.append(current_line)\n\n        return lines\n\n\ndef fill_spans_in_blocks(blocks, spans, radio):\n    block_with_spans = []\n    for block in blocks:\n        block_type = block[7]\n        block_bbox = block[0:4]\n        block_dict = {\n            'type': block_type,\n            'bbox': block_bbox,\n        }\n        if block_type in [\n            BlockType.ImageBody, BlockType.ImageCaption, BlockType.ImageFootnote,\n            BlockType.TableBody, BlockType.TableCaption, BlockType.TableFootnote\n        ]:\n            block_dict['group_id'] = block[-1]\n        block_spans = []\n        for span in spans:\n            span_bbox = span['bbox']\n            if calculate_overlap_area_in_bbox1_area_ratio(\n                    span_bbox, block_bbox) > radio:\n                block_spans.append(span)\n\n        block_dict['spans'] = block_spans\n        block_with_spans.append(block_dict)\n\n\n        if len(block_spans) > 0:\n            for span in block_spans:\n                spans.remove(span)\n\n    return block_with_spans, spans\n\n\ndef fix_block_spans_v2(block_with_spans):\n    fix_blocks = []\n    for block in block_with_spans:\n        block_type = block['type']\n\n        if block_type in [BlockType.Text, BlockType.Title,\n                          BlockType.ImageCaption, BlockType.ImageFootnote,\n                          BlockType.TableCaption, BlockType.TableFootnote\n                          ]:\n            block = fix_text_block(block)\n        elif block_type in [BlockType.InterlineEquation, BlockType.ImageBody, BlockType.TableBody]:\n            block = fix_interline_block(block)\n        else:\n            continue\n        fix_blocks.append(block)\n    return fix_blocks\n\n\ndef fix_discarded_block(discarded_block_with_spans):\n    fix_discarded_blocks = []\n    for block in discarded_block_with_spans:\n        block = fix_text_block(block)\n        fix_discarded_blocks.append(block)\n    return fix_discarded_blocks\n\n\ndef fix_text_block(block):\n\n    for span in block['spans']:\n        if span['type'] == ContentType.InterlineEquation:\n            span['type'] = ContentType.InlineEquation\n    block_lines = merge_spans_to_line(block['spans'])\n    sort_block_lines = line_sort_spans_by_left_to_right(block_lines)\n    block['lines'] = sort_block_lines\n    del block['spans']\n    return block\n\n\ndef fix_interline_block(block):\n    block_lines = merge_spans_to_line(block['spans'])\n    sort_block_lines = line_sort_spans_by_left_to_right(block_lines)\n    block['lines'] = sort_block_lines\n    del block['spans']\n    return block\n"
  },
  {
    "file_name": "magic_pdf/pre_proc/ocr_span_list_modify.py",
    "file_contents": "\nfrom magic_pdf.config.drop_tag import DropTag\nfrom magic_pdf.config.ocr_content_type import BlockType\nfrom magic_pdf.libs.boxbase import calculate_iou, get_minbox_if_overlap_by_ratio\n\n\ndef remove_overlaps_low_confidence_spans(spans):\n    dropped_spans = []\n\n    for span1 in spans:\n        for span2 in spans:\n            if span1 != span2:\n\n                if span1 in dropped_spans or span2 in dropped_spans:\n                    continue\n                else:\n                    if calculate_iou(span1['bbox'], span2['bbox']) > 0.9:\n                        if span1['score'] < span2['score']:\n                            span_need_remove = span1\n                        else:\n                            span_need_remove = span2\n                        if (\n                            span_need_remove is not None\n                            and span_need_remove not in dropped_spans\n                        ):\n                            dropped_spans.append(span_need_remove)\n\n    if len(dropped_spans) > 0:\n        for span_need_remove in dropped_spans:\n            spans.remove(span_need_remove)\n            span_need_remove['tag'] = DropTag.SPAN_OVERLAP\n\n    return spans, dropped_spans\n\n\ndef check_chars_is_overlap_in_span(chars):\n    for i in range(len(chars)):\n        for j in range(i + 1, len(chars)):\n            if calculate_iou(chars[i]['bbox'], chars[j]['bbox']) > 0.35:\n                return True\n    return False\n\n\ndef remove_overlaps_min_spans(spans):\n    dropped_spans = []\n\n    for span1 in spans:\n        for span2 in spans:\n            if span1 != span2:\n\n                if span1 in dropped_spans or span2 in dropped_spans:\n                    continue\n                else:\n                    overlap_box = get_minbox_if_overlap_by_ratio(span1['bbox'], span2['bbox'], 0.65)\n                    if overlap_box is not None:\n                        span_need_remove = next((span for span in spans if span['bbox'] == overlap_box), None)\n                        if span_need_remove is not None and span_need_remove not in dropped_spans:\n                            dropped_spans.append(span_need_remove)\n    if len(dropped_spans) > 0:\n        for span_need_remove in dropped_spans:\n            spans.remove(span_need_remove)\n            span_need_remove['tag'] = DropTag.SPAN_OVERLAP\n\n    return spans, dropped_spans\n\n\ndef get_qa_need_list_v2(blocks):\n\n    images = []\n    tables = []\n    interline_equations = []\n\n    for block in blocks:\n        if block['type'] == BlockType.Image:\n            images.append(block)\n        elif block['type'] == BlockType.Table:\n            tables.append(block)\n        elif block['type'] == BlockType.InterlineEquation:\n            interline_equations.append(block)\n    return images, tables, interline_equations\n"
  },
  {
    "file_name": "magic_pdf/pre_proc/remove_bbox_overlap.py",
    "file_contents": "from magic_pdf.config.drop_reason import DropReason\nfrom magic_pdf.libs.boxbase import _is_in, _is_part_overlap\n\n\ndef _remove_overlap_between_bbox(bbox1, bbox2):\n    if _is_part_overlap(bbox1, bbox2):\n        ix0, iy0, ix1, iy1 = bbox1\n        x0, y0, x1, y1 = bbox2\n\n        diff_x = min(x1, ix1) - max(x0, ix0)\n        diff_y = min(y1, iy1) - max(y0, iy0)\n\n        if diff_y > diff_x:\n            if x1 >= ix1:\n                mid = (x0 + ix1) // 2\n                ix1 = min(mid - 0.25, ix1)\n                x0 = max(mid + 0.25, x0)\n            else:\n                mid = (ix0 + x1) // 2\n                ix0 = max(mid + 0.25, ix0)\n                x1 = min(mid - 0.25, x1)\n        else:\n            if y1 >= iy1:\n                mid = (y0 + iy1) // 2\n                y0 = max(mid + 0.25, y0)\n                iy1 = min(iy1, mid - 0.25)\n            else:\n                mid = (iy0 + y1) // 2\n                y1 = min(y1, mid - 0.25)\n                iy0 = max(mid + 0.25, iy0)\n\n        if ix1 > ix0 and iy1 > iy0 and y1 > y0 and x1 > x0:\n            bbox1 = [ix0, iy0, ix1, iy1]\n            bbox2 = [x0, y0, x1, y1]\n            return bbox1, bbox2, None\n        else:\n            return bbox1, bbox2, DropReason.NEGATIVE_BBOX_AREA\n    else:\n        return bbox1, bbox2, None\n\n\ndef _remove_overlap_between_bboxes(arr):\n    drop_reasons = []\n    N = len(arr)\n    keeps = [True] * N\n    res = [None] * N\n    for i in range(N):\n        for j in range(N):\n            if i == j:\n                continue\n            if _is_in(arr[i]['bbox'], arr[j]['bbox']):\n                keeps[i] = False\n\n    for idx, v in enumerate(arr):\n        if not keeps[idx]:\n            continue\n        for i in range(N):\n            if res[i] is None:\n                continue\n\n            bbox1, bbox2, drop_reason = _remove_overlap_between_bbox(\n                v['bbox'], res[i]['bbox']\n            )\n            if drop_reason is None:\n                v['bbox'] = bbox1\n                res[i]['bbox'] = bbox2\n            else:\n                if v['score'] > res[i]['score']:\n                    keeps[i] = False\n                    res[i] = None\n                else:\n                    keeps[idx] = False\n                drop_reasons.append(drop_reason)\n        if keeps[idx]:\n            res[idx] = v\n    return res, drop_reasons\n\n\ndef remove_overlap_between_bbox_for_span(spans):\n    arr = [{'bbox': span['bbox'], 'score': span.get('score', 0.1)} for span in spans]\n    res, drop_reasons = _remove_overlap_between_bboxes(arr)\n    ret = []\n    for i in range(len(res)):\n        if res[i] is None:\n            continue\n        spans[i]['bbox'] = res[i]['bbox']\n        ret.append(spans[i])\n    return ret, drop_reasons\n\n\ndef remove_overlap_between_bbox_for_block(all_bboxes):\n    arr = [{'bbox': bbox[:4], 'score': bbox[-1]} for bbox in all_bboxes]\n    res, drop_reasons = _remove_overlap_between_bboxes(arr)\n    ret = []\n    for i in range(len(res)):\n        if res[i] is None:\n            continue\n        all_bboxes[i][:4] = res[i]['bbox']\n        ret.append(all_bboxes[i])\n    return ret, drop_reasons\n"
  },
  {
    "file_name": "magic_pdf/utils/annotations.py",
    "file_contents": "\nfrom loguru import logger\n\n\ndef ImportPIL(f):\n    try:\n        import PIL  # noqa: F401\n    except ImportError:\n        logger.error('Pillow not installed, please install by pip.')\n        exit(1)\n    return f\n"
  },
  {
    "file_name": "magic_pdf/utils/load_image.py",
    "file_contents": "# Copyright (c) OpenMMLab. All rights reserved.\n# # Revised from the utils.py of LMDeploy, a library for deploying large language models.\nimport base64\nimport os\nfrom io import BytesIO\nfrom typing import Union\n\nimport requests\nimport fitz\nfrom typing import List\nfrom PIL import Image, ImageFile\nfrom loguru import logger\n\n\ndef encode_image_base64(image: Union[str, Image.Image]) -> str:\n    \"\"\"encode raw data to base64 format.\"\"\"\n    buffered = BytesIO()\n    FETCH_TIMEOUT = int(os.environ.get('LMDEPLOY_FETCH_TIMEOUT', 10))\n    headers = {\n        'User-Agent':\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    }\n    try:\n        if isinstance(image, str):\n            url_or_path = image\n            if url_or_path.startswith('http'):\n                response = requests.get(url_or_path, headers=headers, timeout=FETCH_TIMEOUT)\n                response.raise_for_status()\n                buffered.write(response.content)\n            elif os.path.exists(url_or_path):\n                with open(url_or_path, 'rb') as image_file:\n                    buffered.write(image_file.read())\n        elif isinstance(image, Image.Image):\n            image.save(buffered, format='PNG')\n    except Exception as error:\n        if isinstance(image, str) and len(image) > 100:\n            image = image[:100] + ' ...'\n        logger.error(f'{error}, image={image}')\n        # use dummy image\n        image = Image.new('RGB', (32, 32))\n        image.save(buffered, format='PNG')\n    res = base64.b64encode(buffered.getvalue()).decode('utf-8')\n    return res\n\n\ndef load_image_from_base64(image: Union[bytes, str]) -> Image.Image:\n    \"\"\"load image from base64 format.\"\"\"\n    return Image.open(BytesIO(base64.b64decode(image)))\n\n\ndef load_image(image_url: Union[str, Image.Image], max_size: int = None, min_size: int = None) -> Image.Image:\n    \"\"\"load image from url, local path or openai GPT4V.\"\"\"\n    FETCH_TIMEOUT = int(os.environ.get('LMDEPLOY_FETCH_TIMEOUT', 10))\n    headers = {\n        'User-Agent':\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    }\n    try:\n        ImageFile.LOAD_TRUNCATED_IMAGES = True\n        if isinstance(image_url, Image.Image):\n            img = image_url\n        elif image_url.startswith('http'):\n            response = requests.get(image_url, headers=headers, timeout=FETCH_TIMEOUT)\n            response.raise_for_status()\n            img = Image.open(BytesIO(response.content))\n        elif image_url.startswith('data:image'):\n            img = load_image_from_base64(image_url.split(',')[1])\n        else:\n            # Load image from local path\n            img = Image.open(image_url)\n\n        # check image valid\n        img = img.convert('RGB')\n\n        # resize image if too small\n        if min_size and min(img.size) < min_size:\n            scale = min_size / min(img.size)\n            new_size = (int(img.size[0] * scale), int(img.size[1] * scale))\n            img = img.resize(new_size, Image.LANCZOS)\n\n        # resize image if too large\n        if max_size and max(img.size) > max_size:\n            scale = max_size / max(img.size)\n            new_size = (int(img.size[0] * scale), int(img.size[1] * scale))\n            img = img.resize(new_size, Image.LANCZOS)\n    except Exception as error:\n        if isinstance(image_url, str) and len(image_url) > 100:\n            image_url = image_url[:100] + ' ...'\n        logger.error(f'{error}, image_url={image_url}')\n        # use dummy image\n        img = Image.new('RGB', (32, 32))\n\n    return img\n\n\ndef pdf_to_images(pdf_path: str, dpi: int = 200) -> List[Image.Image]:\n    \"\"\"Read PDF from path to a list of PIL images.\"\"\"\n    doc = fitz.open(pdf_path)\n\n    imgs = []\n    for page_num in range(len(doc)):\n        page = doc.load_page(page_num)\n        zoom = dpi / 72\n        mat = fitz.Matrix(zoom, zoom)\n        pm = page.get_pixmap(matrix=mat, alpha=False)\n\n        # If the width or height exceeds 4500 after scaling, do not scale further.\n        if pm.width > 4500 or pm.height > 4500:\n            pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n        img = Image.frombytes('RGB', (pm.width, pm.height), pm.samples)\n        imgs.append(img)\n\n    return imgs"
  },
  {
    "file_name": "magic_pdf/utils/office_to_pdf.py",
    "file_contents": "import os\nimport subprocess\n\n\nclass ConvertToPdfError(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n        super().__init__(self.msg)\n\n\ndef convert_file_to_pdf(input_path, output_dir):\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"The input file {input_path} does not exist.\")\n\n    os.makedirs(output_dir, exist_ok=True)\n    \n    cmd = [\n        'soffice',\n        '--headless',\n        '--convert-to', 'pdf',\n        '--outdir', str(output_dir),\n        str(input_path)\n    ]\n    \n    process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    if process.returncode != 0:\n        raise ConvertToPdfError(process.stderr.decode())\n"
  },
  {
    "file_name": "magic_pdf/data/data_reader_writer/__init__.py",
    "file_contents": "from magic_pdf.data.data_reader_writer.filebase import \\\n    FileBasedDataReader  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.filebase import \\\n    FileBasedDataWriter  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.multi_bucket_s3 import \\\n    MultiBucketS3DataReader  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.multi_bucket_s3 import \\\n    MultiBucketS3DataWriter  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.s3 import S3DataReader  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.s3 import S3DataWriter  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.base import DataReader  # noqa: F401\nfrom magic_pdf.data.data_reader_writer.base import DataWriter  # noqa: F401"
  },
  {
    "file_name": "magic_pdf/data/data_reader_writer/base.py",
    "file_contents": "\nfrom abc import ABC, abstractmethod\n\n\nclass DataReader(ABC):\n\n    def read(self, path: str) -> bytes:\n        \"\"\"Read the file.\n\n        Args:\n            path (str): file path to read\n\n        Returns:\n            bytes: the content of the file\n        \"\"\"\n        return self.read_at(path)\n\n    @abstractmethod\n    def read_at(self, path: str, offset: int = 0, limit: int = -1) -> bytes:\n        \"\"\"Read the file at offset and limit.\n\n        Args:\n            path (str): the file path\n            offset (int, optional): the number of bytes skipped. Defaults to 0.\n            limit (int, optional): the length of bytes want to read. Defaults to -1.\n\n        Returns:\n            bytes: the content of the file\n        \"\"\"\n        pass\n\n\nclass DataWriter(ABC):\n    @abstractmethod\n    def write(self, path: str, data: bytes) -> None:\n        \"\"\"Write the data to the file.\n\n        Args:\n            path (str): the target file where to write\n            data (bytes): the data want to write\n        \"\"\"\n        pass\n\n    def write_string(self, path: str, data: str) -> None:\n        \"\"\"Write the data to file, the data will be encoded to bytes.\n\n        Args:\n            path (str): the target file where to write\n            data (str): the data want to write\n        \"\"\"\n\n        def safe_encode(data: str, method: str):\n            try:\n                bit_data = data.encode(encoding=method, errors='replace')\n                return bit_data, True\n            except:  # noqa\n                return None, False\n\n        for method in ['utf-8', 'ascii']:\n            bit_data, flag = safe_encode(data, method)\n            if flag:\n                self.write(path, bit_data)\n                break\n"
  },
  {
    "file_name": "magic_pdf/data/data_reader_writer/filebase.py",
    "file_contents": "import os\n\nfrom magic_pdf.data.data_reader_writer.base import DataReader, DataWriter\n\n\nclass FileBasedDataReader(DataReader):\n    def __init__(self, parent_dir: str = ''):\n        \"\"\"Initialized with parent_dir.\n\n        Args:\n            parent_dir (str, optional): the parent directory that may be used within methods. Defaults to ''.\n        \"\"\"\n        self._parent_dir = parent_dir\n\n    def read_at(self, path: str, offset: int = 0, limit: int = -1) -> bytes:\n        \"\"\"Read at offset and limit.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            offset (int, optional): the number of bytes skipped. Defaults to 0.\n            limit (int, optional): the length of bytes want to read. Defaults to -1.\n\n        Returns:\n            bytes: the content of file\n        \"\"\"\n        fn_path = path\n        if not os.path.isabs(fn_path) and len(self._parent_dir) > 0:\n            fn_path = os.path.join(self._parent_dir, path)\n\n        with open(fn_path, 'rb') as f:\n            f.seek(offset)\n            if limit == -1:\n                return f.read()\n            else:\n                return f.read(limit)\n\n\nclass FileBasedDataWriter(DataWriter):\n    def __init__(self, parent_dir: str = '') -> None:\n        \"\"\"Initialized with parent_dir.\n\n        Args:\n            parent_dir (str, optional): the parent directory that may be used within methods. Defaults to ''.\n        \"\"\"\n        self._parent_dir = parent_dir\n\n    def write(self, path: str, data: bytes) -> None:\n        \"\"\"Write file with data.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            data (bytes): the data want to write\n        \"\"\"\n        fn_path = path\n        if not os.path.isabs(fn_path) and len(self._parent_dir) > 0:\n            fn_path = os.path.join(self._parent_dir, path)\n\n        if not os.path.exists(os.path.dirname(fn_path)) and os.path.dirname(fn_path) != \"\":\n            os.makedirs(os.path.dirname(fn_path), exist_ok=True)\n\n        with open(fn_path, 'wb') as f:\n            f.write(data)\n"
  },
  {
    "file_name": "magic_pdf/data/data_reader_writer/multi_bucket_s3.py",
    "file_contents": "\nfrom magic_pdf.config.exceptions import InvalidConfig, InvalidParams\nfrom magic_pdf.data.data_reader_writer.base import DataReader, DataWriter\nfrom magic_pdf.data.io.s3 import S3Reader, S3Writer\nfrom magic_pdf.data.schemas import S3Config\nfrom magic_pdf.libs.path_utils import (parse_s3_range_params, parse_s3path,\n                                       remove_non_official_s3_args)\n\n\nclass MultiS3Mixin:\n    def __init__(self, default_prefix: str, s3_configs: list[S3Config]):\n        \"\"\"Initialized with multiple s3 configs.\n\n        Args:\n            default_prefix (str): the default prefix of the relative path. for example, {some_bucket}/{some_prefix} or {some_bucket}\n            s3_configs (list[S3Config]): list of s3 configs, the bucket_name must be unique in the list.\n\n        Raises:\n            InvalidConfig: default bucket config not in s3_configs.\n            InvalidConfig: bucket name not unique in s3_configs.\n            InvalidConfig: default bucket must be provided.\n        \"\"\"\n        if len(default_prefix) == 0:\n            raise InvalidConfig('default_prefix must be provided')\n\n        arr = default_prefix.strip('/').split('/')\n        self.default_bucket = arr[0]\n        self.default_prefix = '/'.join(arr[1:])\n\n        found_default_bucket_config = False\n        for conf in s3_configs:\n            if conf.bucket_name == self.default_bucket:\n                found_default_bucket_config = True\n                break\n\n        if not found_default_bucket_config:\n            raise InvalidConfig(\n                f'default_bucket: {self.default_bucket} config must be provided in s3_configs: {s3_configs}'\n            )\n\n        uniq_bucket = set([conf.bucket_name for conf in s3_configs])\n        if len(uniq_bucket) != len(s3_configs):\n            raise InvalidConfig(\n                f'the bucket_name in s3_configs: {s3_configs} must be unique'\n            )\n\n        self.s3_configs = s3_configs\n        self._s3_clients_h: dict = {}\n\n\nclass MultiBucketS3DataReader(DataReader, MultiS3Mixin):\n    def read(self, path: str) -> bytes:\n        \"\"\"Read the path from s3, select diffect bucket client for each request\n        based on the bucket, also support range read.\n\n        Args:\n            path (str): the s3 path of file, the path must be in the format of s3://bucket_name/path?offset,limit.\n            for example: s3://bucket_name/path?0,100.\n\n        Returns:\n            bytes: the content of s3 file.\n        \"\"\"\n        may_range_params = parse_s3_range_params(path)\n        if may_range_params is None or 2 != len(may_range_params):\n            byte_start, byte_len = 0, -1\n        else:\n            byte_start, byte_len = int(may_range_params[0]), int(may_range_params[1])\n        path = remove_non_official_s3_args(path)\n        return self.read_at(path, byte_start, byte_len)\n\n    def __get_s3_client(self, bucket_name: str):\n        if bucket_name not in set([conf.bucket_name for conf in self.s3_configs]):\n            raise InvalidParams(\n                f'bucket name: {bucket_name} not found in s3_configs: {self.s3_configs}'\n            )\n        if bucket_name not in self._s3_clients_h:\n            conf = next(\n                filter(lambda conf: conf.bucket_name == bucket_name, self.s3_configs)\n            )\n            self._s3_clients_h[bucket_name] = S3Reader(\n                bucket_name,\n                conf.access_key,\n                conf.secret_key,\n                conf.endpoint_url,\n                conf.addressing_style,\n            )\n        return self._s3_clients_h[bucket_name]\n\n    def read_at(self, path: str, offset: int = 0, limit: int = -1) -> bytes:\n        \"\"\"Read the file with offset and limit, select diffect bucket client\n        for each request based on the bucket.\n\n        Args:\n            path (str): the file path.\n            offset (int, optional): the number of bytes skipped. Defaults to 0.\n            limit (int, optional): the number of bytes want to read. Defaults to -1 which means infinite.\n\n        Returns:\n            bytes: the file content.\n        \"\"\"\n        if path.startswith('s3://'):\n            bucket_name, path = parse_s3path(path)\n            s3_reader = self.__get_s3_client(bucket_name)\n        else:\n            s3_reader = self.__get_s3_client(self.default_bucket)\n            if self.default_prefix:\n                path = self.default_prefix + '/' + path\n        return s3_reader.read_at(path, offset, limit)\n\n\nclass MultiBucketS3DataWriter(DataWriter, MultiS3Mixin):\n    def __get_s3_client(self, bucket_name: str):\n        if bucket_name not in set([conf.bucket_name for conf in self.s3_configs]):\n            raise InvalidParams(\n                f'bucket name: {bucket_name} not found in s3_configs: {self.s3_configs}'\n            )\n        if bucket_name not in self._s3_clients_h:\n            conf = next(\n                filter(lambda conf: conf.bucket_name == bucket_name, self.s3_configs)\n            )\n            self._s3_clients_h[bucket_name] = S3Writer(\n                bucket_name,\n                conf.access_key,\n                conf.secret_key,\n                conf.endpoint_url,\n                conf.addressing_style,\n            )\n        return self._s3_clients_h[bucket_name]\n\n    def write(self, path: str, data: bytes) -> None:\n        \"\"\"Write file with data, also select diffect bucket client for each\n        request based on the bucket.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            data (bytes): the data want to write.\n        \"\"\"\n        if path.startswith('s3://'):\n            bucket_name, path = parse_s3path(path)\n            s3_writer = self.__get_s3_client(bucket_name)\n        else:\n            s3_writer = self.__get_s3_client(self.default_bucket)\n            if self.default_prefix:\n                path = self.default_prefix + '/' + path\n        return s3_writer.write(path, data)\n"
  },
  {
    "file_name": "magic_pdf/data/data_reader_writer/s3.py",
    "file_contents": "from magic_pdf.data.data_reader_writer.multi_bucket_s3 import (\n    MultiBucketS3DataReader, MultiBucketS3DataWriter)\nfrom magic_pdf.data.schemas import S3Config\n\n\nclass S3DataReader(MultiBucketS3DataReader):\n    def __init__(\n        self,\n        default_prefix_without_bucket: str,\n        bucket: str,\n        ak: str,\n        sk: str,\n        endpoint_url: str,\n        addressing_style: str = 'auto',\n    ):\n        \"\"\"s3 reader client.\n\n        Args:\n            default_prefix_without_bucket: prefix that not contains bucket\n            bucket (str): bucket name\n            ak (str): access key\n            sk (str): secret key\n            endpoint_url (str): endpoint url of s3\n            addressing_style (str, optional): Defaults to 'auto'. Other valid options here are 'path' and 'virtual'\n            refer to https://boto3.amazonaws.com/v1/documentation/api/1.9.42/guide/s3.html\n        \"\"\"\n        super().__init__(\n            f'{bucket}/{default_prefix_without_bucket}',\n            [\n                S3Config(\n                    bucket_name=bucket,\n                    access_key=ak,\n                    secret_key=sk,\n                    endpoint_url=endpoint_url,\n                    addressing_style=addressing_style,\n                )\n            ],\n        )\n\n\nclass S3DataWriter(MultiBucketS3DataWriter):\n    def __init__(\n        self,\n        default_prefix_without_bucket: str,\n        bucket: str,\n        ak: str,\n        sk: str,\n        endpoint_url: str,\n        addressing_style: str = 'auto',\n    ):\n        \"\"\"s3 writer client.\n\n        Args:\n            default_prefix_without_bucket: prefix that not contains bucket\n            bucket (str): bucket name\n            ak (str): access key\n            sk (str): secret key\n            endpoint_url (str): endpoint url of s3\n            addressing_style (str, optional): Defaults to 'auto'. Other valid options here are 'path' and 'virtual'\n            refer to https://boto3.amazonaws.com/v1/documentation/api/1.9.42/guide/s3.html\n        \"\"\"\n        super().__init__(\n            f'{bucket}/{default_prefix_without_bucket}',\n            [\n                S3Config(\n                    bucket_name=bucket,\n                    access_key=ak,\n                    secret_key=sk,\n                    endpoint_url=endpoint_url,\n                    addressing_style=addressing_style,\n                )\n            ],\n        )\n"
  },
  {
    "file_name": "magic_pdf/data/io/__init__.py",
    "file_contents": "\nfrom magic_pdf.data.io.base import IOReader, IOWriter  # noqa: F401\nfrom magic_pdf.data.io.http import HttpReader, HttpWriter  # noqa: F401\nfrom magic_pdf.data.io.s3 import S3Reader, S3Writer  # noqa: F401\n\n__all__ = ['IOReader', 'IOWriter', 'HttpReader', 'HttpWriter', 'S3Reader', 'S3Writer']"
  },
  {
    "file_name": "magic_pdf/data/io/base.py",
    "file_contents": "from abc import ABC, abstractmethod\n\n\nclass IOReader(ABC):\n    @abstractmethod\n    def read(self, path: str) -> bytes:\n        \"\"\"Read the file.\n\n        Args:\n            path (str): file path to read\n\n        Returns:\n            bytes: the content of the file\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def read_at(self, path: str, offset: int = 0, limit: int = -1) -> bytes:\n        \"\"\"Read at offset and limit.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            offset (int, optional): the number of bytes skipped. Defaults to 0.\n            limit (int, optional): the length of bytes want to read. Defaults to -1.\n\n        Returns:\n            bytes: the content of file\n        \"\"\"\n        pass\n\n\nclass IOWriter(ABC):\n\n    @abstractmethod\n    def write(self, path: str, data: bytes) -> None:\n        \"\"\"Write file with data.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            data (bytes): the data want to write\n        \"\"\"\n        pass\n"
  },
  {
    "file_name": "magic_pdf/data/io/http.py",
    "file_contents": "\nimport io\n\nimport requests\n\nfrom magic_pdf.data.io.base import IOReader, IOWriter\n\n\nclass HttpReader(IOReader):\n\n    def read(self, url: str) -> bytes:\n        \"\"\"Read the file.\n\n        Args:\n            path (str): file path to read\n\n        Returns:\n            bytes: the content of the file\n        \"\"\"\n        return requests.get(url).content\n\n    def read_at(self, path: str, offset: int = 0, limit: int = -1) -> bytes:\n        \"\"\"Not Implemented.\"\"\"\n        raise NotImplementedError\n\n\nclass HttpWriter(IOWriter):\n    def write(self, url: str, data: bytes) -> None:\n        \"\"\"Write file with data.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            data (bytes): the data want to write\n        \"\"\"\n        files = {'file': io.BytesIO(data)}\n        response = requests.post(url, files=files)\n        assert 300 > response.status_code and response.status_code > 199\n"
  },
  {
    "file_name": "magic_pdf/data/io/s3.py",
    "file_contents": "import boto3\nfrom botocore.config import Config\n\nfrom magic_pdf.data.io.base import IOReader, IOWriter\n\n\nclass S3Reader(IOReader):\n    def __init__(\n        self,\n        bucket: str,\n        ak: str,\n        sk: str,\n        endpoint_url: str,\n        addressing_style: str = 'auto',\n    ):\n        \"\"\"s3 reader client.\n\n        Args:\n            bucket (str): bucket name\n            ak (str): access key\n            sk (str): secret key\n            endpoint_url (str): endpoint url of s3\n            addressing_style (str, optional): Defaults to 'auto'. Other valid options here are 'path' and 'virtual'\n            refer to https://boto3.amazonaws.com/v1/documentation/api/1.9.42/guide/s3.html\n        \"\"\"\n        self._bucket = bucket\n        self._ak = ak\n        self._sk = sk\n        self._s3_client = boto3.client(\n            service_name='s3',\n            aws_access_key_id=ak,\n            aws_secret_access_key=sk,\n            endpoint_url=endpoint_url,\n            config=Config(\n                s3={'addressing_style': addressing_style},\n                retries={'max_attempts': 5, 'mode': 'standard'},\n            ),\n        )\n\n    def read(self, key: str) -> bytes:\n        \"\"\"Read the file.\n\n        Args:\n            path (str): file path to read\n\n        Returns:\n            bytes: the content of the file\n        \"\"\"\n        return self.read_at(key)\n\n    def read_at(self, key: str, offset: int = 0, limit: int = -1) -> bytes:\n        \"\"\"Read at offset and limit.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            offset (int, optional): the number of bytes skipped. Defaults to 0.\n            limit (int, optional): the length of bytes want to read. Defaults to -1.\n\n        Returns:\n            bytes: the content of file\n        \"\"\"\n        if limit > -1:\n            range_header = f'bytes={offset}-{offset+limit-1}'\n            res = self._s3_client.get_object(\n                Bucket=self._bucket, Key=key, Range=range_header\n            )\n        else:\n            res = self._s3_client.get_object(\n                Bucket=self._bucket, Key=key, Range=f'bytes={offset}-'\n            )\n        return res['Body'].read()\n\n\nclass S3Writer(IOWriter):\n    def __init__(\n        self,\n        bucket: str,\n        ak: str,\n        sk: str,\n        endpoint_url: str,\n        addressing_style: str = 'auto',\n    ):\n        \"\"\"s3 reader client.\n\n        Args:\n            bucket (str): bucket name\n            ak (str): access key\n            sk (str): secret key\n            endpoint_url (str): endpoint url of s3\n            addressing_style (str, optional): Defaults to 'auto'. Other valid options here are 'path' and 'virtual'\n            refer to https://boto3.amazonaws.com/v1/documentation/api/1.9.42/guide/s3.html\n        \"\"\"\n        self._bucket = bucket\n        self._ak = ak\n        self._sk = sk\n        self._s3_client = boto3.client(\n            service_name='s3',\n            aws_access_key_id=ak,\n            aws_secret_access_key=sk,\n            endpoint_url=endpoint_url,\n            config=Config(\n                s3={'addressing_style': addressing_style},\n                retries={'max_attempts': 5, 'mode': 'standard'},\n            ),\n        )\n\n    def write(self, key: str, data: bytes):\n        \"\"\"Write file with data.\n\n        Args:\n            path (str): the path of file, if the path is relative path, it will be joined with parent_dir.\n            data (bytes): the data want to write\n        \"\"\"\n        self._s3_client.put_object(Bucket=self._bucket, Key=key, Body=data)\n"
  },
  {
    "file_name": "magic_pdf/model/sub_modules/model_init.py",
    "file_contents": "import torch\nfrom loguru import logger\n\nfrom magic_pdf.config.constants import MODEL_NAME\nfrom magic_pdf.model.model_list import AtomicModel\n\n\ndef doclayout_yolo_model_init(weight, device='cpu'):\n    from magic_pdf.model.sub_modules.layout.doclayout_yolo.DocLayoutYOLO import \\\n        DocLayoutYOLOModel\n    if str(device).startswith(\"npu\"):\n        device = torch.device(device)\n    model = DocLayoutYOLOModel(weight, device)\n    return model\n\n\ndef paddex_layout_model_init(device: str, model_dir: str = None):\n    from magic_pdf.model.sub_modules.layout.paddlex_layout.PaddleXLayoutModel import \\\n        PaddleXLayoutModelWrapper\n    model = PaddleXLayoutModelWrapper(model_name=MODEL_NAME.PaddleXLayoutModel, device=device, model_dir=model_dir)\n    return model\n\n\nclass AtomModelSingleton:\n    _instance = None\n    _models = {}\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def get_atom_model(self, atom_model_name: str, **kwargs):\n\n        layout_model_name = kwargs.get('layout_model_name', None)\n\n        if atom_model_name in [AtomicModel.Layout]:\n            key = (atom_model_name, layout_model_name)\n        else:\n            key = atom_model_name\n\n        if key not in self._models:\n            self._models[key] = atom_model_init(model_name=atom_model_name, **kwargs)\n        return self._models[key]\n\n\ndef atom_model_init(model_name: str, **kwargs):\n    atom_model = None\n    if model_name == AtomicModel.Layout:\n        if kwargs.get('layout_model_name') == MODEL_NAME.DocLayout_YOLO:\n            atom_model = doclayout_yolo_model_init(\n                kwargs.get('doclayout_yolo_weights'),\n                kwargs.get('device')\n            )\n        elif kwargs.get('layout_model_name') == MODEL_NAME.PaddleXLayoutModel:\n            atom_model = paddex_layout_model_init(\n                model_dir=kwargs.get('paddlexlayout_model_dir'),\n                device=kwargs.get('device')\n            )\n        else:\n            logger.error('layout model name not allowed')\n            exit(1)\n    else:\n        logger.error('model name not allowed')\n        exit(1)\n\n    if atom_model is None:\n        logger.error('model init failed')\n        exit(1)\n    else:\n        return atom_model\n"
  },
  {
    "file_name": "magic_pdf/model/sub_modules/model_utils.py",
    "file_contents": "import time\n\nimport torch\nfrom PIL import Image\nfrom loguru import logger\n\nfrom magic_pdf.libs.clean_memory import clean_memory\n\n\ndef crop_img(input_res, input_pil_img, crop_paste_x=0, crop_paste_y=0):\n    crop_xmin, crop_ymin = int(input_res['poly'][0]), int(input_res['poly'][1])\n    crop_xmax, crop_ymax = int(input_res['poly'][4]), int(input_res['poly'][5])\n    # Create a white background with an additional width and height of 50\n    crop_new_width = crop_xmax - crop_xmin + crop_paste_x * 2\n    crop_new_height = crop_ymax - crop_ymin + crop_paste_y * 2\n    return_image = Image.new('RGB', (crop_new_width, crop_new_height), 'white')\n\n    # Crop image\n    crop_box = (crop_xmin, crop_ymin, crop_xmax, crop_ymax)\n    cropped_img = input_pil_img.crop(crop_box)\n    return_image.paste(cropped_img, (crop_paste_x, crop_paste_y))\n    return_list = [crop_paste_x, crop_paste_y, crop_xmin, crop_ymin, crop_xmax, crop_ymax, crop_new_width, crop_new_height]\n    return return_image, return_list\n\n\n# Select regions for OCR / formula regions / table regions\ndef get_res_list_from_layout_res(layout_res):\n    ocr_res_list = []\n    table_res_list = []\n    single_page_mfdetrec_res = []\n    for res in layout_res:\n        if int(res['category_id']) in [13, 14]:\n            single_page_mfdetrec_res.append({\n                \"bbox\": [int(res['poly'][0]), int(res['poly'][1]),\n                         int(res['poly'][4]), int(res['poly'][5])],\n            })\n        elif int(res['category_id']) in [0, 1, 2, 4, 6, 7]:\n            ocr_res_list.append(res)\n        elif int(res['category_id']) in [5]:\n            table_res_list.append(res)\n    return ocr_res_list, table_res_list, single_page_mfdetrec_res\n\n\ndef clean_vram(device, vram_threshold=8):\n    total_memory = get_vram(device)\n    if total_memory and total_memory <= vram_threshold:\n        gc_start = time.time()\n        clean_memory(device)\n        gc_time = round(time.time() - gc_start, 2)\n        logger.info(f\"gc time: {gc_time}\")\n\n\ndef get_vram(device):\n    if torch.cuda.is_available() and device != 'cpu':\n        total_memory = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)\n        return total_memory\n    elif str(device).startswith(\"npu\"):\n        import torch_npu\n        if torch_npu.npu.is_available():\n            total_memory = torch_npu.npu.get_device_properties(device).total_memory / (1024 ** 3)\n            return total_memory\n    else:\n        return None"
  },
  {
    "file_name": "magic_pdf/model/sub_modules/layout/doclayout_yolo/DocLayoutYOLO.py",
    "file_contents": "from doclayout_yolo import YOLOv10\nimport torch\n\nclass DocLayoutYOLOModel(object):\n    def __init__(self, weight, device):\n        self.model = YOLOv10(weight)\n        self.device = device\n\n    def predict(self, image):\n        layout_res = []\n        doclayout_yolo_res = self.model.predict(\n            image,\n            imgsz=1280,\n            conf=0.10,\n            iou=0.45,\n            verbose=False, device=self.device\n        )[0]\n        for xyxy, conf, cla in zip(\n            doclayout_yolo_res.boxes.xyxy.cpu(),\n            doclayout_yolo_res.boxes.conf.cpu(),\n            doclayout_yolo_res.boxes.cls.cpu(),\n        ):\n            xmin, ymin, xmax, ymax = [int(p.item()) for p in xyxy]\n            new_item = {\n                \"category_id\": int(cla.item()),\n                \"poly\": [xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax],\n                \"score\": round(float(conf.item()), 3),\n            }\n            layout_res.append(new_item)\n        return layout_res\n\n    def batch_predict(self, images: list, batch_size: int) -> list:\n        images_layout_res = []\n        if batch_size <= 0:\n            raise ValueError(\"batch_size ÂøÖÈ°ª‰∏∫Ê≠£Êï¥Êï∞\")\n        for index in range(0, len(images), batch_size):\n            doclayout_yolo_res = [\n                image_res.cpu()\n                for image_res in self.model.predict(\n                    images[index : index + batch_size],\n                    imgsz=1280,\n                    conf=0.10,\n                    iou=0.45,\n                    verbose=False,\n                    device=self.device,\n                )\n            ]\n            for image_res in doclayout_yolo_res:\n                layout_res = []\n                for xyxy, conf, cla in zip(\n                    image_res.boxes.xyxy,\n                    image_res.boxes.conf,\n                    image_res.boxes.cls,\n                ):\n                    xmin, ymin, xmax, ymax = [int(p.item()) for p in xyxy]\n                    new_item = {\n                        \"category_id\": int(cla.item()),\n                        \"poly\": [xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax],\n                        \"score\": round(float(conf.item()), 3),\n                    }\n                    layout_res.append(new_item)\n                images_layout_res.append(layout_res)\n\n        return images_layout_res\n"
  },
  {
    "file_name": "magic_pdf/model/sub_modules/layout/paddlex_layout/PaddleXLayoutModel.py",
    "file_contents": "import numpy as np\nfrom PIL import Image\nfrom typing import List, Union\nfrom loguru import logger\n\nfrom magic_pdf.config.ocr_content_type import CategoryId\n\ntry:\n    from paddlex import create_model\nexcept ImportError:\n    raise ImportError(\"Paddlex is not installed. Please install it using 'pip install paddlex'.\")\n\n\nclass PaddleXLayoutModelWrapper:\n    def __init__(self, model_name: str, device: str, model_dir: str = None):\n        self.model_name = model_name\n        self.device = device  # Note: Device may not be directly used by paddlex.create_model\n        logger.info(f\"Loading {self.model_name} model from {model_dir}...\")\n        if model_dir is not None:\n            self.model_dir = model_dir\n            self.model = create_model(model_name=self.model_name, model_dir=self.model_dir)\n        else:\n            self.model = create_model(model_name=self.model_name)\n\n        self.category_mapping = {\n            \"paragraph_title\": CategoryId.Title,\n            \"image\": CategoryId.ImageBody,\n            \"text\": CategoryId.Text,\n            \"number\": CategoryId.Abandon,\n            \"abstract\": CategoryId.Text,\n            \"content\": CategoryId.Text,\n            \"figure_title\": CategoryId.Text,\n            \"formula\": CategoryId.InterlineEquation_Layout,\n            \"table\": CategoryId.TableBody,\n            \"reference\": CategoryId.Text,\n            \"doc_title\": CategoryId.Title,\n            \"footnote\": CategoryId.Abandon,\n            \"header\": CategoryId.Abandon,\n            \"algorithm\": CategoryId.Text,\n            \"footer\": CategoryId.Abandon,\n            \"seal\": CategoryId.Abandon,\n            \"chart\": CategoryId.ImageBody,\n            \"formula_number\": CategoryId.Abandon,\n            \"aside_text\": CategoryId.Text,\n            \"reference_content\": CategoryId.Text,\n        }\n\n    def _process_paddlex_result(self, paddlex_result_obj: dict) -> List[dict]:\n        layout_res = []\n        for det in paddlex_result_obj.get('boxes', []):\n            label_name = det.get('label')\n            category_id = self.category_mapping.get(label_name, -1)\n            \n            # Skip unknown or incomplete detections\n            if category_id == -1 or not det.get('coordinate') or not det.get('score'):\n                continue\n            \n            xmin, ymin, xmax, ymax = [int(p) for p in det['coordinate']]\n            new_item = {\n                \"category_id\": category_id,\n                \"original_label\": label_name,\n                \"poly\": [xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax],\n                \"score\": round(float(det['score']), 3),\n            }\n            layout_res.append(new_item)\n        return layout_res\n\n    def _prepare_image(self, image: Union[np.ndarray, Image.Image]) -> np.ndarray:\n        if isinstance(image, Image.Image):\n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            return np.array(image)\n        elif isinstance(image, np.ndarray):\n            if image.ndim == 2:\n                return np.stack([image] * 3, axis=-1)\n            if image.shape[2] == 4:\n                return image[:, :, :3]\n            return image\n        else:\n            raise TypeError(\"Unsupported image type. Expected PIL.Image or numpy.ndarray.\")\n\n    def predict(self, image: Union[np.ndarray, Image.Image]) -> List[dict]:\n        image_input = self._prepare_image(image)\n        paddlex_output = list(self.model.predict(image_input, batch_size=1, layout_nms=True))\n        if not paddlex_output:\n            return []\n        return self._process_paddlex_result(paddlex_output[0])\n\n    def batch_predict(self, images: List[Union[np.ndarray, Image.Image]], batch_size: int) -> List[List[dict]]:\n        prepared_images = [self._prepare_image(img) for img in images]\n        \n        # The model.predict itself handles batching, but we call it once.\n        paddlex_outputs = list(self.model.predict(prepared_images, batch_size=batch_size, layout_nms=True))\n        return [self._process_paddlex_result(res) for res in paddlex_outputs]\n"
  },
  {
    "file_name": "magic_pdf/model/sub_modules/reading_oreder/layoutreader/helpers.py",
    "file_contents": "from collections import defaultdict\nfrom typing import List, Dict\n\nimport torch\nfrom transformers import LayoutLMv3ForTokenClassification\n\nMAX_LEN = 510\nCLS_TOKEN_ID = 0\nUNK_TOKEN_ID = 3\nEOS_TOKEN_ID = 2\n\n\nclass DataCollator:\n    def __call__(self, features: List[dict]) -> Dict[str, torch.Tensor]:\n        bbox = []\n        labels = []\n        input_ids = []\n        attention_mask = []\n\n        # clip bbox and labels to max length, build input_ids and attention_mask\n        for feature in features:\n            _bbox = feature[\"source_boxes\"]\n            if len(_bbox) > MAX_LEN:\n                _bbox = _bbox[:MAX_LEN]\n            _labels = feature[\"target_index\"]\n            if len(_labels) > MAX_LEN:\n                _labels = _labels[:MAX_LEN]\n            _input_ids = [UNK_TOKEN_ID] * len(_bbox)\n            _attention_mask = [1] * len(_bbox)\n            assert len(_bbox) == len(_labels) == len(_input_ids) == len(_attention_mask)\n            bbox.append(_bbox)\n            labels.append(_labels)\n            input_ids.append(_input_ids)\n            attention_mask.append(_attention_mask)\n\n        # add CLS and EOS tokens\n        for i in range(len(bbox)):\n            bbox[i] = [[0, 0, 0, 0]] + bbox[i] + [[0, 0, 0, 0]]\n            labels[i] = [-100] + labels[i] + [-100]\n            input_ids[i] = [CLS_TOKEN_ID] + input_ids[i] + [EOS_TOKEN_ID]\n            attention_mask[i] = [1] + attention_mask[i] + [1]\n\n        # padding to max length\n        max_len = max(len(x) for x in bbox)\n        for i in range(len(bbox)):\n            bbox[i] = bbox[i] + [[0, 0, 0, 0]] * (max_len - len(bbox[i]))\n            labels[i] = labels[i] + [-100] * (max_len - len(labels[i]))\n            input_ids[i] = input_ids[i] + [EOS_TOKEN_ID] * (max_len - len(input_ids[i]))\n            attention_mask[i] = attention_mask[i] + [0] * (\n                max_len - len(attention_mask[i])\n            )\n\n        ret = {\n            \"bbox\": torch.tensor(bbox),\n            \"attention_mask\": torch.tensor(attention_mask),\n            \"labels\": torch.tensor(labels),\n            \"input_ids\": torch.tensor(input_ids),\n        }\n        # set label > MAX_LEN to -100, because original labels may be > MAX_LEN\n        ret[\"labels\"][ret[\"labels\"] > MAX_LEN] = -100\n        # set label > 0 to label-1, because original labels are 1-indexed\n        ret[\"labels\"][ret[\"labels\"] > 0] -= 1\n        return ret\n\n\ndef boxes2inputs(boxes: List[List[int]]) -> Dict[str, torch.Tensor]:\n    bbox = [[0, 0, 0, 0]] + boxes + [[0, 0, 0, 0]]\n    input_ids = [CLS_TOKEN_ID] + [UNK_TOKEN_ID] * len(boxes) + [EOS_TOKEN_ID]\n    attention_mask = [1] + [1] * len(boxes) + [1]\n    return {\n        \"bbox\": torch.tensor([bbox]),\n        \"attention_mask\": torch.tensor([attention_mask]),\n        \"input_ids\": torch.tensor([input_ids]),\n    }\n\n\ndef prepare_inputs(\n    inputs: Dict[str, torch.Tensor], model: LayoutLMv3ForTokenClassification\n) -> Dict[str, torch.Tensor]:\n    ret = {}\n    for k, v in inputs.items():\n        v = v.to(model.device)\n        if torch.is_floating_point(v):\n            v = v.to(model.dtype)\n        ret[k] = v\n    return ret\n\n\ndef parse_logits(logits: torch.Tensor, length: int) -> List[int]:\n    \"\"\"\n    parse logits to orders\n\n    :param logits: logits from model\n    :param length: input length\n    :return: orders\n    \"\"\"\n    logits = logits[1 : length + 1, :length]\n    orders = logits.argsort(descending=False).tolist()\n    ret = [o.pop() for o in orders]\n    while True:\n        order_to_idxes = defaultdict(list)\n        for idx, order in enumerate(ret):\n            order_to_idxes[order].append(idx)\n        # filter idxes len > 1\n        order_to_idxes = {k: v for k, v in order_to_idxes.items() if len(v) > 1}\n        if not order_to_idxes:\n            break\n        # filter\n        for order, idxes in order_to_idxes.items():\n            # find original logits of idxes\n            idxes_to_logit = {}\n            for idx in idxes:\n                idxes_to_logit[idx] = logits[idx, order]\n            idxes_to_logit = sorted(\n                idxes_to_logit.items(), key=lambda x: x[1], reverse=True\n            )\n            # keep the highest logit as order, set others to next candidate\n            for idx, _ in idxes_to_logit[1:]:\n                ret[idx] = orders[idx].pop()\n\n    return ret\n\n\ndef check_duplicate(a: List[int]) -> bool:\n    return len(a) != len(set(a))\n"
  },
  {
    "file_name": "magic_pdf/model/sub_modules/reading_oreder/layoutreader/xycut.py",
    "file_contents": "from typing import List\nimport cv2\nimport numpy as np\n\n\ndef projection_by_bboxes(boxes: np.array, axis: int) -> np.ndarray:\n    assert axis in [0, 1]\n    length = np.max(boxes[:, axis::2])\n    res = np.zeros(length, dtype=int)\n    # TODO: how to remove for loop?\n    for start, end in boxes[:, axis::2]:\n        res[start:end] += 1\n    return res\n\n\n# from: https://dothinking.github.io/2021-06-19-%E9%80%92%E5%BD%92%E6%8A%95%E5%BD%B1%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/#:~:text=%E9%80%92%E5%BD%92%E6%8A%95%E5%BD%B1%E5%88%86%E5%89%B2%EF%BC%88Recursive%20XY,%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%88%92%E5%88%86%E6%AE%B5%E8%90%BD%E3%80%81%E8%A1%8C%E3%80%82\ndef split_projection_profile(arr_values: np.array, min_value: float, min_gap: float):\n    \"\"\"Split projection profile:\n\n    ```\n                              ‚îå‚îÄ‚îÄ‚îê\n         arr_values           ‚îÇ  ‚îÇ       ‚îå‚îÄ‚îê‚îÄ‚îÄ‚îÄ\n             ‚îå‚îÄ‚îÄ‚îê             ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ |\n             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇmin_value\n             ‚îÇ  ‚îÇ<- min_gap ->‚îÇ  ‚îÇ ‚îÇ   ‚îÇ ‚îÇ ‚îÇ |\n         ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚îÄ\n         0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n    ```\n\n    Args:\n        arr_values (np.array): 1-d array representing the projection profile.\n        min_value (float): Ignore the profile if `arr_value` is less than `min_value`.\n        min_gap (float): Ignore the gap if less than this value.\n\n    Returns:\n        tuple: Start indexes and end indexes of split groups.\n    \"\"\"\n    # all indexes with projection height exceeding the threshold\n    arr_index = np.where(arr_values > min_value)[0]\n    if not len(arr_index):\n        return\n\n    # find zero intervals between adjacent projections\n    # |  |                    ||\n    # ||||<- zero-interval -> |||||\n    arr_diff = arr_index[1:] - arr_index[0:-1]\n    arr_diff_index = np.where(arr_diff > min_gap)[0]\n    arr_zero_intvl_start = arr_index[arr_diff_index]\n    arr_zero_intvl_end = arr_index[arr_diff_index + 1]\n\n    # convert to index of projection range:\n    # the start index of zero interval is the end index of projection\n    arr_start = np.insert(arr_zero_intvl_end, 0, arr_index[0])\n    arr_end = np.append(arr_zero_intvl_start, arr_index[-1])\n    arr_end += 1  # end index will be excluded as index slice\n\n    return arr_start, arr_end\n\n\ndef recursive_xy_cut(boxes: np.ndarray, indices: List[int], res: List[int]):\n\n    assert len(boxes) == len(indices)\n\n    _indices = boxes[:, 1].argsort()\n    y_sorted_boxes = boxes[_indices]\n    y_sorted_indices = indices[_indices]\n\n    # debug_vis(y_sorted_boxes, y_sorted_indices)\n\n    y_projection = projection_by_bboxes(boxes=y_sorted_boxes, axis=1)\n    pos_y = split_projection_profile(y_projection, 0, 1)\n    if not pos_y:\n        return\n\n    arr_y0, arr_y1 = pos_y\n    for r0, r1 in zip(arr_y0, arr_y1):\n\n        _indices = (r0 <= y_sorted_boxes[:, 1]) & (y_sorted_boxes[:, 1] < r1)\n\n        y_sorted_boxes_chunk = y_sorted_boxes[_indices]\n        y_sorted_indices_chunk = y_sorted_indices[_indices]\n\n        _indices = y_sorted_boxes_chunk[:, 0].argsort()\n        x_sorted_boxes_chunk = y_sorted_boxes_chunk[_indices]\n        x_sorted_indices_chunk = y_sorted_indices_chunk[_indices]\n\n\n        x_projection = projection_by_bboxes(boxes=x_sorted_boxes_chunk, axis=0)\n        pos_x = split_projection_profile(x_projection, 0, 1)\n        if not pos_x:\n            continue\n\n        arr_x0, arr_x1 = pos_x\n        if len(arr_x0) == 1:\n\n            res.extend(x_sorted_indices_chunk)\n            continue\n\n\n        for c0, c1 in zip(arr_x0, arr_x1):\n            _indices = (c0 <= x_sorted_boxes_chunk[:, 0]) & (\n                x_sorted_boxes_chunk[:, 0] < c1\n            )\n            recursive_xy_cut(\n                x_sorted_boxes_chunk[_indices], x_sorted_indices_chunk[_indices], res\n            )\n\n\ndef points_to_bbox(points):\n    assert len(points) == 8\n\n    # [x1,y1,x2,y2,x3,y3,x4,y4]\n    left = min(points[::2])\n    right = max(points[::2])\n    top = min(points[1::2])\n    bottom = max(points[1::2])\n\n    left = max(left, 0)\n    top = max(top, 0)\n    right = max(right, 0)\n    bottom = max(bottom, 0)\n    return [left, top, right, bottom]\n\n\ndef bbox2points(bbox):\n    left, top, right, bottom = bbox\n    return [left, top, right, top, right, bottom, left, bottom]\n\n\ndef vis_polygon(img, points, thickness=2, color=None):\n    br2bl_color = color\n    tl2tr_color = color\n    tr2br_color = color\n    bl2tl_color = color\n    cv2.line(\n        img,\n        (points[0][0], points[0][1]),\n        (points[1][0], points[1][1]),\n        color=tl2tr_color,\n        thickness=thickness,\n    )\n\n    cv2.line(\n        img,\n        (points[1][0], points[1][1]),\n        (points[2][0], points[2][1]),\n        color=tr2br_color,\n        thickness=thickness,\n    )\n\n    cv2.line(\n        img,\n        (points[2][0], points[2][1]),\n        (points[3][0], points[3][1]),\n        color=br2bl_color,\n        thickness=thickness,\n    )\n\n    cv2.line(\n        img,\n        (points[3][0], points[3][1]),\n        (points[0][0], points[0][1]),\n        color=bl2tl_color,\n        thickness=thickness,\n    )\n    return img\n\n\ndef vis_points(\n    img: np.ndarray, points, texts: List[str] = None, color=(0, 200, 0)\n) -> np.ndarray:\n    \"\"\"\n\n    Args:\n        img:\n        points: [N, 8]  8: x1,y1,x2,y2,x3,y3,x3,y4\n        texts:\n        color:\n\n    Returns:\n\n    \"\"\"\n    points = np.array(points)\n    if texts is not None:\n        assert len(texts) == points.shape[0]\n\n    for i, _points in enumerate(points):\n        vis_polygon(img, _points.reshape(-1, 2), thickness=2, color=color)\n        bbox = points_to_bbox(_points)\n        left, top, right, bottom = bbox\n        cx = (left + right) // 2\n        cy = (top + bottom) // 2\n\n        txt = texts[i]\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n\n        img = cv2.rectangle(\n            img,\n            (cx - 5 * len(txt), cy - cat_size[1] - 5),\n            (cx - 5 * len(txt) + cat_size[0], cy - 5),\n            color,\n            -1,\n        )\n\n        img = cv2.putText(\n            img,\n            txt,\n            (cx - 5 * len(txt), cy - 5),\n            font,\n            0.5,\n            (255, 255, 255),\n            thickness=1,\n            lineType=cv2.LINE_AA,\n        )\n\n    return img\n\n\ndef vis_polygons_with_index(image, points):\n    texts = [str(i) for i in range(len(points))]\n    res_img = vis_points(image.copy(), points, texts)\n    return res_img"
  }
]